"Id","PostTypeId","AcceptedAnswerId","ParentId","CreationDate","Score","ViewCount","Body","OwnerUserId","OwnerDisplayName","LastEditorUserId","LastEditDate","LastActivityDate","Title","Tags","AnswerCount","CommentCount","ClosedDate","FavoriteCount"
"11062","2","","11060","2020-01-01T02:05:33.300","3","","<p>If you have issue with memory and dealing with large object, maybe <code>data.table</code> is the way to go (<a href=""https://github.com/Rdatatable/data.table/wiki"" rel=""nofollow noreferrer"">https://github.com/Rdatatable/data.table/wiki</a>): </p>\n\n<p>Let's take two dummy data.frame:</p>\n\n<pre class=""lang-r prettyprint-override""><code>dfA &lt;- data.frame(Chrom = rep(""chr1"",150),\n                  Pos = 1:150,\n                  ref = rep(""A"",150))\ndfB &lt;- data.frame(Chrom = rep(""chr1"",10),\n                  Pos = 11:20,\n                  Vaf = rep(""VAF"", 10))\n</code></pre>\n\n<p>The big data.frame <code>dfA</code>:</p>\n\n<pre class=""lang-r prettyprint-override""><code>&gt; head(dfA)\n  Chrom Pos ref\n1  chr1   1   A\n2  chr1   2   A\n3  chr1   3   A\n4  chr1   4   A\n5  chr1   5   A\n6  chr1   6   A\n</code></pre>\n\n<p>and the small dataset:</p>\n\n<pre class=""lang-r prettyprint-override""><code>&gt; dfB\n   Chrom Pos Vaf\n1   chr1  11 VAF\n2   chr1  12 VAF\n3   chr1  13 VAF\n4   chr1  14 VAF\n5   chr1  15 VAF\n6   chr1  16 VAF\n7   chr1  17 VAF\n8   chr1  18 VAF\n9   chr1  19 VAF\n10  chr1  20 VAF\n</code></pre>\n\n<p>Now, using <code>data.table</code> that is more efficient and less memory consuming for the manipulation of dataframe, you can do:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(data.table)\nsetDT(dfA)\nsetDT(dfB)\ndfA[dfB, on = c(""Chrom"",""Pos""), vaf := i.Vaf]\n</code></pre>\n\n<p>And now, your data.frame <code>dfA</code> looks like:</p>\n\n<pre class=""lang-r prettyprint-override""><code>&gt; dfA[5:25]\n    Chrom Pos ref  vaf\n 1:  chr1   5   A &lt;NA&gt;\n 2:  chr1   6   A &lt;NA&gt;\n 3:  chr1   7   A &lt;NA&gt;\n 4:  chr1   8   A &lt;NA&gt;\n 5:  chr1   9   A &lt;NA&gt;\n 6:  chr1  10   A &lt;NA&gt;\n 7:  chr1  11   A  VAF\n 8:  chr1  12   A  VAF\n 9:  chr1  13   A  VAF\n10:  chr1  14   A  VAF\n11:  chr1  15   A  VAF\n12:  chr1  16   A  VAF\n13:  chr1  17   A  VAF\n14:  chr1  18   A  VAF\n15:  chr1  19   A  VAF\n16:  chr1  20   A  VAF\n17:  chr1  21   A &lt;NA&gt;\n18:  chr1  22   A &lt;NA&gt;\n19:  chr1  23   A &lt;NA&gt;\n20:  chr1  24   A &lt;NA&gt;\n21:  chr1  25   A &lt;NA&gt;\n    Chrom Pos ref  vaf\n</code></pre>\n\n<p>As you see, we incorporate the column <code>Vaf</code> from the small dataframe into the big dataframe.</p>\n\n<p>With your example, I will first rename the colnames of your small dataframe in order to have the perfect match with the big dataframe:</p>\n\n<pre class=""lang-r prettyprint-override""><code>colnames(data) &lt;- gsub(""Chrom"", ""Chromosome"", colnames(data))\n</code></pre>\n\n<p>then,</p>\n\n<pre class=""lang-r prettyprint-override""><code>setDT(data1)\nsetDT(data)\ndata1[data, on = c(""Chromosome"",""Position""), i_TumorVAF_WU := i.VAF]\n</code></pre>\n\n<p>Does it look what you are trying to obtain ?</p>\n","6437","","6437","2020-01-01T02:11:24.293","2020-01-01T02:11:24.293","","","","0","",""
"11063","1","","","2020-01-01T09:40:54.017","0","19","<p><a href=""https://bioconductor.org/packages/release/bioc/html/GeneOverlap.html"" rel=""nofollow noreferrer"">GeneOverlap</a> I am trying to use this library so in the default data sets it has different list of genes which are categorized as high low or medium expressed genes. So my question is can I run this one using deferentially expressed genes list various comparison or I need the gene list to be categorized into various groups based on expression.</p>\n\n<p>My objective is to look for significant overlap happening or not for example I came across this figure in this <a href=""https://www.nature.com/articles/s41386-018-0296-1"" rel=""nofollow noreferrer"">paper</a> <a href=""https://i.stack.imgur.com/I9Zz6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I9Zz6.png"" alt=""figureE ""></a></p>\n","285","","","","2020-01-01T09:40:54.017","Geneoverlap library data input","<r><bioconductor>","0","0","",""
"11064","2","","11061","2020-01-01T11:06:24.120","1","","<p>Here is a quick and kind of dirty solution with minimum intervention to the code:</p>\n\n<ul>\n<li>Add <code>$outname'\t'</code> right after <code>echo -e</code> to two two lines, the lines after <code>#Header for output file</code> and <code>#Print output</code>. The former will alter your header and the latter your actual output: you will have a first column with file name information.</li>\n<li>Then you can run this script in a for loop to generate multiple output files which then can be merged into a single file with <code>cat</code>.</li>\n</ul>\n\n<p>Note that with this workaround, you will have multiple headers in the merged file. If you need a clean output, you will need to put your while loop in a for loop.</p>\n","3541","","","","2020-01-01T11:06:24.120","","","","3","",""
"11067","2","","11061","2020-01-01T17:03:10.967","5","","<p>Using shell loops for text practice <a href=""https://unix.stackexchange.com/q/169716/22222"">is considered bad practice</a>. It is exceedingly slow, the syntax is complicated so it's very easy to get it wrong and it's just painful. The shell isn't designed as a proper scripting language, so while it can be (ab)used that way, you really should avoid it whenever possible. </p>\n\n<p>Here's a Perl script that should do exactly what your bash script does but will be several times faster and can deal with multiple input files:</p>\n\n<pre class=""lang-perl prettyprint-override""><code>#!/usr/bin/env perl\n\n## Iterate over all files given at the command line\nforeach my <span class=""math-container"">$inFile (@ARGV) {\n  print STDERR ""Processing $inFile...\n"";\n  my $outFile = ""$inFile.parsed.txt"";\n  $outFile =~ m|([^/]+)$|;\n  #/#Useless comment to fix SE syntax highlighting\n  my $outFileName = $1;\n  open(my $outFileHandle, '&gt;', $outFile) or\n      die(""Couldn't open '$outFile' for writing: $!"");\n  open(my $inFileHandle, '&lt;', $inFile) or\n      die(""Couldn't open '$inFile' for writing: $</span>!"");\n\n  ## Print header\n  print <span class=""math-container"">$outFileHandle ""$</span>outFileName\tChrom\tPosition\tRef\tAlt\tTumorReadCount\t"" .\n      ""TumorVariantAlleleCount\tTumorReferenceAlleleCount\tNormalReadCount"" .\n          ""\tNormalVariantAlleleCount\tNormalReferenceAlleleCount\tVAF\n"";\n  ## Read all lines of the input file\n  while (&lt;<span class=""math-container"">$inFileHandle&gt;) {\n    ## Skip headers\n    next if /^#/;\n    ## Get VCF fields\n    my ($chrom, $pos, $name, $ref, $alt, $qual,\n        $filter, $info, $format, $values) = split(/\t/);\n    my @infoFields = split(/;/, $info);\n    my $readCount = $infoFields[12];\n    $</span>readCount =~ s/ReadCount=//;\n    #/#Useless comment to fix SE syntax highlighting\n\n    my <span class=""math-container"">$variantAlleleCount = $</span>infoFields[25];\n    <span class=""math-container"">$variantAlleleCount =~ s/VariantAlleleCount=//;\n    #/#Useless comment to fix SE syntax highlighting\n    my $</span>referenceAlleleCount= <span class=""math-container"">$readCount - $</span>variantAlleleCount;\n\n    my <span class=""math-container"">$readCountControl = $</span>infoFields[13];\n    $readCountControl =~ s/ReadCountControl=//;\n    #/#Useless comment to fix SE syntax highlighting\n\n    my <span class=""math-container"">$variantAlleleCountControl = $</span>infoFields[26];\n    $variantAlleleCountControl =~ s/VariantAlleleCountControl=//;\n    #/#Useless comment to fix SE syntax highlighting\n\n    my <span class=""math-container"">$referenceAlleleCountControl = $</span>readCountControl - <span class=""math-container"">$variantAlleleCountControl;\n    my $</span>VAF = <span class=""math-container"">$infoFields[27];\n    $</span>VAF =~ s/VariantAlleleFrequency=//;\n    #/#Useless comment to fix SE syntax highlighting\n\n    my <span class=""math-container"">$outString = join(""\t"", $</span>outFileName, <span class=""math-container"">$chrom, $</span>pos, <span class=""math-container"">$ref, $</span>alt, <span class=""math-container"">$readCount,\n                         $</span>variantAlleleCount, <span class=""math-container"">$referenceAlleleCount, $</span>readCountControl,\n                         <span class=""math-container"">$variantAlleleCountControl, $</span>referenceAlleleCountControl, <span class=""math-container"">$VAF);\n    print $</span>outFileHandle ""<span class=""math-container"">$outString\n"";\n  }\n  close($</span>outFileHandle);\n  close($inFileHandle);\n}\n</code></pre>\n\n<p>Save that file as <code>parser.pl</code> and then run it on all your vcf files with:</p>\n\n<pre><code>perl parser.pl /path/to/vcfs/*vcf\n</code></pre>\n\n<p>That should create a <code>.parsed.txt</code> file for each input file in the target directory. </p>\n\n<hr>\n\n<p>If you just have to use the shell for this, you can try with this improved version of your script:</p>\n\n<pre class=""lang-sh prettyprint-override""><code>#!/usr/bin/env bash\n\nfor file in ""<span class=""math-container"">$@""; do\n  outFile=""$</span>{file}.parsed.txt""\n  outFileName=<span class=""math-container"">$(basename ""$</span>outFile"")\n  (\n    printf '%s\tChrom\tPosition\tRef\tAlt\tTumorReadCount\t' ""<span class=""math-container"">$outFileName""\n    printf 'TumorVariantAlleleCount\tTumorReferenceAlleleCount\tNormalReadCount\t'\n    printf 'NormalVariantAlleleCount\tNormalReferenceAlleleCount\tVAF\n'\n  ) &gt; ""$</span>outFile""\n\n  echo ""Reading <span class=""math-container"">$file""\n  grep -v '^#' ""$</span>file"" |\n    while read -r line; do\n      read chrom Pos Ref Alt rest &lt; &lt;(awk -F""\t"" '{print <span class=""math-container"">$1,$</span>2,<span class=""math-container"">$4,$</span>5}' &lt;&lt;&lt;""<span class=""math-container"">$line"")\n      ReadCount=$</span>(echo ""<span class=""math-container"">$line"" | cut -f 8 | sed 's/;/\t/g' | cut -f 13 | sed 's/ReadCount=//' )\n      VariantAlleleCount=$</span>(echo ""<span class=""math-container"">$line"" | cut -f 8 | sed 's/;/\t/g' | cut -f 26| sed 's/VariantAlleleCount=//')\n      ReferenceAlleleCount=$</span>((<span class=""math-container"">$ReadCount - $</span>VariantAlleleCount))\n\n      ReadCountControl=<span class=""math-container"">$(echo ""$</span>line"" | cut -f 8 | sed 's/;/\t/g' | cut -f 14 | sed 's/ReadCountControl=//')\n      VariantAlleleCountControl=<span class=""math-container"">$(echo ""$</span>line"" | cut -f 8 | sed 's/;/\t/g' | cut -f 27 | sed 's/VariantAlleleCountControl=//')   \n\n      ReferenceAlleleCountControl=<span class=""math-container"">$(($</span>ReadCountControl - $VariantAlleleCountControl))\n\n      VAF=<span class=""math-container"">$(echo ""$</span>line"" | cut -f 8 | sed 's/;/\t/g' | cut -f 28 | sed 's/VariantAlleleFrequency=//')\n      printf '%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n' ""<span class=""math-container"">$outFileName"" ""$</span>chrom"" ""<span class=""math-container"">$Pos"" ""$</span>Ref"" ""<span class=""math-container"">$Alt"" \\n             ""$</span>ReadCount"" ""<span class=""math-container"">$VariantAlleleCount"" ""$</span>ReferenceAlleleCount"" ""<span class=""math-container"">$ReadCountControl"" ""$</span>VariantAlleleCountControl"" \\n             ""<span class=""math-container"">$ReferenceAlleleCountControl"" ""$</span>VAF"" &gt;&gt; ""$outFile"" \n    done\ndone\n</code></pre>\n\n<hr>\n\n<p>Just to illustrate what I mean about the shell being slow, I ran both scripts above on the example files you provided:</p>\n\n<pre><code>$ time foo.pl *vcf 2&gt;/dev/null\n\nreal    0m0.051s\nuser    0m0.047s\nsys     0m0.003s\n\n$ time bar.sh *vcf 2&gt;/dev/null\n\nreal    2m28.842s\nuser    3m33.648s\nsys     1m12.456s\n</code></pre>\n\n<p>As you can see above, the perl script took less than a second, while the bash script took almost two and a half minutes!</p>\n\n<hr>\n\n<p>Finally, if you find the perl script too long, here's the same thing in a more condensed style:</p>\n\n<pre class=""lang-perl prettyprint-override""><code>#!/usr/bin/env perl\n\n## Iterate over all files given at the command line\nforeach my <span class=""math-container"">$inFile (@ARGV) {\n  print STDERR ""Processing $inFile...\n"";\n  my $outFile = ""$inFile.parsed.txt"";\n  $outFile =~ m|([^/]+)$|;\n  #/#Useless comment to fix SE syntax highlighting\n  my $outFileName = $1;\n  open(my $outFileHandle, '&gt;', $outFile);\n  open(my $inFileHandle, '&lt;', $</span>inFile);\n\n  ## Print header\n  print <span class=""math-container"">$outFileHandle ""$</span>outFileName\tChrom\tPosition\tRef\tAlt\tTumorReadCount\t"" .\n      ""TumorVariantAlleleCount\tTumorReferenceAlleleCount\tNormalReadCount"" .\n          ""\tNormalVariantAlleleCount\tNormalReferenceAlleleCount\tVAF\n"";\n  ## Read all lines of the input file\n  while (&lt;<span class=""math-container"">$inFileHandle&gt;) {\n    ## Skip headers\n    next if /^#/;\n    ## Get VCF fields\n    my ($chrom, $pos, $name, $ref, $alt, $qual,\n        $filter, $info, $format, $values) = split(/\t/);\n    $info=~s/[^;]+=//g;\n    my @infoFields = split(/;/, $info);\n    print $outFileHandle (join ""\t"", $outFileName, $chrom, $pos, $ref, $alt,\n                               $infoFields[12],$infoFields[25],\n                               $infoFields[12]-$infoFields[25],\n                               $infoFields[13],$infoFields[26],\n                               $infoFields[13]-$infoFields[26],\n                               $</span>infoFields[27]) . ""\n"";\n  }\n}\n</code></pre>\n","298","","","","2020-01-01T17:03:10.967","","","","6","",""
"11068","1","","","2020-01-01T17:55:04.727","2","33","<p>I am using IGV 2.5.2 and would not like to see the insertions  in my aligned reads. How can I do that? I have managed to remove mismatched bases because there is an option to do so when I right-click on the window (""Show mismatched bases""). Now I would like to do the same with these purple insertions.</p>\n\n<p><a href=""https://i.stack.imgur.com/hxliq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hxliq.png"" alt=""IGV screenshot""></a></p>\n\n<h3>3rd January 2020 update</h3>\n\n<p><a href=""https://i.stack.imgur.com/avy5K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/avy5K.png"" alt=""IGV preferences""></a></p>\n\n<p>Thanks to a member's response, I found some parameter in the preferences that might be helpful to hide the indels. However, I have tried different numbers from 0 to 1000 and the indels are still shown. What numbers should I put to the parameters ""Show indel threshold (bases)"" and should I click the box or not?</p>\n","1075","","1075","2020-01-03T18:04:22.677","2020-01-03T18:04:22.677","How can I not show insertions in the Integrative Genome Viewer (IGV)?","<alignment><sequence-alignment><visualization><igv>","1","1","",""
"11070","2","","11068","2020-01-02T22:10:34.813","3","","<p>Preferences <code>-&gt;</code> Alignments <code>-&gt;</code> Hide indels &lt; 'x' bases.</p>\n\n<hr>\n\n<p>Credit to <a href=""https://www.biostars.org/p/339007/"" rel=""nofollow noreferrer"">Pierre Lindenbaum</a></p>\n","181","","","","2020-01-02T22:10:34.813","","","","0","",""
"11071","2","","3500","2020-01-03T22:39:24.113","0","","<p>Use this line of code:    </p>\n\n<pre><code>print(analysed_seq.protein_scale(analysed_seq.get_amino_acids_percent(), window=9, edge=0.4))\n</code></pre>\n","6714","","","","2020-01-03T22:39:24.113","","","","1","",""
"11072","1","","","2020-01-04T00:20:18.280","2","19","<p>I found that there are a couple of projects that allow the user to simulate the neural system of C Elegans. I don't know how to navigate sources in bioinformatics and thus struggle to evaluate their quality. Are there any fundamental / most widely accepted data sources for raw data of C Elegans connectome or electron microscope scans of this organism, similar to Human Genome Project? If not, what are the ways to derive the data without having access to an electron microscope?</p>\n","6715","","6715","2020-01-04T09:17:22.693","2020-01-04T09:17:22.693","Are there any ""canonical"" sources of C. Elegans connectome?","<public-databases>","0","1","",""
"11074","1","11085","","2020-01-04T16:39:12.990","0","95","<p>I have a bash script:</p>\n\n<p>I am wondering how I can change this script to loop over a bunch of .vcf files and give output .txt with the name of corresponding .vcf </p>\n\n<p>I tried changes done in similar script in  <a href=""https://bioinformatics.stackexchange.com/questions/11061/changing-this-code-in-a-way-to-work-for-my-files/11067?noredirect=1#comment15031_11067"">Changing this code in a way to work for my files</a> but I just  messed everything up</p>\n","4595","","4595","2020-01-07T12:10:26.760","2020-01-07T15:28:39.057","Looping over several files in bash","<vcf><bash><linux><wgs>","2","0","","1"
"11075","1","11099","","2020-01-04T22:07:38.867","2","36","<h2><a href=""https://i.stack.imgur.com/cnOA0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cnOA0.png"" alt=""Subchains view.""></a></h2>\n\n<p>I'm trying to programmatically construct a name for each protein subchain in any ribosome from Uniprot in accordance with Ban et. al's 2014 proposal (excerpt given below) using PDB's and Uniprot's APIs.\nThis is to be a part of a bigger ribosomal analysis suite released next year and we would like to standardize the names as much as possible.\nBan et. al's proposal: [<a href=""https://bangroup.ethz.ch/research/nomenclature-of-ribosomal-proteins.html]"" rel=""nofollow noreferrer"">https://bangroup.ethz.ch/research/nomenclature-of-ribosomal-proteins.html]</a></p>\n\n<hr>\n\n<p>I'm however not remotely a biologist (I work in compilers) and <strong><em>would really appreciate some clarification on the way nomenclature works across domains of life and how homology plays into it.</em></strong> \nThis is my <em>naive</em> approach thus far: </p>\n\n<ul>\n<li>I get a ribosome from PDB let's say (<em>3J9M</em> or <em>5MYJ</em> or any other for that matter) and split it into protein subchains. So on the order of 20-80 proteins.</li>\n<li>For each protein subchain, I can get all the information in the world on it from Uniprot: names, sequence, organisms containing it, publications, synonymous names, you name it...</li>\n<li>Right now (again, naively) for those proteins that <em>don't already have a new name assigned to them</em>, I take name-synonyms that PDB has plenty of (from various publications, I assume) and scan them with regexes (ex.<code>/[LS]\d{1,2}/g</code>) for things like <code>""L8""</code> or <code>""S15""</code>. </li>\n<li>Thus, for each subchain, I end up with something like <code>""L15""</code> or <code>""S28""</code> in multiple copies (depending on the abundance of synonymous entries in PDB).</li>\n<li>The rut i'm in right now is this: having something like <code>""L15""</code> to match it to a new ""Ban-nomenclature"" name: they provide the nomenclature look-up tables in terms of these three ""historical"" naming conventions(i.e ""human"", ""yeast"", ""bacteria"") and a taxonomic range while all the I have is the ribosomal structure(ex `` from which I got the subchain. I'm sure there's plenty of information that I can get </li>\n</ul>\n\n<p>Could somebody elucidate to me which assumptions I can and cannot make in this field and how exactly to use the <em>taxonomic range</em>? </p>\n\n<p>Say, (i) can I assume that all archean names are the same as bacterial homologs?\nIf I draw a eukaryotic cytoplasmic ribosome, do I match its subchains against yeast column or human column (since both are eukaryotic)?</p>\n\n<hr>\n\n<p>Somebody suggested that I start grouping these subchains based on sequence, not on a domain and I do have access to each protein's sequence, but I am not sure how to go about it. I was thinking to get each of Ban's new names' <strong>UniRef90</strong> cluster, let's say, and then start checking which cluster a given subchain belongs to. </p>\n\n<p>But I'm obviously guessing at this point. Any advice or pointers are very appreciated. </p>\n\n<h2> <a href=""https://i.stack.imgur.com/TlcEE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TlcEE.png"" alt=""Excerpt from Ban""></a></h2>\n","6511","","6511","2020-01-06T06:09:29.777","2020-01-07T21:44:58.037","Infer the new (Ban et. al) ribosomal nomenlature (ex. uL53 ) from the ribosomal protein's sequence","<biopython><uniprot><ribosomal><nomenclature><sequence>","1","5","",""
"11077","1","","","2020-01-05T08:58:12.143","0","49","<p>we performed a combined gene expression and CiteSeq experiment with the 10x VDJ kit and 20 conjugated antibodies and sequenced on hiseq. I used cellranger to process the sequencing output. The cellranger summary shows overall good values except for a low percentage of ""Antibody Reads Usable"" and a low fraction of ""Antibody Reads in Cells"". How is it possible that so few antibody reads are usable? Does this mean that most antibodies were not bound to the cell surface?  <a href=""https://i.stack.imgur.com/cHkxg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cHkxg.png"" alt=""cellranger metrics""></a> </p>\n","6718","","","","2020-01-31T09:05:28.280","Low Fraction of usable antibody reads in CiteSeq","<scrnaseq><sequencing><single-cell><10x-genomics>","1","1","",""
"11078","2","","11074","2020-01-05T11:11:41.790","2","","<p>Error means that quotations are missing. I am not going to review this chunk but at first glance check </p>\n\n<pre><code>printf 'TumorAltAlleleCount\tTumorAltAlleleFrequency\t'\n'\n</code></pre>\n\n<p>There are three quotation marks so one is not matched. I suggest you develop code within a good IDE that will help you debug. Please google around for potential solutions. I personally do R and Shell scripting in RStudio. It differently colors quoted and unquoted code so you immediately see if you missed a closing quotation somewhere.</p>\n","3051","","","","2020-01-05T11:11:41.790","","","","0","",""
"11079","1","11080","","2020-01-05T17:20:38.883","2","50","<p>I have a list of vcf files; I also have a list of names in a txt file like</p>\n\n<pre><code>LP6005409-DNA_F01\nLP2000325-DNA_A01\nLP6005409-DNA_E02\nLP6005500-DNA_C03\n</code></pre>\n\n<p>What I have in this txt files are part of name of each vcf files</p>\n\n<p>I want to move .vcf files having these in their name to a separate folder</p>\n\n<p>I done</p>\n\n<pre><code>[fi1d18@cyan01 TRG]<span class=""math-container"">$ for i in *.vcf\n&gt; do if grep -q $</span>i 1.txt;\n&gt; cp *<span class=""math-container"">$out* /temp/hgig/fi1d18/TRG45/snp/snp/TRG/pre/\n&gt; done\n-bash: syntax error near unexpected token `done'\n[fi1d18@cyan01 TRG]$</span>\n</code></pre>\n\n<p>How I can complete this task?</p>\n\n<p>Thank you</p>\n","4595","","298","2020-01-06T09:43:15.410","2020-01-21T19:29:07.280","Moving file based on their names","<bash><linux>","2","0","","1"
"11080","2","","11079","2020-01-05T19:34:42.530","5","","<p>First of all you are getting an error because you are missing a <code>then</code> in the third line of your code and then a <code>fi</code> to close the <code>if</code>. It should be:</p>\n\n<pre><code>for i in *.vcf\ndo if grep -q <span class=""math-container"">$i 1.txt; then\n    cp *$</span>out* /temp/hgig/fi1d18/TRG45/snp/snp/TRG/pre/\nfi\ndone\n</code></pre>\n\n<p>And I guess the one-liner below achieves what you are trying to do:</p>\n\n<pre><code>while IFS= read -r line; do mv ""$line""*.vcf some-directory; done &lt; file-list.txt \n</code></pre>\n","3541","","298","2020-01-06T09:38:19.013","2020-01-06T09:38:19.013","","","","0","",""
"11081","1","","","2020-01-05T19:47:10.463","0","20","<p>I'm studying on a plant that has no reference genome and only has one scaffold assembly and one gff3 annotation file. Can I create an index with the same assembly and gff3 in STAR and do the mapping? If I want to do de novo assembly using the Galaxy Can I import the links of 20 fastq files from the RNA-seq of the samples together to construct the Reference Genome with Trinity? If this is possible, please help me import the links of these files together into Galaxy and make the reference genome.</p>\n\n<p>Thanks a lot</p>\n","6719","","","","2020-01-05T20:43:29.183","mapping and de novo assembly of plant without reference genome","<rna-seq><assembly>","1","0","",""
"11082","2","","11081","2020-01-05T20:43:29.183","1","","<p>You have multiple questions, I will try to answer one by one:</p>\n\n<ul>\n<li><p>On creating an index with <code>STAR</code>: As long as you provide a fasta file and its corresponding gff, you will be able to generate an index.</p></li>\n<li><p>On creating a reference with <code>Trinity</code>: You have mentioned ""to construct the Reference Genome with Trinity"", however, with your RNA-seq data and Trinity, you can construct a de novo transcriptome and not a genome.</p></li>\n<li><p>Uploading files to <code>Galaxy</code>: I suggest to do so via FTP using <code>FileZilla</code> or something similar. The transfer can take some time if your files are large though.</p></li>\n</ul>\n\n<p>If I understand your questions correctly, you are performing RNA-seq and you are concerned about the completeness of the genome of your model organism. In that case, I would advise to bring together a de novo transcriptome from your RNA-seq data (with Trinity), use one of the quasi-mapping methods (with a tool like <code>Salmon</code> or <code>kallisto</code>).</p>\n","3541","","","","2020-01-05T20:43:29.183","","","","0","",""
"11083","1","","","2020-01-06T01:49:45.790","0","47","<p>I have a <code>.vcf</code> file generated using <code>samtools</code>. I want to know mutations in specific genes by comparing <code>.vcf</code> files with a reference genome. I cannot use the Ensembl VEP tool as my reference genome is not listed there. Can you please help?</p>\n","6720","","3541","2020-01-06T07:46:03.750","2020-02-05T10:01:50.360","bedtool intersect to compare .vcf file to reference genome","<vcf>","1","1","",""
"11084","2","","11083","2020-01-06T09:59:52.677","0","","<p>If you want to filter a VCF file, you might want to give <strong>VCFtools</strong> a try.</p>\n\n<p>Here is the documentation : <a href=""http://vcftools.sourceforge.net/man_latest.html"" rel=""nofollow noreferrer"">VCFtools Manual</a></p>\n\n<p>I am not a specialist of this tool but you could do as the following. First build a bedfile with the chromosome names and positions of your genes (start and stop) such as this :</p>\n\n<pre><code>chr   start   stop   (name)   (score)   (strand)\n</code></pre>\n\n<p>Then it should work with this command line :</p>\n\n<pre><code>vcftools --vcf file.vcf --bed positions.bed --out filtered.vcf\n</code></pre>\n\n<p>The output will be a VCF filtered on the provided regions</p>\n","6273","","","","2020-01-06T09:59:52.677","","","","1","",""
"11085","2","","11074","2020-01-06T11:36:28.780","2","","<p>If we save the script pasted in the main post as a sh file and we have some .vcf files in a folder, by this line we can iterate over vcf files to extract what mentioned in the script returning .txt output by the name of corresponding vcf</p>\n\n<p><code>for file in *.vcf ; do bash indel_vcf_parasing.sh $file ; done</code></p>\n","4595","","4595","2020-01-07T15:28:39.057","2020-01-07T15:28:39.057","","","","3","",""
"11087","1","","","2020-01-06T14:37:48.307","0","44","<p>I am analyzing a single-cell sequencing dataset from the website 10xgenomics, with 2000 cells. It is a BAM file and I am trying to obtain the individual cells per sample. I used the command</p>\n\n<pre><code>samtools view mtdnaAsorted.bam | grep CB:Z: | sed 's/.CB:Z:([ACGT]).*/\1/' | sort | uniq -c &gt; reads_per_barcode_mtdnaA\n</code></pre>\n\n<p>to get a list of the individual bar codes and their reads, but there are 100,000 unique barcodes even with only 2000 cells. Does anyone know what could be the problem?</p>\n\n<p>I know that there could be noise, but this is a problem for me because I'm trying to analyze coverage per cell, and if there are a lot of cells that contribute towards the total number of reads, the net coverage per cell is going to be really low</p>\n","6722","","964","2020-01-06T14:51:37.723","2020-01-06T15:37:37.463","Single-cell sequencing dataset has too many barcodes","<samtools><single-cell><10x-genomics>","1","1","",""
"11088","2","","11087","2020-01-06T15:37:37.463","4","","<p>The BAM file you downloaded has all of the detected 10X barcodes instead of those that represent cells.  In a given 10X experiment the number of input barcodes vastly outnumbers the input cell count.  After the reaction some of barcoded beads are invariably sequenced because they are encapsulated with ambient RNA, dead cells etc. After processing the BAM file Cell Ranger infers which cells likely represent cells and generates gene expression matrices from these files.  </p>\n\n<p>You would need to download one of the output files that contains the list of passed barcodes such as the <code>Gene / cell matrix (filtered)</code> and then use the list of passed barocode sequences to filter your BAM file. Alternatively you could approximately select the same cells by selecting the top 2,000 barcodes with the highest numbers of reads.</p>\n","64","","","","2020-01-06T15:37:37.463","","","","0","",""
"11089","1","","","2020-01-06T19:52:36.393","0","9","<p>I found that triplet is statistically consistent, meaning, <em>Given large amount data, most occurring Gene tree containing three taxa will likely be a true species tree</em> I understand the proof. Is quartret also consistent? How to prove  that?</p>\n","6724","","","","2020-01-06T19:52:36.393","Quartert and Statistical Consistency","<statistics><phylogeny>","0","0","",""
"11090","1","","","2020-01-06T21:12:25.983","0","34","<p>I'm wondering if there are any tools out there that perform statistical phasing of genetic data (e.g. Eagle) that take read-based phasing from GATK into account.  I've found <a href=""https://academic.oup.com/bioinformatics/article/35/14/i242/5529122"" rel=""nofollow noreferrer"">this paper</a> which sounds like what I am looking for, but that tool was developed specifically for Hi-C data, and I only have exomes.</p>\n\n<p>Thanks</p>\n","59","","","","2020-02-09T19:01:35.300","Read-informed statistical phasing","<exome><genetics><phasing>","1","0","",""
"11091","1","","","2020-01-07T00:50:58.093","0","8","<p>My group wants to use Galaxy's htseq_count to map reads from a BAM file onto the IWGSC v1.0 lncRNA GFF file however we have encountered several difficulties.  Specifically, there are many lines for what appears to be the same coding sequence, for instance the first nine rows all start at position 61723.  This means that htseq_count does not know which of these 9 rows to match a particular read with.  Furthermore, in the group column each entry starts with ID=STRG... rather than the traditional gene_id=... which also confounds our approach by making the htseq_count unable to recognise which lines in the GFF file are all actually just one feature.</p>\n\n<p>How do I circumvent these problems - are there other tools on galaxy I should use first to clean the GFF file (see image), or do I need to use special settings or some other trick?  Let me know if there is any other information you need or if I should share my galaxy history with you to clarify things.\n<a href=""https://i.stack.imgur.com/ylfcI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ylfcI.png"" alt=""screenshot from galaxy of GFF file""></a></p>\n","6726","","","","2020-01-07T00:50:58.093","Using galaxy tool htseq_count for lncRNA in wheat reference genome","<gff3><rna-alignment><galaxy>","0","0","",""
"11092","1","","","2020-01-07T07:33:01.640","0","28","<p>I have a group of genes with the same functionality belonging to 3 different families. I would like to see how their expression is distributed across the genome: do they tend to be expressed in a kind of clusters or not? I would also like to use some data from GTEX database or Human Protein Atlas. Could anyone help with a tool/method to solve this problem? </p>\n","4542","","","","2020-01-07T07:33:01.640","How to check the distribution of gene expression across the genome?","<gene><genome>","0","0","",""
"11093","1","11130","","2020-01-07T10:32:50.877","1","38","<p><strong>Background:</strong>\nI am looking for some advice on the use of SNP variants versus INDEL variants for determining Genetic Genealogy, specifically for determining accurately the ""START"" and ""END"" of shared DNA segments (shared between two DNA testers' results). The test data used is RAW DNA from direct to consumer DNA testing companies such as AncestryDNA and 23andMe.</p>\n\n<p><strong>Question:</strong>\nFor the purpose of Genetic Genealogy, are the INDEL variants as useful as the SNP variants? Should the INDEL variants be ignored? Or, alternatively, do the INDEL variants hold a greater significance than the SNP variants?</p>\n\n<p>I hope that using INDEL variants would increase the data in the comparison algorithm, thus increasing the certainty and resolution when determining each shared segment.</p>\n","6728","","3233","2020-01-13T15:51:43.230","2020-01-13T15:51:43.230","Genetic Genealogy related to SNP variants and INDEL variants","<genetics>","2","0","",""
"11094","1","11095","","2020-01-07T13:27:10.803","2","252","<p>I got NGS reads back from sequencing platform. I check for adaptors and trimmed them. But I realized only a fraction (eg 30%) have adaptors... why not all of them?</p>\n\n<p>Thanks</p>\n","6459","","181","2020-01-07T16:59:20.903","2020-01-07T16:59:20.903","Why don't all reads have adaptors?","<sequencing><adapter><cutadapt><fastqc>","1","0","",""
"11095","2","","11094","2020-01-07T13:39:04.447","9","","<p>Things like this might depend on your specific library prep, but in general: sequencing starts at the end of the adapter, not before it. You will only see adapters if you sequence through the entire fragment into the adapters on the other side. If your fragment is long enough you won't see any adapter, and of course, that's the desirable outcome of your library prep.</p>\n","681","","","","2020-01-07T13:39:04.447","","","","0","",""
"11096","2","","5454","2020-01-07T20:55:28.797","0","","<p>For me it works best to combine <code>cat</code>, <code>while</code> and official download tool <code>fasterq-dump</code>.   </p>\n\n<pre><code>cat your_accession_list.txt | while read i\ndo  \nfasterq-dump ${i} &lt;options&gt;\ndone\n</code></pre>\n\n<p>As commented above, <code>your_accession_list.txt</code> should have Linux endings. Link to <code>fasterq-dump</code> is <a href=""https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump"" rel=""nofollow noreferrer"">here</a>.</p>\n","2268","","","","2020-01-07T20:55:28.797","","","","0","",""
"11097","2","","7027","2020-01-07T21:00:57.383","0","","<p>Not a command-line tool but also useful is redesigned <a href=""https://www.ncbi.nlm.nih.gov/Traces/study/?"" rel=""nofollow noreferrer"">SRA Run Selector</a>. Insert comma separated accessions and you can filter/download based on metadata.</p>\n","2268","","","","2020-01-07T21:00:57.383","","","","0","",""
"11098","2","","411","2020-01-07T21:06:08.390","1","","<p>Another possibility in 2020 is to use <a href=""https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump"" rel=""nofollow noreferrer"">fasterq-dump</a> which is promoted to be faster than previous <code>fastq-dump</code>.</p>\n","2268","","","","2020-01-07T21:06:08.390","","","","0","",""
"11099","2","","11075","2020-01-07T21:44:58.037","1","","<p><em>This is a copy-paste nearly verbatim of comments so that the question has an answer</em></p>\n\n<p>What you are trying to do is find what genes are in what homology cluster. This is common problem and there are many solution each with some issues. </p>\n\n<p>Uniprot90 is indeed a cluster of homologues but it is too limited. Whereas you require clusters that span all the domains (universal).</p>\n\n<h3>COG</h3>\n\n<p>The nearly original grouping, is COG (cluster of orthologous genes). It is really handy for this but aren't really available from Uniprot and NCBI (its curator) has been phasing COGs out. In Uniprot records, they are <em>patchily</em> present as its derivative eggNOG (which has varying degrees of ""scope"", LUCA being the universal one):</p>\n\n<pre><code>&lt;dbReference type=""eggNOG"" id=""COG0451""&gt;\n&lt;property type=""taxonomic scope"" value=""LUCA""/&gt;\n&lt;/dbReference&gt;\n</code></pre>\n\n<h3>Pfam</h3>\n\n<p>Another option that is well mapped and always present when possible in Uniprot entries is <a href=""https://pfam.xfam.org/"" rel=""nofollow noreferrer"">Pfam</a> ids. These are domain family annotations and are also good option. It is a <em>domain</em> fold group. So domains within protein with homologous folds are grouped into one. If a protein has two known domains, it will have two Pfam entries —repeat proteins have loads, but most protein have one domain. In Uniprot XML you will find these are:</p>\n\n<pre><code>&lt;dbReference type=""Pfam"" id=""PF01370""&gt;\n</code></pre>\n\n<p>Similarly there is InterPro, which is more narrow than Pfam, but generally overlaps.</p>\n\n<h3>DIY</h3>\n\n<p>Then there's always the option of making your own psi-Blast motif on the NCBI website (setting high number of hits, setting the database to refseq or even PDB and doing a few iterations), saving the motif pattern (PSSM) and using local blast with it.</p>\n\n<h3>Caveats</h3>\n\n<p>In general you may however encounter these pitfalls:</p>\n\n<ol>\n<li>you may have two protein clusters that share a single id</li>\n<li>you have gene fusions (one protein, 2 domains of interest). In Pfam the protein will have two ids, with COGs or similar, you'd have only an anamalous length to go by.</li>\n<li>you may have wished to find analogues (i.e. two protein that arose independently that have the same role (a lot of ribosomal protein are simply structural). No homology clustering scheme can find these.  Say a bacterium has evolved to have a different protein with that role, yet as it has never been studied it is a domain of unknown function (guilt by association is the common strategy, but that is beyond your scope here).</li>\n<li>you have a very small protein which is poorly conserved</li>\n</ol>\n","6322","","","","2020-01-07T21:44:58.037","","","","0","",""
"11100","1","","","2020-01-07T23:27:34.313","1","81","<p>Is there any user friendly way to find rare mutations in the individual human whole genome sequencing raw data? (from Dante, 30x coverage).</p>\n\n<p>To be more specific, I want to find mutations from this paper:</p>\n\n<p><a href=""https://docs.google.com/document/d/1EkRMuD6J0-zyMY3MegKhz7Th0hzhzVXg6DWg2Hm6l2Y/edit?usp=sharing"" rel=""nofollow noreferrer"">https://docs.google.com/document/d/1EkRMuD6J0-zyMY3MegKhz7Th0hzhzVXg6DWg2Hm6l2Y/edit?usp=sharing</a>\n(it's very short, less than one page).</p>\n\n<p>But I'm confused. In their paper they just talk about genes (RAD21, B3GAT2, SMC3, SCN11A , SCN5A, SCN9A, SCN10A, SCN11A, TRPA1), but not mutations? Can we find that diseased-mutations which they talking about in genes, which their list? Or is there not enough data in the paper/study for this?</p>\n\n<p>And if there is not enough data, which data I need to request from authors?</p>\n\n<p>Or those genes have kind a ""gold standard sequences"" and if that sequence different from standard - there is ""diseased"" gene?\n(I have only very basic genetics and bioinformatics knowledges)</p>\n","6733","","6733","2020-01-07T23:36:13.367","2020-02-12T15:03:10.487","Any user friendly way to find rare mutations in whole genome raw?","<gene><sequencing><genome><ngs><sequence-analysis>","1","4","",""
"11101","1","","","2020-01-08T10:20:54.703","0","15","<p>I am trying to code a shiny app for RNA-Seq data analysis.</p>\n\n<p>I would like to include glimma interactive plots in it. \nHowever, in my current interface, clicking the action button <code>Glimma plot</code> opens the glimma interface in a new page of my web browser and not in the shiny UI.</p>\n\n<p>I use <code>shiny</code>, <code>glimma</code> and <code>EdgeR</code>. Here is my code:</p>\n\n<pre><code>ui &lt;- fluidPage(\n  actionButton(inputId=""run.glimma"", label=""Glimma plot""),\n  htmlOutput(""glimma"")\n\n\nserver &lt;- function(input, output, session){\n  observeEvent(input<span class=""math-container"">$run.glimma, {\n    glMDSPlot(dgeObj.norm(), labels=rownames(dgeObj.norm()$samples), groups=dgeObj.norm()$samples[,as.character(input$</span>mds.grouping.feature)])\n  }),\n\n  output$glimma &lt;- renderUI({\n      includeHTML(""glimma-plots/MDS-Plot.html"")\n    })\n}\n</code></pre>\n\n<p>I only indicated the part of the code that I use to generate the <code>glimma</code> MDS plot.</p>\n\n<p>Is there a way to display the <code>glimma</code> plots in the shiny app UI and not in a new webpage ?</p>\n\n<p>Thank you</p>\n","6734","","","","2020-01-08T10:20:54.703","include a glimma interface in a shiny app","<r><rna-seq><bioconductor>","0","0","",""
"11102","2","","11060","2020-01-08T10:33:08.700","1","","<p>Read the manuals for <code>data.table::merge</code> and <code>base::merge</code>.</p>\n\n<p>This (wrong syntax):</p>\n\n<pre><code>unique(merge(data1,data, x.by = data1<span class=""math-container"">$Position, y.by = data$</span>Position))\n</code></pre>\n\n<p>Should work fine as below:</p>\n\n<pre><code>merge(data1, data, by = ""Position"")\n</code></pre>\n\n<p>Regarding: </p>\n\n<blockquote>\n  <p>""losing a big part of big data frame""`</p>\n</blockquote>\n\n<p>It is only returning rows when there is a match on <em>Position</em>, if you need to return the full dataset, then use:</p>\n\n<pre><code>merge(data1, data, by = ""Position"", all.x = TRUE)\n</code></pre>\n","131","","","","2020-01-08T10:33:08.700","","","","0","",""
"11103","5","","","2020-01-08T10:42:27.667","0","","<blockquote>\n  <p>Looper is a pipeline submitting engine. Looper deploys any command-line pipeline for each sample in a project organized in standard PEP format. You can think of looper as providing a single user interface to running, summarizing, monitoring, and otherwise managing all of your sample-intensive research projects the same way, regardless of data type or pipeline used.</p>\n</blockquote>\n\n<ul>\n<li><a href=""https://github.com/pepkit/looper"" rel=""nofollow noreferrer"">GitHub</a></li>\n<li><a href=""http://looper.databio.org/en/latest/"" rel=""nofollow noreferrer"">Website</a></li>\n</ul>\n","131","","131","2020-01-08T11:19:11.060","2020-01-08T11:19:11.060","","","","0","",""
"11104","4","","","2020-01-08T10:42:27.667","0","","A job submitter that uses Portable Encapsulated Projects","131","","131","2020-01-08T11:19:06.460","2020-01-08T11:19:06.460","","","","0","",""
"11105","2","","10960","2020-01-08T10:50:10.843","2","","<p>Change to <em>factor</em> then to <em>numeric</em>:</p>\n\n<pre><code>mat<span class=""math-container"">$Tumor_Sample_Barcode &lt;- as.numeric(as.factor(mat$</span>Tumor_Sample_Barcode))\n</code></pre>\n\n<p>Using @dc37 example data:</p>\n\n<pre><code>as.numeric(as.factor(df$vect))\n# [1] 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4\n</code></pre>\n\n<p>If we need to prefix with ""A""</p>\n\n<pre><code>paste0(""A"", as.numeric(as.factor(df$vect)))\n# [1] ""A1"" ""A2"" ""A3"" ""A4"" ""A1"" ""A2"" ""A3"" ""A4"" ""A1"" ""A2"" ""A3"" ""A4"" ""A1"" ""A2"" ""A3"" ""A4"" ""A1"" ""A2"" ""A3"" ""A4""\n</code></pre>\n\n<p>Or we can keep them as <em>factors</em>:</p>\n\n<pre><code>x &lt;- sort(unique(df<span class=""math-container"">$vect))\nfactor(df$</span>vect, levels = x, labels = paste0(""A"", seq_along(x)))\n# [1] A1 A2 A3 A4 A1 A2 A3 A4 A1 A2 A3 A4 A1 A2 A3 A4 A1 A2 A3 A4\n# Levels: A1 A2 A3 A4\n</code></pre>\n","131","","131","2020-01-08T10:56:49.523","2020-01-08T10:56:49.523","","","","0","",""
"11106","1","11107","","2020-01-08T13:24:29.093","1","19","<p>I am running the <code>STAR-RSEM</code> pipeline, I have a simple shell script(*), which I use to loop over fasta files corresponding to different samples. From the output, I am interested in the <code>*.genes.results</code> files, for which the columns are:</p>\n\n<pre><code>&gt; names(my_df1)\n[1] ""gene_id"" ""transcript_id(s)"" ""length"" ""effective_length"" ""expected_count"" ""TPM"" ""FPKM""            \n</code></pre>\n\n<p>To my surprise, whereas the <code>gene_id</code> and <code>transcript_id(s)</code> columns are the same, I see different <code>length</code> and <code>effective_length</code> values for the same genes in different sample results (omitting the <code>transcript_id(s)</code> column for formatting purposes):</p>\n\n<pre><code>&gt; my_df1 &lt;- fread(paste0(""alignment/"", my_files[1]))\n&gt; head(my_df1[1:5,c(1,3:6)])\n              gene_id  length effective_length expected_count   TPM\n1: ENSG00000000003.14 2292.23          2242.23            978 26.69\n2:  ENSG00000000005.5 1339.00          1289.00             11  0.52\n3: ENSG00000000419.12 1024.45           974.45            469 29.45\n4: ENSG00000000457.13 4204.00          4154.00            241  3.55\n5: ENSG00000000460.16 2953.40          2903.40             62  1.31\n&gt; \n&gt; my_df2 &lt;- fread(paste0(""alignment/"", my_files[2]))\n&gt; head(my_df2[1:5,c(1,3:6)])\n              gene_id  length effective_length expected_count    TPM\n1: ENSG00000000003.14 2260.38          2210.38       10052.00 117.78\n2:  ENSG00000000005.5 1269.22          1219.22         132.00   2.80\n3: ENSG00000000419.12 1058.13          1008.13        4386.00 112.68\n4: ENSG00000000457.13 4175.05          4125.05         873.00   5.48\n5: ENSG00000000460.16 2783.92          2733.92         780.01   7.39\n&gt; \n&gt; my_df3 &lt;- fread(paste0(""alignment/"", my_files[3]))\n&gt; head(my_df3[1:5,c(1,3:6)])\n              gene_id  length effective_length expected_count  TPM\n1: ENSG00000000003.14 2662.59          2612.59             49 2.90\n2:  ENSG00000000005.5  940.50           890.50              0 0.00\n3: ENSG00000000419.12  956.87           906.87             58 9.89\n4: ENSG00000000457.13 6364.00          6314.00             66 1.62\n5: ENSG00000000460.16 2661.00          2611.00             15 0.89\n</code></pre>\n\n<p>The <code>length</code> and <code>effective_length</code> are defined per gene (and its transcripts) and should not differ across samples. Below is from the <a href=""http://deweylab.biostat.wisc.edu/rsem/rsem-calculate-expression.html"" rel=""nofollow noreferrer"">manual</a>:</p>\n\n<blockquote>\n  <p>A gene's 'length' and 'effective_length' are defined as the weighted\n  average of its transcripts' lengths and effective lengths (weighted by\n  'IsoPct')</p>\n</blockquote>\n\n<p>I would appreciate any hints pointing out what I am missing.</p>\n\n<p>* The script I am using:</p>\n\n<pre><code>for file in ...; \\ndo rsem-calculate-expression \\n-p 8 \\n--star \\n# --output-genome-bam \\n--star-path path-to-star-executable \\n--star-gzipped-read-file \\n--forward-prob=0 \\n<span class=""math-container"">$file \\npath-to-genome \ # should end with the string used for the ""reference_name"" parameter of ""rsem-prepare-reference""\n$</span>{file:11:7}; \ # sample prefix\ndone\n</code></pre>\n\n<p>EDIT: For those who will be interested, <a href=""https://groups.google.com/d/msg/rsem-users/gDh0OBLK6_4/z-3ZoETxn5sJ"" rel=""nofollow noreferrer"">this entry</a> from the RSEM Users Google Group demonstrates how to calculate gene length from weighted transcript length.</p>\n","3541","","3541","2020-01-08T14:19:46.073","2020-01-08T14:19:46.073","Why I am getting different gene lengths for the same genes in different samples with rsem-calculate-expression?","<star><rsem>","1","0","",""
"11107","2","","11106","2020-01-08T13:27:32.387","2","","<p>If there are differences in the expression of individual transcripts between samples then both the length and effective length will differ between samples. It is commonly the case that the ratio of longer and short transcript expression varies a bit between samples, which gives rise to this.</p>\n","77","","","","2020-01-08T13:27:32.387","","","","0","",""
"11108","1","","","2020-01-08T15:18:24.273","3","51","<p>I am mapping kmers back to a few bacterial genomes using <code>bwa fastmap</code>:</p>\n\n<pre><code>bwa fastmap -l 9 ref.fasta kmers.fasta &gt; out.fastmap                                           \n[M::bwa_idx_load_from_disk] read 0 ALT contigs                                                                                                                                                                     \n[main] Version: 0.7.17-r1188                                                                                                                                                                                       \n[main] CMD: bwa fastmap -l 9 ref.fasta kmers.fasta                                                                                                                   \n[main] Real time: 0.168 sec; CPU: 0.157 sec\n</code></pre>\n\n<p>The output looks like this:</p>\n\n<pre><code>SQ      AAAAGTAGTAAGCAGGAAGACAACACGGTTG 31\nEM      0       11      1       contig1:+2041368\nEM      3       13      2       contig1:+1875252        contig1:-2474744\nEM      4       15      1       contig1:+3779779\nEM      5       16      1       contig1:-3253348\nEM      6       17      2       contig1:+938710 contig1:+1066682\nEM      7       20      1       contig1:-4912797\nEM      10      21      4       contig1:+4803473        contig1:-2830252        contig1:+4495150        contig1:-4907283\nEM      11      23      1       contig1:-3148132\nEM      13      24      1       contig1:-4651172\nEM      14      25      3       contig1:-2142223        contig1:+4994474        contig1:+873066\nEM      16      26      4       contig1:+156775 contig1:+27749  contig1:+2207492        contig1:-1340811\nEM      17      29      1       contig1:+3523989\nEM      19      30      24      *\n\nEM      20      31      3       contig1:-2533354        contig1:-208660 contig1:-1080177\n//\n</code></pre>\n\n<p>My question is: what is the meaning of the <code>*</code> char in the row before last? Why is <code>bwa fastmap</code> not reporting all the hits as in the other lines? I could not find a page that explains this output anywhere.</p>\n","123","","","","2020-01-09T10:05:18.677","What is the meaning of the ""*"" character in bwa fastmap's output?","<sequence-alignment><bwa>","1","0","",""
"11109","2","","11108","2020-01-09T00:25:34.623","3","","<p>It means the read is unmapped. No origin in the reference could be found that was sufficiently similar to the read to call it a proper alignment.</p>\n\n<p>In the <a href=""http://samtools.github.io/hts-specs/SAMv1.pdf"" rel=""nofollow noreferrer"">SAM specification</a>, <code>*</code> is always used if the information for that field is not available. Therefore, if no mapping information is available (=unmapped) one sets <code>*</code>. </p>\n","3051","","298","2020-01-09T10:05:18.677","2020-01-09T10:05:18.677","","","","0","",""
"11110","1","11111","","2020-01-09T12:30:53.240","-2","47","<p>I have two dataframes</p>\n\n<pre><code>&gt; head(a[1:4,1:4])\n   Tumor_Sample_Barcode Chromosome Position End_Position\n1:                   A1       chr4 90169866     90169866\n2:                   A1      chr11 60235747     60235747\n3:                   A1       chr1   983023       983023\n4:                   A1       chr1 11346060     11346060\n&gt; \n\n&gt; head(c)\n   Chromosome Position      VAF\n1:       chrM     6691 0.610284\n2:       chrM    14503 0.693325\n3:       chr1 31412236 0.645161\n4:       chr1 55693305 0.602941\n5:       chr1 69963412 0.709302\n6:       chr1 72608266 0.720000\n&gt;\n\n&gt; str(c)\nClasses ‘data.table’ and 'data.frame':  44175 obs. of  3 variables:\n <span class=""math-container"">$ Chromosome: chr  ""chrM"" ""chrM"" ""chr1"" ""chr1"" ...\n $</span> Position  : num  6691 14503 31412236 55693305 69963412 ...\n $ VAF       : num  0.61 0.693 0.645 0.603 0.709 ...\n - attr(*, "".internal.selfref"")=&lt;externalptr&gt; \n</code></pre>\n\n<p>I want to merge them like this but complaining about not being integer</p>\n\n<pre><code>&gt; a=a[c, on = c(""Chromosome"",""Position""), VAF := i.VAF]\nError in bmerge(i, x, leftcols, rightcols, roll, rollends, nomatch, mult,  : \n  Incompatible join types: x.Position (character) and i.Position (integer)\n</code></pre>\n\n<p>I have tried something like these but not working</p>\n\n<pre><code>&gt; c=c[!is.na(as.numeric(as.character(c<span class=""math-container"">$Position))), ]\n&gt; \n&gt; c=c[!is.na(as.numeric(as.character(c$</span>VAF))), ]\n&gt; \n&gt; c<span class=""math-container"">$VAF &lt;- as.numeric(as.character(c$</span>VAF)) \n\nc<span class=""math-container"">$Position &lt;- as.numeric(as.character(c$</span>Position)) \n</code></pre>\n\n<p>which not working</p>\n","4595","","","","2020-01-09T14:25:50.377","Making some columns if a dataframe numeric","<r>","1","2","",""
"11111","2","","11110","2020-01-09T14:25:50.377","2","","<p>Your issue seems to be related to the column <code>Position</code> being a character column in <code>a</code>, you should try:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(data.table)\na$Position &lt;- as.numeric(a$Position)\na[c, on = c(""Chromosome"",""Position""), VAF := i.VAF]\n</code></pre>\n","6437","","","","2020-01-09T14:25:50.377","","","","0","",""
"11113","1","11116","","2020-01-09T14:47:55.620","1","65","<p>I have generated a list of canonical markers for cluster 0 using the following command:</p>\n\n<pre><code>cluster0_canonical &lt;- FindMarkers(project, ident.1=0,  ident.2=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), grouping.var = ""status"", \nmin.pct = 0.25, print.bar = FALSE)\n</code></pre>\n\n<p>And I got the following output, but I do not understand why the canonically expressed <strong>ribosomal</strong> markers seem to be expressed at high levels in both the cells within the cluster AND cells that are NOT in the cluster (i.e. both pct.1 and pct.2 are close to 1, but P value is highly significant). We would not expect this for canonical markers - we would instead expect the genes to be expressed at high levels in the cells within the cluster but at low levels in cells not within the cluster. I don't seem to have such an issue with non-ribosomal genes within cluster 0 and cluster 0 is the only cluster where I get a high number of ribosomal genes as canonical markers.</p>\n\n<p><a href=""https://i.stack.imgur.com/gtJs4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gtJs4.png"" alt=""enter image description here""></a></p>\n","4621","","181","2020-01-09T17:05:16.613","2020-01-09T17:05:16.613","Seurat FindMarkers() output, percentage","<seurat>","1","0","",""
"11115","1","","","2020-01-09T16:20:47.123","1","52","<p>I want to convert my gff3 annotation files to genbank format for use in Mauve. I found the seqret tool here <a href=""https://www.ebi.ac.uk/Tools/sfc/emboss_seqret/"" rel=""nofollow noreferrer"">https://www.ebi.ac.uk/Tools/sfc/emboss_seqret/</a> which can perform this task, but my files (bacterial genomes) are too big.</p>\n\n<p>Can anyone suggest a simple method to make this conversion?</p>\n","6738","","","","2020-01-14T17:07:14.633","Existing tool for converting gff3 to genbank (gbk)","<format-conversion><gff3><genbank>","2","2","",""
"11116","2","","11113","2020-01-09T16:57:56.690","2","","<p>Your interpretation of <code>pct.1</code> and <code>pct.2</code> is misguided. If <code>pct.1 == 1</code>, the given gene was detected in all of the cells. This carries no information about high expression or low expression in the cell, just that it was detected. <code>avg_logFC</code> is the indication of expression <strong>levels</strong>. So your expectation, that </p>\n\n<blockquote>\n  <p>""we would instead expect the genes to be expressed at high levels in the cells within the cluster but at low levels in cells not within the cluster""</p>\n</blockquote>\n\n<p>holds true, as indicated by the positive <code>avg_logFC</code> and significant p-values of the listed RP genes.</p>\n\n<p>Seurat use nautral log, so the FC of RPS6 in cluster 0 vs. all other clusters indicated is 2.718281828459^.55947=1.750.</p>\n\n<hr>\n\n<p>Seurat <code>FindMarkers()</code> <a href=""https://www.rdocumentation.org/packages/Seurat/versions/3.1.2/topics/FindMarkers"" rel=""nofollow noreferrer"">documentation</a>.</p>\n","181","","","","2020-01-09T16:57:56.690","","","","0","",""
"11117","1","11119","","2020-01-09T18:36:47.323","0","59","<p>I have a data frame like this</p>\n\n<pre><code>&gt; head(a[1:2,1:4])\n   Tumor_Sample_Barcode Chromosome Position End_Position\n1:                   A1       chr4 90169866     90169866\n2:                   A1      chr11 60235747     60235747\n    &gt; dim(a)\n[1] 753655    134\n&gt;\n&gt; unique(a$Tumor_Sample_Barcode)\n [1] ""A1""  ""A10"" ""A11"" ""A12"" ""A13"" ""A14"" ""A15"" ""A16"" ""A17"" ""A18"" ""A19"" ""A2""  ""A20"" ""A21"" ""A22"" ""A23"" ""A24"" ""A25"" ""A26"" ""A27"" ""A28""\n[22] ""A29"" ""A3""  ""A30"" ""A31"" ""A32"" ""A33"" ""A4""  ""A5""  ""A6""  ""A7""  ""A8""  ""A9"" \n</code></pre>\n\n<p>I want to change <code>a$Tumor_Sample_Barcode</code>  from <strong><em>A1:A21</em></strong> to <strong><em>res1:res21</em></strong> but I don't how to do that</p>\n","4595","","131","2020-01-10T15:52:22.353","2020-01-10T15:52:22.353","Replace a prefix of a string","<r>","2","0","",""
"11118","1","","","2020-01-09T19:12:21.117","2","40","<p>I'm building a somatic variant calling pipeline with <code>snakemake</code>. The pipeline is hosted on Github, and I'd like to use Travis CI for continuous integration testing. Due to privacy concerns, I am looking for a publicly available tumor-normal dataset similar to Genome in a Bottle's NA12878 dataset that I can place online without privacy concerns and use for integration testing. Any sources to find tumor-normal FASTQ data that is publicly available?</p>\n","6674","","","","2020-01-09T19:12:21.117","NA12878 equivalent for somatic variant calling","<variant-calling><snakemake><gatk><public-dataset>","0","3","2020-01-14T08:08:12.453",""
"11119","2","","11117","2020-01-09T20:01:30.590","8","","<p>I think you are looking for <code>gsub</code> function:</p>\n\n<pre class=""lang-r prettyprint-override""><code>a$Tumor_Sample_Barcode &lt;- gsub(""A"",""res"",a$Tumor_Sample_Barcode)\n</code></pre>\n\n<p>If you prefer, you can use the <code>str_replace_all</code> function from <code>stringr</code> package. But it tooks a little bit more time than <code>gsub</code>:</p>\n\n<p><strong>Dummy data</strong></p>\n\n<pre class=""lang-r prettyprint-override""><code>seq = as.factor(paste0(""A"",1:21))\nlibrary(data.table)\ndt &lt;- data.table(Barcode = rep(seq,10000),\n                 Value = rnorm(210000))\n</code></pre>\n\n<p><strong>Test of <code>gsub</code> vs <code>str_replace_all</code></strong></p>\n\n<pre class=""lang-r prettyprint-override""><code>&gt; system.time(dt<span class=""math-container"">$Barcode &lt;- gsub(""A"",""res"",dt$</span>Barcode))\n   user  system elapsed \n  0.077   0.000   0.077 \n\n&gt; system.time(dt<span class=""math-container"">$Barcode &lt;- str_replace_all(dt$</span>Barcode,""A"",""res""))\n   user  system elapsed \n  0.123   0.004   0.126 \n<span class=""math-container"">```</span>\n</code></pre>\n","6437","","6437","2020-01-09T20:26:55.773","2020-01-09T20:26:55.773","","","","0","",""
"11120","2","","11117","2020-01-10T10:04:06.367","7","","<p>Other options:</p>\n\n<ul>\n<li>Drop 1st character and prefix with <code>""res""</code></li>\n<li>We get 5x speed up if we set <code>fixed = TRUE</code> for <em>gsub</em>.   </li>\n</ul>\n\n<pre><code>microbenchmark::microbenchmark(\n  # dc37 answer\n  gsub(""A"",""res"", dt<span class=""math-container"">$Barcode),\n  stringr::str_replace_all(dt$</span>Barcode, ""A"", ""res""),\n  # other options\n  paste0(""res"", substring(dt<span class=""math-container"">$Barcode, 2)),\n  gsub(""A"",""res"",dt$</span>Barcode, fixed = TRUE),\n\n  # bench set up\n  check = ""identical"", \n  unit = ""relative""\n  )\n\nUnit: relative\n                                             expr      min       lq     mean   median       uq      max neval\n                     gsub(""A"", ""res"", dt<span class=""math-container"">$Barcode) 5.171682 5.132816 4.283014 5.033968 3.177772 2.545173   100\n stringr::str_replace_all(dt$</span>Barcode, ""A"", ""res"") 6.914598 6.870849 5.747396 6.728126 4.809257 3.518024   100\n          paste0(""res"", substring(dt<span class=""math-container"">$Barcode, 2)) 1.983757 1.991876 1.779557 1.968488 1.480892 3.415619   100\n       gsub(""A"", ""res"", dt$</span>Barcode, fixed = TRUE) 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000   100\n</code></pre>\n","131","","","","2020-01-10T10:04:06.367","","","","0","",""
"11121","1","11123","","2020-01-10T14:26:01.717","-1","37","<p>I ran module preservation analysis between two condition and on the basis of preservation z summary i would to go for downstream analysis.So I m interested in looking for hub-genes of those modules which have lowest z summary. But so far in the tutorial i didn't find a section in the WGCNA tutorial where how to find hubgenes after module preservation analysis was mentioned.</p>\n\n<p>Any suggestion would be really appreciated </p>\n","285","","","","2020-01-10T19:57:48.983","Module preservation and hub genes finding","<networks><wgcna>","1","0","",""
"11122","2","","11090","2020-01-10T18:04:13.090","0","","<p><a href=""https://odelaneau.github.io/shapeit4/"" rel=""nofollow noreferrer"">shapeit4</a> is currently (probably) the most accurate and quickest phasing algorithm which can account for sequencing reads in the model. They suggest you extract the phase information from the reads using <a href=""https://whatshap.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">WhatsHap</a> first however. The guide for WhatsHap can be found <a href=""https://whatshap.readthedocs.io/en/latest/guide.html"" rel=""nofollow noreferrer"">here</a> and has some info on using data processed using GATK. </p>\n","3694","","","","2020-01-10T18:04:13.090","","","","0","",""
"11123","2","","11121","2020-01-10T19:57:48.983","1","","<p>Simply select the module(s) you are interested in and look for hub genes using the standard calculations. Having done a module preservation analysis does not change the procedure in the slightest.</p>\n","851","","","","2020-01-10T19:57:48.983","","","","3","",""
"11125","2","","11093","2020-01-11T19:02:35.177","0","","<p>first of all there's no way to accurately determine the start/end position of shared DNA segments when using microarray data from DTC's like 23andMe and Ancestry.</p>\n\n<p>INDEL's are indeed important when it comes to identify shared segments though I'm not sure how often the change over the course of several generations.</p>\n\n<p>My mum's sister has an INDEL that both my mum and her brother didn't inherit, it will be inherited by her children (my 1st cousins).</p>\n\n<p>Unfortunately the microarray from 23andMe doesn't give you any detailed information of than DI, II and II but not how long the deletion or insert is.</p>\n\n<p>To determine such it requires to use WGS data.</p>\n","6743","","","","2020-01-11T19:02:35.177","","","","3","",""
"11126","2","","933","2020-01-12T18:14:07.807","2","","<p>I faced same issue recently, I found those link and python script:</p>\n\n<p>gnomAD GraphQL api\n<a href=""https://gnomad.broadinstitute.org/api"" rel=""nofollow noreferrer"">https://gnomad.broadinstitute.org/api</a>\nIt works great but it is a kind of different query language. \nPlease check here for the docs: <a href=""https://graphql.org/learn/queries/"" rel=""nofollow noreferrer"">https://graphql.org/learn/queries/</a></p>\n\n<p>gnomAD Python Api\n<a href=""https://github.com/furkanmtorun/gnomad_python_api"" rel=""nofollow noreferrer"">https://github.com/furkanmtorun/gnomad_python_api</a></p>\n","6754","","","","2020-01-12T18:14:07.807","","","","0","",""
"11127","1","","","2020-01-13T13:50:41.243","3","28","<p>A given sequencing machine assigns a 'certainty' to each base call in the form of a per-base quality score (FASTQ format).</p>\n\n<p>I have reads from several machines aligned to the corresponding reference (BAM format or similar). Ignoring SNPs and misalignments (for simplicity), mismatches should represent sequencing errors.</p>\n\n<p>I'd like to parse through the FASTQ and BAM file and evaluate how closely the per-base quality score reflects 'true' sequencing errors.</p>\n\n<p>Is there an existing efficient, user-friendly, open source, well documented tool to do this? Failing that, is there a paper doing this that claims to have made the tool freely available?</p>\n\n<p>Another method would be to look at bases corrected using kmer analysis or similar. Again, the per-base quality score can be compared to the 'true' sequencing errors.</p>\n\n<p>Any tool to do that? I guess it could be baked into an existing error correction tool.</p>\n\n<p>Do short read aligners actually make use of per-base quality score?</p>\n\n<p>Many thanks,</p>\n","3233","","","","2020-01-13T20:56:02.023","Tool for calibrating base quality scores?","<bam><fastq><quality-control>","1","1","",""
"11128","2","","11115","2020-01-13T13:55:52.120","0","","<p>You can use BioPerl to do this. Read in the GFF and make gene objects and write out those gene objects in Genbank format.</p>\n\n<p>Perhaps this tool (doing the opposite) will get you started:\n<a href=""https://metacpan.org/pod/bp_genbank2gff3.pl"" rel=""nofollow noreferrer"">https://metacpan.org/pod/bp_genbank2gff3.pl</a></p>\n\n<p>Here is a script doing what you ask but it doesn't use any libraries (so is probably buggy):\n<a href=""https://gist.github.com/avrilcoghlan/5386810"" rel=""nofollow noreferrer"">https://gist.github.com/avrilcoghlan/5386810</a></p>\n\n<p>Hope that helps,\nDan.</p>\n","3233","","","","2020-01-13T13:55:52.120","","","","0","",""
"11129","2","","11100","2020-01-13T14:02:56.187","0","","<p>You need to run a SNP finding pipeline, filter for SNPs in those genes and then run a SNP annotation pipeline. (You could annotate before you filter but it will take longer).</p>\n\n<p>SNP finding usually has three steps, 1) read alignment to reference, 2) SNP calling and 3) SNP filtering (by quality).</p>\n\n<p>You then bring in gene annotation data to filter SNPs to just those genes.</p>\n\n<p>You then run SNP annotation using tools like VEP, SIFT, PolyPhen, etc.</p>\n\n<p>People use tools like GATK to do this.</p>\n\n<p>Hopefully that's enough information to get you started.</p>\n","3233","","","","2020-01-13T14:02:56.187","","","","0","",""
"11130","2","","11093","2020-01-13T14:16:45.127","1","","<p>InDels are just as informative, but are less reliable to call. Errors should be averaged out in this kind of analysis though (so long as they don't predominate).</p>\n\n<p>Both SNPs and InDels are valid data points to consider when comparing triads, but identifying IBD segments will only ever be probabilistic.</p>\n","3233","","","","2020-01-13T14:16:45.127","","","","0","",""
"11131","1","11148","","2020-01-13T16:53:02.027","2","44","<p>I would like to create a simple QC check of RNA-Seq data that simply maps the data to rRNA and then counts the number of reads that map.</p>\n\n<p>I've done this manually for human using sequences for 5S, 5.8S, 18S, and 28S rRNA (rDNA, actually).  I gathered those sequences manually to create a small fasta file to use as a reference and it worked well.</p>\n\n<p>I would like to have a more definitive source for this reference though, and I don't necessarily want to use a species-specific database, so I thought that using a fasta file downloaded from <a href=""https://www.arb-silva.de/no_cache/download/archive/release_138/Exports/"" rel=""nofollow noreferrer"">SILVA</a> would be great, but I don't know if I'm using the wrong fasta file or what.  I'm not very familiar with SILVA, but I noticed that when I tested it using the same sequencing data, I got unexpectedly many fewer hits than I did with my manually created 4-sequence reference than with the SILVA NR99 fasta file.</p>\n\n<p>So I blasted my 4 rDNA sequences against the SILVA NR99 data and only the 18S was found.  The others (5S, 5.8S, and 28S) don't appear to be there.  Am I misunderstanding the nature of the SILVA project or could I be doing something wrong?</p>\n\n<p><em>UPDATE</em>: Please note, this question is about <strong>where to find rRNA and/or rDNA sequences to use as a reference</strong>, not about tools that can be employed in quantifying rRNA read content.  While I appreciate hearing about tools that can generate nice reports about rRNA reads in an RNA-Seq run, the question is not answered unless it addresses the missing rRNA/rDNA sequences that are necessary for comprehensive quantification.  Any tool can count rRNA/rDNA mappings, but the results will only be as good as the rRNA reference data supplied (unless it is a reference-less tool).  And it's the lack of 5S, 18S, and 28S ribosomal subunits in SILVA that is my main concern.  It's the difference between 1k reads and 500k reads mapping to rRNA/rDNA sequences.</p>\n","5063","","5063","2020-01-15T22:17:07.970","2020-01-16T21:02:05.700","Ribosomal RNA QC quantification using SILVA","<ribosomal>","3","10","",""
"11132","1","11136","","2020-01-13T19:57:38.370","0","28","<p>Before I do this myself...</p>\n\n<p>Is there any compiled batch-corrected dataset of the major scRNA-seq atlases (Mouse Cell Atlas, Mouse Organogenesis Atlas, Mouse Gastrulation Atlas, Tabula Muris, Tabula Muris Senis, etc.) with processed spliced AND unspliced count matrices?</p>\n\n<p>I would be surprised if there is, but in the event there isn't, are there new pipelines in development that will make processing of raw data easier for studying RNA velocity?</p>\n","6498","","","","2020-01-14T03:45:55.333","Processed spliced and unspliced count matrices for existing scRNA-seq atlases","<r><python><scrnaseq>","1","0","",""
"11133","2","","11127","2020-01-13T20:56:02.023","1","","<p>I think what you are looking for is GATK's Base Quality Score Recalibration.  Their <a href=""https://gatk.broadinstitute.org/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR-"" rel=""nofollow noreferrer"">documentation</a> is great so I won't rewrite it here.  </p>\n\n<blockquote>\n  <p>BQSR stands for Base Quality Score Recalibration. In a nutshell, it is\n  a data pre-processing step that detects systematic errors made by the\n  sequencing machine when it estimates the accuracy of each base call.</p>\n</blockquote>\n","964","","","","2020-01-13T20:56:02.023","","","","4","",""
"11134","5","","","2020-01-13T21:05:48.940","0","","","-1","","-1","2020-01-13T21:05:48.940","2020-01-13T21:05:48.940","","","","0","",""
"11135","4","","","2020-01-13T21:05:48.940","0","","pybedtools wraps and extends BEDTools and offers feature-level manipulations from within Python. ","964","","964","2020-01-22T14:40:32.600","2020-01-22T14:40:32.600","","","","0","",""
"11136","2","","11132","2020-01-14T03:45:55.333","1","","<p>I am not sure if there are preprocessed datasets available, however, <a href=""https://www.kallistobus.tools/velocity_tutorial.html"" rel=""nofollow noreferrer"">Kallisto + Bustools</a> was designed specifically for the rapid quantification of spliced and unspliced transcripts. </p>\n","64","","","","2020-01-14T03:45:55.333","","","","1","",""
"11137","2","","6804","2020-01-14T10:54:25.777","0","","<p>Although @JamesHawley answer was informative, I don't think it solved the initial issue requested by the OP. (<strong>edit</strong>: or maybe it partially did, I'm not sure as I got a bit confused with the wildcards on the initial question/answer. I'm anyway leaving this answer here, in case another way of explaining it is useful for others)</p>\n\n<h3>Solution</h3>\n\n<p>I think you only need to change the output of your <code>rule multiqc</code> from:</p>\n\n<p><code>os.path.join(qc_dir, 'multiqc_report.html')</code></p>\n\n<p>to</p>\n\n<p><code>os.path.join(qc_dir, '{sample}.html')</code></p>\n\n<p>This seemed counter-intuitive to me initially, but I think what's happening is that <code>snakemake</code> will look at <code>rule all</code> and see that you want an output called ""qc/multiqc_report.html"". And the only place where it can find something of that sort is in <code>rule multiqc</code>, by replacing the wildcard <code>{sample}</code> by the string <code>multiqc_report</code>. </p>\n\n<p>It felt confusing to me, because the same <code>{sample}</code> wildcard is used in other rules (namely the rules you use to generate the outputs in your <code>analysed_dir</code>). But <code>snakemake</code> will be smart to replace <code>{sample}</code> with whatever it needs to recreate the output files you're requesting. </p>\n\n<hr>\n\n<h3>Longer explanation</h3>\n\n<p>Here is a (hopefully) reproducible example, if it helps.</p>\n\n<p>First create some fake directory structure with some files:</p>\n\n<pre class=""lang-sh prettyprint-override""><code>mkdir inputs\nmkdir analysed_dir\nmkdir qc_dir\n\ntouch inputs/file1.txt\ntouch inputs/file2.txt\n</code></pre>\n\n<p>Then, this would be the snakefile:</p>\n\n<pre class=""lang-py prettyprint-override""><code># Define your variables, fetch things from config, or csv, etc...\ninput_dir = ""inputs""\noutput_dir = ""outputs""\nsamples = [""file1.txt"", ""file2.txt""]\n\n\n# Rule all defining all the output files required\nrule all:\n  input:\n    expand(""analysed_dir/{samples}"", samples = samples),\n    ""qc_dir/multi_qc_report.html""\n\n# this rule is equivalent to whichever rules process your data individually\nrule analyse_each_file:\n  input:\n    ""inputs/{sample}.txt""\n  output:\n    ""analysed_dir/{sample}.txt""\n  shell:\n    ""cat {input} &gt; {output}""\n\n# This rule is equivalent to your multiqc step where multiple files result in a single output\nrule multiqc:\n  input:\n    expand(""analysed_dir/{samples}"", samples = samples)\n  output:\n    ""qc_dir/{sample}.html""\n  shell:\n    ""cat {input} &gt; {output}""\n</code></pre>\n\n<p>to generate this pipeline:</p>\n\n<p><a href=""https://i.stack.imgur.com/9LnDy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9LnDy.png"" alt=""dag""></a></p>\n\n<p>And then issuing:</p>\n\n<p><code>snakemake --cluster ""qsub -N {wildcards.sample}-{rule}"" --jobs 10</code></p>\n\n<p>Generates the following job names:</p>\n\n<ul>\n<li>file1-analyse_each_file</li>\n<li>file2-analyse_each_file</li>\n<li>multi_qc_report-multiqc</li>\n</ul>\n\n<p><strong>Edit with further explanation:</strong> </p>\n\n<p>Notice in the DAG how the <code>sample</code> wildcard takes different values for the different rules. I think the snakemake logic is as follows:</p>\n\n<ul>\n<li>Starting with <strong><code>rule all</code></strong> I see that you want three files as input (the first two come from the <code>expand()</code> call): \n\n<ul>\n<li><code>analysed_dir/file1.txt</code></li>\n<li><code>analysed_dir/file2.txt</code></li>\n<li><code>qc_dir/multi_qc_report.html</code></li>\n</ul></li>\n<li>Let me start with the first one <code>analysed_dir/file1.txt</code>. The file doesn't exist, so let me look in the other rules and see if I can find it in their output. Actually in <strong><code>rule analyse_each_file</code></strong> I see the output is set to <code>""analysed_dir/{sample}.txt""</code>, so if I replace <code>{sample}</code> by <code>file1.txt</code> I can recreate your file from it. Therefore, the input for that rule will be <code>""inputs/file1.txt""</code> (and this file already exists, so I can issue this job).</li>\n<li>For the second file <code>analysed_dir/file2.txt</code>, well it's a similar case as with the previous one, so I'll issue a job with the same <strong><code>rule analyse_each_file</code></strong>, but this time <code>{sample}</code> = <code>file2.txt</code>.</li>\n<li>Finally, you want a file called <code>qc_dir/multi_qc_report.html</code>. Where can I create such a file from? Well, the output of <strong><code>rule multiqc</code></strong> is set to <code>""qc_dir/{sample}.html""</code>, so if I make <code>{sample}</code> = <code>multi_qc_report</code>, then I can issue the job and produce your file of interest. In this case, the rule takes as input two files <code>""analysed_dir/file1.txt""</code> and <code>""analysed_dir/file2.txt""</code>, which I already know how to create from another rule. </li>\n</ul>\n\n<p>The key thing to realise is that <code>snakemake</code> <strong>defines the wildcard <code>{sample}</code> for each rule separately</strong>. So, in this example, <code>wildcard.sample</code> in <code>rule analyse_each_file</code> does not take on the same values as in <code>rule multiqc</code>.</p>\n","6760","","6760","2020-01-16T11:23:18.193","2020-01-16T11:23:18.193","","","","2","",""
"11138","2","","7456","2020-01-14T13:12:20.430","1","","<p>You said that you want to compare with other microbial genome data. Have you already annotated your genome? If not, you might consider a pipeline I've developed, I can gladly help if you face some problems using it.</p>\n\n<p>It is called <a href=""https://github.com/fmalmeida/bacannot"" rel=""nofollow noreferrer"">bacannot</a>, it does a complete annotation using Prokka, Kofam, CARD, VFDB, and other resources. It might help you to have a first scratch of your data since it also collapses annotation information in a genome browser</p>\n","6767","","","","2020-01-14T13:12:20.430","","","","0","",""
"11139","2","","4367","2020-01-14T17:02:01.170","1","","<p><strong>singleCellNet</strong> is a computational method using supervised machine learning for <strong>quantitatively</strong> <strong>cell type annotation</strong>. singleCellNet also enables <strong>cross-platform</strong> and <strong>cross-species</strong> comparison. It is also actively-supported. </p>\n\n<p>Here is the github <a href=""https://github.com/pcahan1/singleCellNet"" rel=""nofollow noreferrer"">https://github.com/pcahan1/singleCellNet</a>\n, and the vignette <a href=""https://pcahan1.github.io/singleCellNet/"" rel=""nofollow noreferrer"">https://pcahan1.github.io/singleCellNet/</a></p>\n","2993","","","","2020-01-14T17:02:01.170","","","","0","",""
"11140","2","","11115","2020-01-14T17:07:14.633","1","","<p>I believe <code>seqret</code> is already the simplest approach. If you install the emboss command line utility, genome (file) sizes will not be a problem. You can even install it with conda, see <a href=""https://anaconda.org/bioconda/emboss"" rel=""nofollow noreferrer"">https://anaconda.org/bioconda/emboss</a>.</p>\n\n<p>After that, you can use it as the example:</p>\n\n<pre><code>seqret -sequence {genome file} -feature -fformat gff -fopenfile {gff file} -osformat genbank -osname_outseq {output prefix} -ofdirectory_outseq gbk_file -auto\n</code></pre>\n\n<p>Hope it helps</p>\n","6767","","","","2020-01-14T17:07:14.633","","","","0","",""
"11141","1","11142","","2020-01-14T23:17:51.827","3","34","<p><a href=""http://www.htslib.org/doc/samtools-phase.1.html"" rel=""nofollow noreferrer"">samtools phase</a> is an easy-to-use tool to phase variants from a bam file and a reference.</p>\n\n<p>For a sequence that has two haplotypes, like this:</p>\n\n<pre><code>   position: 123456789\nhaplotype 0: GACCATATT\n       SNPs:  **    *\nhaplotype 1: GTACATACT\n</code></pre>\n\n<p>The output looks like this:</p>\n\n<pre><code>CC  M?   chr    PS     pos  allele0 allele1 hetIndex #supports0 #errors0 #supp1 #err1\nCC\nCC\nPS      frame-1   2    9\nM1      frame-1  60    2      A       T       1       3       0       23      0\nM1      frame-1  60    3      C       A       2       35      1       24      0\nM1      frame-1  60    8      T       C       3       46      0       30      0\n</code></pre>\n\n<p>How can I take the reference fasta file and the output of <code>samtools phase</code> to generate one <code>fasta</code> file for each of the haplotypes?</p>\n","2085","","2085","2020-01-14T23:25:04.617","2020-01-20T12:51:38.150","How to generate two fasta files from samtools phase output?","<bam><fasta><samtools><phase>","1","0","",""
"11142","2","","11141","2020-01-14T23:17:51.827","3","","<p><strong>Answer January 20th, 2020.</strong> If you run <code>samtools phase</code> with the option <code>-b my_prefix</code> it outputs a bam file for both haplotypes, named <code>my_prefix.0.bam</code> and <code>my_prefix.1.bam</code>. If the sequence has been correctly phased, use these bam files to correct the reference using <a href=""https://github.com/broadinstitute/pilon"" rel=""nofollow noreferrer"">pilon</a>. This is preferable to the method described in the <strong>January 14th, 2020</strong> answer because pilon will correct SNPs and indels, whereas the python program below will only correct the specific SNPs that <code>samtools phase</code> found. <code>samtools phase</code> does not find indels.</p>\n\n<p><strong>Answer January 14th, 2020.</strong> This python program below takes a fasta file as an argument and the output of samtools phase piped in, then prints out a fasta file of the two haplotypes.</p>\n\n<p>Run the program like this: </p>\n\n<p><code>samtools phase reads.bam | phase2fasta.py ref.fa &gt; haplotypes.fasta</code>.</p>\n\n<p>Here is the code of <code>phase2fasta.py</code>:</p>\n\n<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python\n\nimport os\nimport sys\n\nif (len(sys.argv) == 1) or not os.path.exists(sys.argv[1]):\n    raise Exception(""The first and only argument is the reference fasta."")\nallele0 = []\nseqname = """"\nwith open(sys.argv[1]) as f:\n    for line in f:\n        if line[0] == ""&gt;"":\n            seqname = line[1:].split("" "")[0].strip()\n        else:\n            for c in line.strip().upper():\n                allele0.append(c)\nallele1 = [x for x in allele0]\nfor line in sys.stdin:\n    if line[0:2] == ""M1"":\n        fields = line.strip().split()\n        if fields[1] == seqname:\n            pos = int(fields[3]) - 1\n            allele0[pos] = fields[4]\n            allele1[pos] = fields[5]\nprint(""&gt;{}_allele0"".format(seqname))\nprint("""".join(allele0))\nprint(""&gt;{}_allele1"".format(seqname))\nprint("""".join(allele1))\n</code></pre>\n","2085","","2085","2020-01-20T12:51:38.150","2020-01-20T12:51:38.150","","","","1","",""
"11143","1","","","2020-01-15T05:40:41.560","0","38","<p>I am trying to use ComplexHeatmap to plot heatmap with annotation. \nThe code that I  have tried </p>\n\n<pre><code>library(ComplexHeatmap)\nlibrary(circlize)\n\nset.seed(123)\nmat = matrix(rnorm(25), 5)\nrownames(mat) = paste0(""R"", 1:5)\ncolnames(mat) = paste0(""C"", 1:5)\n\ncol_fun = colorRamp2(c(0, 2, 5), c(""white"", ""blue"", ""red""))\ndata_foo = c(0.1, 3, 1.6, 2.8, 4)\n\nha = HeatmapAnnotation(which = ""row"", foo = data_foo, col = list(foo = col_fun))\nHeatmap(mat, name = ""mat"", right_annotation = ha)\n\nha1 = HeatmapAnnotation(which = ""row"", foo = anno_text(data_foo))\nHeatmap(mat, name = ""mat"", right_annotation = ha1)\n\n</code></pre>\n\n<p>Can I obtain the annotation with colors varying as per the annotation values and also display the values in the annotation? i.e is it possible to combine the annotation in both figures?\n<a href=""https://i.stack.imgur.com/y1DSx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y1DSx.png"" alt=""HeatMap with color annotation""></a></p>\n\n<p><a href=""https://i.stack.imgur.com/0qBYQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0qBYQ.png"" alt=""HeatMap with values as annotation""></a></p>\n","2247","","6437","2020-01-15T15:28:32.700","2020-01-15T15:28:32.700","Heatmap annotation with value and colour gradient","<r><heatmap>","0","3","",""
"11144","1","","","2020-01-15T16:28:44.180","0","53","<p>I am aware of this question <a href=""https://bioinformatics.stackexchange.com/questions/4249/manually-define-clusters-in-seurat-and-determine-marker-genes"">Manually define clusters in Seurat and determine marker genes</a> that is similar but I couldn't make tit work for my use case.</p>\n\n<p>So I have a single cell experiments and the clustering id not great I have a small groups of 6 cells (I know it is extremely small, but nonetheless I would like to make the most of it) that are clearly isolate in UMAP and display marker that I am interresting in. </p>\n\n<p>I managed to get those cells names in barcode (e.g.: AGCCAGCTCGCTTTAT).</p>\n\n<p>I would like to compute differential expression of those cells against all others groups (using FindMarkers or FindAllMarkers).</p>\n\n<p>I tried to manually define the cluster in: \nSeurat_obj@meta.data$seurat_clusters\nBut using FindMarkers (it says this groups don't exist)</p>\n\n<p>then I try to modify \ns_19270BC@active.ident</p>\n\n<p>but then it could not found metadata attached to my newly define group.</p>\n\n<p>I am kind of stuck and any help is warmly welcome</p>\n\n<h1>Code</h1>\n\n<p>My R is quite rusty...</p>\n\n<pre><code>names_ # a vector with the names of the cell I want as a cluster\n\n# my seurat object have 5 cluster 0..4. So I want to add a 6th clusters with id 5\n\n#First attempt manually modify the vector active.ident and cluster\n# they are both factor so we need to add a factor level\nlevels(seurat_obj@active.ident) = c(levels(seurat_obj@active.ident), 5)\nlevels(seurat_obj@meta.data<span class=""math-container"">$seurat_clusters) = c(levels(seurat_obj@meta.data$</span>seurat_clusters), 5)\n# then modify them :\nseurat_obj@active.ident[which(row.names(seurat_obj@meta.data) %in%  names_)] = 5\nseurat_obj@meta.data$seurat_clusters[which(row.names(seurat_obj@meta.data) %in%  names_)] = 5\n\nFindMarkers(s_19270BC, ident.1 = 5)\n</code></pre>\n\n<p>So This morning it works... I don't know what I have done yesterday...</p>\n\n<p>EDIT:\nI added the Code I used</p>\n","6655","","6655","2020-01-16T08:18:53.223","2020-02-15T09:02:45.337","Seurat define manually a cluster and find markers","<r><seurat><differential-expression>","1","0","",""
"11145","2","","11144","2020-01-15T16:49:54.260","1","","<p>It would have been better if you have shared your code as well as the error messages you got format these as such.</p>\n\n<p>As far as I can understand, your problem is that you failed to make your cluster as default ids. For this you can use:</p>\n\n<pre><code>seurat_obj &lt;- SetIdent(seurat_obj, id=seurat_obj@meta.data$column_of_interest).\n</code></pre>\n\n<p>The cluster identities are stored in the <code>@meta.data</code> slot of your object.</p>\n","3541","","","","2020-01-15T16:49:54.260","","","","1","",""
"11148","2","","11131","2020-01-15T19:57:30.487","1","","<p>SILVA is intended for metagenomic samples and thus does not contain the human ribosomal subunits.  While the 5.8S subunit is in the ensembl, ncbi, and ucsc annotations, and there are numerous rRNA ""biotype"" (or ""biomol"") entries, the 5S, 18S, and 28S subunit sequences are absent.  Even downloading rRNA entries in the rmsk table from ucsc will not return all 4 major human subunits.</p>\n\n<p>You can however (as I discovered) download all rDNA gene sequences (including those 4 subunits) from the nucleotide db on ncbi using the search term ""biomol_rrna[PROP]"".  This returns 240,240 coding sequences (or 30,775 gene sequences).  You can then use this as a reference for mapping, blasting, or as input to rRNA QC tools.</p>\n\n<p>The following e-Utilities commands from ncbi will download the gene sequences:</p>\n\n<pre><code>esearch -db nuccore -query ""biomol_rRNA[prop]"" | efetch -format fasta &gt; all_rrnas.fasta\n</code></pre>\n\n<p>This is not a common analysis.  Usually, rRNA reads can just be ignored and analyses are unaffected, which is likely why there is a paucity of records for these sequences in annotation files, but mapping to just these sequences could make for a simple QC measure as a gauge for rRNA contamination.  While the sequences may be present in the genome, they're not in the annotations and may even be masked as repetitive, since there are so many copies.  So you have to be careful to count unique reads and not mappings.</p>\n","5063","","","","2020-01-15T19:57:30.487","","","","0","",""
"11149","2","","11131","2020-01-15T21:57:58.117","0","","<p>I'd recommend using <a href=""https://ccb.jhu.edu/software/kraken2/"" rel=""nofollow noreferrer"">kraken2</a> for this. There's a pre-built kraken2 database for SILVA (16S only) which can be downloaded from here:</p>\n\n<p><a href=""https://ccb.jhu.edu/software/kraken2/downloads.shtml"" rel=""nofollow noreferrer"">https://ccb.jhu.edu/software/kraken2/downloads.shtml</a></p>\n\n<p>For other ribosomal subunits, it's necessary to create a kraken2 database from scratch. 16S/18S subunits are in the SSU database, while 23S/28S are in the LSU database:</p>\n\n<p><a href=""https://www.arb-silva.de/download/arb-files/"" rel=""nofollow noreferrer"">https://www.arb-silva.de/download/arb-files/</a></p>\n\n<p>Note that SILVA is a database of collected rRNA sequences. As far as I'm aware there's no canonical ""human"" 18S or 28S sequence in SILVA, but it includes a collection of multiple 18S &amp; 28S sequences that have been annotated with the human taxon.</p>\n\n<p>Kraken2 has a command-line interface for creating reports:</p>\n\n<pre><code>kraken2 --db &lt;location_of_database&gt; --gzip-compressed input.fastq.gz --report output.kraken.report.txt &gt; output.kraken.txt\n</code></pre>\n\n<p><a href=""https://ccb.jhu.edu/software/pavian/"" rel=""nofollow noreferrer"">Pavian</a> can be used for visualising the results and creating proportion tables.</p>\n","73","","73","2020-01-16T04:37:25.810","2020-01-16T04:37:25.810","","","","4","",""
"11150","1","","","2020-01-15T21:58:11.953","0","25","<p>I am using Seurat to analyze my single cell data. I have 2 conditions, treated and untreated.  I am trying to create a stacked bar graph in order to show the differences in cell types for each condition but need to collect the percentages of each cluster for the specific cell types. How to I put together a sheet that contains the percentages of each cluster and the make a stacked bar graph of comparison of each condition. </p>\n","6774","","","","2020-01-15T21:58:11.953","Percentage of each cluster in Seurat","<r><scrnaseq><seurat>","0","0","",""
"11151","1","","","2020-01-16T02:35:35.053","0","8","<p>How do you set QC measurements? Seems very subjective. Is there a right way to set:\n1) individual call rate, 2) SNP call rate, 3) minor allele frequency (MAF), and 4) linkage disequilibrium (LD) in PLINK based on my data? I am working with data from a 50K SNP chip, includes about 200 individuals. </p>\n","6775","","","","2020-01-16T02:35:35.053","Common QC parameters for SNP chip data in PLINK","<snp><quality-control><plink><snp-chip>","0","0","",""
"11152","1","","","2020-01-16T03:24:08.963","0","21","<p>In 2017, Chen et al reported a DNA sequencing technology called error-correction code (ECC) sequencing [1]. They borrowed ideas from communication and coding theory and encoded information redundency in DNA sequencing signals, hence improved sequencing accuracy by detecting and correcting errors.</p>\n\n<p>However, I wonder what kind of ECC is used in this new technology. To my knowledge, in coding theory ECC is roughly classified into two categories [2]:</p>\n\n<ol>\n<li>Block codes, in which the source is divided into packets and each packet is encoded individually;</li>\n<li>Convolutional codes, in which the encoding of each packet is influenced by its previous packet.</li>\n</ol>\n\n<p>But it seems that the ECC sequencing can be put into neither of the two categories: it is certainly not a block code since it uses a Viterbi-like algorithm to decode; But it is not very much like a convolutional code since it uses future signals instead of history signals to correct errors.</p>\n\n<p>So what kind of ECC is used in the ECC sequencing? Is it a new coding scheme?</p>\n\n<h2>Ref</h2>\n\n<p>[1] Chen, Z., Zhou, W., Qiao, S. et al. Highly accurate fluorogenic DNA sequencing with information theory–based error correction. Nat Biotechnol 35, 1170–1178 (2017) doi:10.1038/nbt.3982. <a href=""https://www.nature.com/articles/nbt.3982"" rel=""nofollow noreferrer"">https://www.nature.com/articles/nbt.3982</a></p>\n\n<p>[2] <a href=""https://en.wikipedia.org/wiki/Error_correction_code"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Error_correction_code</a></p>\n","6776","","","","2020-01-16T03:24:08.963","What kind of error-correction code is used in the error-correction code DNA sequencing?","<sequencing><ngs><sequence-analysis><algorithms><base-calling>","0","0","",""
"11154","1","","","2020-01-16T13:29:45.850","1","28","<p>In Strelka vcf in INFO column we have these</p>\n\n<pre><code>Depth=""The number of reads covering the variant position including duplicates, supplementary records and reads that fall below minimum base and mapping quality thresholds.""\n\nReadCount=""The number of reads covering the variant position excluding duplicates, supplementary records and reads that fall below minimum base and mapping quality thresholds.""&gt;\nReadCountControl=""The number of reads covering the variant position in the control excluding duplicates, supplementary records and reads that fall below minimum base and mapping quality thresholds.""&gt;\n</code></pre>\n\n<p>And in FORMAT column we have </p>\n\n<pre><code>AU=""Number of 'A' alleles used in tiers 1,2""&gt;\nCU=""Number of 'C' alleles used in tiers 1,2""&gt;\nDP=""Read depth for tier1 (used+filtered)""&gt;\nGU=""Number of 'G' alleles used in tiers 1,2""&gt;\nTU=""Number of 'T' alleles used in tiers 1,2""&gt;\n</code></pre>\n\n<p>What is the difference of <strong>ReadCount</strong> in INFO column and <strong>DP</strong> in FORMAT column?</p>\n\n<p>What is the difference of <strong>number of allele</strong> versus n<strong>umber of reads covering the variant position</strong> ?</p>\n\n<p>Please help for clarification if you have any intuition on this definitions</p>\n\n<p>Thanks</p>\n","4595","","4595","2020-01-16T13:34:51.457","2020-01-16T13:34:51.457","Helping with some definitions","<vcf><variant-calling><wgs><snv>","0","1","",""
"11155","1","","","2020-01-16T15:45:02.100","-1","36","<p>I have a list of files in which I have this information</p>\n\n<pre><code>Chrom   Position        Ref Alt TumorVAF    NormalVAF\nchr1    56987           CA  C   0.222222    0\nchr1    4133415         AG  A   0.425       0\nchr1    4611030         A   AC  0.36        0\n</code></pre>\n\n<p>This is the name of one of my files</p>\n\n<pre><code>LP6005334-DNA_H01_vs_LP6005333-DNA_H01.passed.somatic.indels.vcf.parsed.txt\n</code></pre>\n\n<p>I have another matched file which has too much information</p>\n\n<p>This is the name of these big files</p>\n\n<pre><code>LP6005334-DNA_H01_vs_LP6005333-DNA_H01.passed.somatic.indels.vcf_fixed_vcf.txt.hg19_multianno.txt\n</code></pre>\n\n<p>As you are seeing, most of the name of txt files are matched</p>\n\n<p>I want to cut the 4th and 5th columns from my small files and add these columns to the matched big files</p>\n\n<p>Manually this is possible but time consuming for a lot of files</p>\n\n<p>Is these any way to do that by coding?</p>\n","4595","","181","2020-01-16T16:35:33.157","2020-01-16T17:13:04.857","Appending some columns from one tabular file to another","<r><bash><loop>","1","1","",""
"11156","1","11174","","2020-01-16T16:37:30.237","0","71","<p>I have R code takes a vcf file returns a parsed .txt output.</p>\n\n<p>I have 21 vcf files in a folder </p>\n\n<p>I named my R code parse_vcf_alt1 function</p>\n","3962","","3962","2020-01-23T11:53:51.477","2020-01-23T11:53:51.477","How I can push a function to act on my files","<r><loop>","3","0","",""
"11157","2","","11155","2020-01-16T16:56:10.367","2","","<blockquote>\n  <p>As you are seeing, most of the name of txt files are matched</p>\n</blockquote>\n\n<p>Our eyes and brains are smarter than computers. We can see patterns that can be difficult to describe. For a computer to be useful in this task, the difference between the two file names must be explicitly defined.</p>\n\n<p>In your example (which seems to be an ANNOVAR-generated output file with a prefix specified during annovar run), the common part seems to be the output of <code>$(basename LP6005334-DNA_H01_vs_LP6005333-DNA_H01.passed.somatic.indels.vcf.parsed.txt .parsed.txt)</code>. If this is true for all files, you could automate that using a loop such as:</p>\n\n<pre><code>for f in *.parsed.txt\ndo\n    other_file=<span class=""math-container"">$(basename ""$</span>f"" .parsed.txt)\n    # Rest of your logic goes here\ndone\n</code></pre>\n\n<p>You could then <code>paste</code> the columns from <code>$f</code> to <code>$other_file</code> like so (not tested):</p>\n\n<pre><code>paste ""<span class=""math-container"">$other_file"" &lt;(cut -f3,4 $</span>f)\n</code></pre>\n\n<p>This is an outline to get to your solution. You should be able to build off this.</p>\n","650","","298","2020-01-16T17:13:04.857","2020-01-16T17:13:04.857","","","","2","",""
"11158","1","","","2020-01-16T18:03:08.230","1","41","<p>What assembler is appropriate for High-Fidelity PacBio reads?</p>\n\n<p>For example, canu is good for high-error PacBio reads. But what algorithm to use for HiFi reads? Would it be OK to use canu without the error-correction step?</p>\n","1075","","57","2020-01-17T14:46:12.283","2020-01-17T14:47:56.043","What assembler is appropriate for High-Fidelity PacBio reads","<assembly><software-recommendation><long-reads><pacbio>","2","0","",""
"11159","2","","11158","2020-01-16T18:29:03.687","3","","<p>As recently <a href=""https://twitter.com/aphillippy/status/1191821832891949056"" rel=""nofollow noreferrer"">tweeted by A. Phillippy</a>, Canu v1.9 now supports HiFi reads. </p>\n\n<p>Therefore you should just read their manual and look for the implementation</p>\n\n<p>:)</p>\n","6767","","57","2020-01-17T14:47:56.043","2020-01-17T14:47:56.043","","","","0","",""
"11160","2","","11156","2020-01-16T19:50:15.470","1","","<p>Why don't you try using <a href=""https://github.com/docopt/docopt.R"" rel=""nofollow noreferrer"">docopt</a> which can make your R script ""understand""  command line parameters.</p>\n\n<p>Doing that, you can create a simple bash loop to execute the script for all vcf files, such as:</p>\n\n<p><code>for files in ""*.vcf"" ; do script.R -i $files -o {output} ; done</code></p>\n\n<p>See this example <a href=""https://github.com/fmalmeida/rscripts/blob/master/parse_sql.R"" rel=""nofollow noreferrer"">here</a></p>\n\n<p>Hope it helps,\nFelipe.</p>\n","6767","","6767","2020-01-16T19:56:10.693","2020-01-16T19:56:10.693","","","","0","",""
"11161","2","","11156","2020-01-16T20:18:53.620","1","","<p>The upper part, where you try to use <code>lapply()</code> over your list of files with your custom function does not quite make sense to me: You declare your <code>files</code> but you do not use this in your <code>lapply()</code> call. I would expect to see something like `lapply(files, your_custom_function).</p>\n\n<p>Moreover, the error you get is results from <code>dplyr::rename()</code> not being able to find <code>#CHROM</code> because <code>readr::read_tsv()</code> could not read the header line properly. In my case the vcf was read as an n x 1 data frame! You could use <code>readr::read_delim()</code> to make sure you are reading the data right.</p>\n\n<p>I would, however, would not try to read a <code>VCF</code> file as an ordinary data frame, there are packages for that, for example <code>[vcfR][1]</code>. Once you create your vcfR object, you can check the different slots:</p>\n\n<blockquote>\n  <p>The vcfR object is an S4 class object with three slots containing the\n  metadata, the fixed data and the genotype data.</p>\n</blockquote>\n","3541","","","","2020-01-16T20:18:53.620","","","","2","",""
"11162","1","11180","","2020-01-16T20:55:32.733","1","27","<p>What algorithms for linear Pseudotime trajectory construction (diffusion-based) are the most scalable to large datasets?</p>\n\n<p>I'm currently using Slingshot based on the recommendation in this manuscript: <a href=""https://www.biorxiv.org/content/biorxiv/early/2018/03/05/276907.full.pdf"" rel=""nofollow noreferrer"">https://www.biorxiv.org/content/biorxiv/early/2018/03/05/276907.full.pdf</a></p>\n\n<p>Slingshot is nice and easy to use, but it turns out it is terrible at scaling to big datasets. </p>\n\n<p>For 2000 highest variable genes and a random sampling of a variable number of cells in my ~80% sparse expression matrix I get the following computation times in R (96 GB RAM):</p>\n\n<pre><code>#cells / Time (min)\n100 / 0.7\n1000 / 4.3\n2000 / 12.3\n5000 / 71.1\n</code></pre>\n\n<p>At this rate it would take nearly 1.5 days to process 200,000 cells and 4.5 months to process 2 million cells based on 2,000 HVG.</p>\n\n<p>I feel like Slingshot is limited by a fundamentally unnecessary comparison of diffusion for all cells against all cells, rather than preclustering and then finer resolution of pseudotime across cluster edges.</p>\n\n<p>What algorithms are better, but still implement a similar realization of pseudotime ordering?  I just need a single linear trajectory.</p>\n","6498","","","","2020-01-19T14:24:33.303","Most scalable pseudotime ordering algorithm","<r><scrnaseq><statistics>","1","2","",""
"11163","2","","11131","2020-01-16T21:02:05.700","0","","<p>SortMeRNA has fasta files from RFAM and Silva for 5S, 5.8S, 18S, and 28S.  You can download them from <a href=""https://github.com/biocore/sortmerna/tree/master/data/rRNA_databases"" rel=""nofollow noreferrer"">GitHub</a></p>\n","6780","","","","2020-01-16T21:02:05.700","","","","2","",""
"11164","1","","","2020-01-16T23:05:27.923","0","22","<p>I am new to this. I have an SNP array composed of digits [0,1,2] for chromosome 21. How do I find the start:end locations for the region that is ""21q22.11"" step by step?</p>\n","6782","","","","2020-01-17T08:07:17.153","How to Convert From Gene Location to SNP Location","<gene><snp>","1","1","",""
"11165","2","","4470","2020-01-17T00:37:41.667","0","","<p>This updates the answer provided by Mack123456 to Seurat 3 and works in 3.1.2. There are some minor difference with colouring, but it is otherwise similar. The HoverLocator plot throws a warning, all about the plot colours and title, but the plot is fine and labels appear as expected.</p>\n\n<pre><code>load(file= ""~/seuset_16.RData"")\n\nseuset_h16 &lt;- UpdateSeuratObject(seuset_h16)\n\nhigh &lt;- c(""s1.1"",""s1.4"", ""s1.80"")\nlow &lt;- setdiff(rownames(seuset_h16@meta.data), high)\nIdents(seuset_h16, cells = high) &lt;- ""high""\nIdents(seuset_h16, cells = low) &lt;- ""low""\nseuset_h16$expr &lt;- Idents(seuset_h16)\nIdents(object = seuset_h16) &lt;- ""res.1""\n\nt &lt;- DimPlot(seuset_h16, pt.size = 2, shape.by = ""expr"")\nt\n\nt2 &lt;- DimPlot(seuset_h16, pt.size = 2, cols = c(""red"", ""orange"",""green"",""blue""))\nt2 &lt;- HoverLocator(plot = t2, information = FetchData(object = seuset_h16, vars = c(""ident"", ""res.1"")))\nt2\n</code></pre>\n","6783","","","","2020-01-17T00:37:41.667","","","","0","",""
"11166","1","","","2020-01-17T05:45:21.197","0","22","<p>Trying to download colorspace data from SRA, but getting an error  </p>\n\n<p><code>abi-dump -A SRR1657115.sra</code></p>\n\n<p>abi-dump.2.9.6 err: item not found while constructing within virtual database module - the path 'SRR1657115.sra' cannot be opened as database or table.</p>\n","4121","","","","2020-02-18T02:01:45.187","Downloading dataset from SRA (SOLiD Platform)","<ngs>","1","0","",""
"11167","2","","11164","2020-01-17T08:07:17.153","1","","<p>You first need to get the coordinates of the ""cytoband"" 21q22.11, which I would search for using the <a href=""https://genome.ucsc.edu/cgi-bin/hgTables?hgsid=791314831_aE2GlksiE79iyiT0FHCRXDZcFwPC"" rel=""nofollow noreferrer"">UCSC table browser</a>. You need the following settings: \nClade: Mammal\ngenome: Human\nassembly: \ngroup: Mapping and Sequencing\ntrack: Chromosome Band\ntable: cytoBand\nregion: position chr21\noutput format: BED</p>\n\n<p>(you can leave other options as default)</p>\n\n<p>That will give you a bed file of cytobands on chr21. You'll need to search (e.g. grep) for your cytoband: q22.11.</p>\n","681","","","","2020-01-17T08:07:17.153","","","","0","",""
"11168","2","","11158","2020-01-17T08:08:41.390","1","","<p>From the group of Heng Li there is <a href=""https://github.com/chhylp123/hifiasm"" rel=""nofollow noreferrer"">hifiasm</a>.</p>\n","681","","","","2020-01-17T08:08:41.390","","","","0","",""
"11169","2","","11077","2020-01-17T14:19:46.727","0","","<p>We have also seen a very similar issue and are also investigating why. Possibly of note, I tried forcing the cellranger program to assign a very large number of cells (10x the actual number of cells we had) using the --force cells option. This appeared to increase the % of usable Ab reads and Ab reads per cell metrics, suggesting that these reads may be associated with barcodes that have relatively few UMIs?</p>\n","6785","","","","2020-01-17T14:19:46.727","","","","0","",""
"11170","1","","","2020-01-17T14:37:10.567","0","32","<p>I know that I don't have the correct format in my .ped file for PLINK but I don't know how to convert it.\nMy actual file is something like:</p>\n\n<pre><code>BSCAN BSCAN20016 0 0 2 0 0 1 1 1....\nBSCAN BSCAN20017 0 0 2 0 0 1 1 1....\nBSCAN BSCAN20018 0 0 2 0 0 1 1 1....\n</code></pre>\n\n<p>Where column 1 is FamilyID, column 2 is ID, column 3 is FatherID, column 4 MotherID, column 5 Sex and then the genotype of each individual.\nI only need to create binary files, so I tried to run:</p>\n\n<pre><code>plink-1.9-rc --file admixed --make-bed --no-pheno --cow --out admixed\n</code></pre>\n\n<p>but the program shows: Error: half-missing call in .ped file at variant 1, line 1.</p>\n\n<p>Anyone know how can I change the format or recode it to make a correct input?</p>\n\n<p>Thanks in advance!</p>\n","6786","","","","2020-02-16T16:02:35.497","Wrong format .ped in PLINK","<file-formats><plink>","1","0","",""
"11171","2","","11170","2020-01-17T14:44:03.237","0","","<p>So, columns 7 and 8 should be the genotype of variant 1 (each column is one of the alleles). You have them as 0 1, which means absent and major allele.</p>\n\n<p>I think you're just missing a column. From the doc:</p>\n\n<p>Column 6 is Phenotype value ('1' = control, '2' = case, '-9'/'0'/non-numeric = missing data if case/control)</p>\n\n<p>Ideally you would know which SNPs are heterozygous based on your SNP calling and have such SNPs as 1 2, but if you have a homozygous organism or if you're working with data that's been flattened you can simply generate the .ped file with the same number twice for each SNP.</p>\n","6771","","6771","2020-01-17T14:54:12.167","2020-01-17T14:54:12.167","","","","9","",""
"11172","2","","7371","2020-01-17T14:45:05.060","1","","<p><a href=""https://busco.ezlab.org/busco_userguide.html#conda-package"" rel=""nofollow noreferrer"">The new version of busco (v4)</a> has the user entry point called <code>busco</code> (it's still a python script). BUSCO v4 is <a href=""https://anaconda.org/bioconda/busco"" rel=""nofollow noreferrer"">available at conda</a>, note it's a very fresh update.</p>\n","57","","","","2020-01-17T14:45:05.060","","","","0","",""
"11173","1","","","2020-01-17T15:27:03.657","0","78","<p>I was following the RNAseq analysis tutorial online(<a href=""https://combine-australia.github.io/RNAseq-R/06-rnaseq-day1.html"" rel=""nofollow noreferrer"">https://combine-australia.github.io/RNAseq-R/06-rnaseq-day1.html</a>) but am not obtaining the same variance values for each row in the logcounts matrix. Please could you help me identify the error.</p>\n\n<pre><code>library(edgeR)\nlibrary(limma)\nlibrary(Glimma)\nlibrary(org.Mm.eg.db)\nlibrary(RColorBrewer)\nlibrary(gplots)\n\n#Read the data into R\nseqdata &lt;- read.delim(""data/GSE60450_Lactation-GenewiseCounts.txt"", stringsAsFactors = FALSE)\nsampleinfo &lt;- read.delim(""data/SampleInfo.txt"")\nhead(seqdata)\ndim(seqdata)\nsampleinfo\n\n##Format the data\ncountdata &lt;- seqdata[,-(1:2)]\nhead(countdata)\nrownames(countdata) &lt;- seqdata[,1]\nhead(countdata)\ncolnames(countdata)\ncolnames\ncolnames(countdata) &lt;- substr(colnames(countdata),start=1,stop=7)\nhead(countdata)\nsampleinfo\ntable(colnames(countdata)==sampleinfo$SampleName)\n\n##Filtering to remove lowly expressed genes\n#Obtain CPMs\nmyCPM &lt;- cpm(countdata)\nhead(myCPM)\nthresh &lt;- myCPM &gt; 0.5\nhead(thresh)\ntable(rowSums(thresh))\nkeep &lt;- rowSums(thresh) &gt;=2\n#Subset the rows of countdata to keep the more highly expressed genes \ncounts.keep &lt;- countdata[keep,]\nsummary(keep)\ndim(counts.keep)\n#Find out whether CPM threshold corresponds to a count of around 10-15\n#Look at first sample\nplot(myCPM[,1],countdata[,1],ylim=c(0,50),xlim=c(0,3))\nabline(v=0.5)\n\n\n#Convert counts to edgeList objects\ny &lt;- DGEList(counts.keep)\ny\nnames(y)\ny$samples\n\n###Quality Control\n\n#Check the number of reads we have for each sample in y\ny<span class=""math-container"">$samples$</span>lib.size\n#Barplot of library size to spot any major discrepancies between the samples\nbarplot(y<span class=""math-container"">$samples$</span>lib.size, names=colnames(y),las=2)\ntitle(""Barplot of Library Sizes"")\n#Get log2 CPM\nlogcounts &lt;- cpm(y,log=TRUE)\n#Check distribution of samples using boxplots\nboxplot(logcounts, xlab="""",ylab=""log2 CPM"", las=2, ylim=c(-10,15))\n#Add a horizontal line corresponding to the median logCPM\nabline(h=median(logcounts), col=""blue"")\ntitle(""Boxplots of logCPMs (unnormalised)"")\n\n##Heirarchical clustering with heatmaps\n#Estimate the variance for each row in the logcounts matrix\nvar_genes &lt;- apply(logcounts,1,var)\nhead(var_genes)\n</code></pre>\n\n<p>My incorrect variance values of the first 6 genes:497097,20671,27395      18777, 21399 and 58175 are 13.6624115, 2.7493077, 0.1581944, 0.1306781, 0.3929526 and 4.8232522 respectively.</p>\n\n<h2>My boxplot</h2>\n\n<p><a href=""https://i.stack.imgur.com/EWsQN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EWsQN.png"" alt=""My boxplot""></a></p>\n\n<h2>Tutorial boxplot</h2>\n\n<p><a href=""https://i.stack.imgur.com/9eOAg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9eOAg.png"" alt=""Tutorial boxplot""></a></p>\n","6787","","6787","2020-01-21T15:58:19.780","2020-01-21T15:58:19.780","Error in my RNAseq analysis","<r><rna-seq><bioconductor><clustering>","0","5","",""
"11174","2","","11156","2020-01-17T15:27:51.177","0","","<p>By adding this to the script </p>\n\n<pre><code>args=(commandArgs(TRUE))\n\nfile_list &lt;- list.files(path=""/path/to/vcf/files/"")\nfor (i in 1:length(file_list))\n{\n  x=file_list[i]\n  fn=paste('test',x,sep = """")\n  parse_vcf_alt1(x,fn)\n}\n</code></pre>\n\n<p>But Also adding </p>\n\n<pre><code>df &lt;- apply(vcf,2,as.character)\n  fn=paste(""C:/Users/44736/Videos/Captures/New folder/"",fn,sep="""")\n  write.csv(df,fn)\n</code></pre>\n\n<p>to the main function I could get this</p>\n\n<p>The problem is I don't know how to have a tab delimited file rather than csv also removing quote from the written output</p>\n","3962","","","","2020-01-17T15:27:51.177","","","","0","",""
"11175","1","11178","","2020-01-17T17:03:44.400","1","49","<p>I need to find peptides/proteins with a certain sequence of amino acids in a certain type of secondary structure. For example, ""ALA PHE GLY"" in alpha-helix. Is there any tool that does that?</p>\n","6483","","","","2020-01-28T17:04:46.340","Searching for proteins by primary and secondary structure","<protein-structure><secondary-structure>","1","0","","1"
"11178","2","","11175","2020-01-18T10:11:58.690","3","","<p>Your request is rather specific, so there is no tool...</p>\n\n<p>It is a straightforwards request though. I'd impose the requirement that the structures be solved and no predicted secondary structure. You can do it two ways if using Python. </p>\n\n<h2>PDB</h2>\n\n<p>After downloading all <a href=""ftp://ftp.wwpdb.org/pub/pdb/data/structures/all/pdb/"" rel=""nofollow noreferrer"">PDB structures</a>, in python with PyMOL (installable with conda, tricky without) you'd do something like</p>\n\n<pre><code>import pymol2, os\nfrom multiprocessing import Pool\n\ndef find_motif(file):\n    motif = 'AFG'\n    try:\n        with pymol2.PyMOL() as pymol\n            pymol.cmd.load(file)\n            seq = pymol.cmd.get_fastastr()\n            if motif not in seq:\n                return None\n            d = {x: []}\n            # str.find will cause issues if there is a gap also structure may not start at one.\n            pymol.cmd.iterate('resn ALA+PHE+GLY', 'x.append((resi, resn, ss))', space=d)\n            sequence = -1\n            i = 0\n            for residue in d['x']:\n                if residue['resn'] = 'ALA':\n                    sequence = 0 # lazy enum\n                    i = int(residue['resi'])\n                elif sequence != -1 :\n                    if int(residue['resi']) == i+1 and residue['resn'] = 'PHE':\n                        sequence = 1\n                    elif int(residue['resi']) == i+2 and residue['resn'] = 'GLY':\n                        return (file, i)\n                    else:\n                        sequence = -1\n    finally:\n        return (file, None)\n\nif __name__ == '__main__':\n    p = Pool(12)\n    structure_files = [f for f in os.listdir('pdb') if os.path.splitext(f)[1] == '.pdb']\n    verdicts = p.map(finder, structure_files)\n    print([(f,v) for f,v in verdicts if v is not None])\n</code></pre>\n\n<p>The end result will be a list of PDB with where in the sequence (in that PDB code).\n(I wrote this in the reply box, so there may be a typo or two).\nPyMOL has a selection algebra called <code>pepseq</code>, which matches missing atoms, but I was a whimp and played it safe by being more verbose. Also, a for loop can be used instead of an iterate. But I think it is slower.</p>\n\n<pre><code>atoms = pymol.cmd.get_model(f""pepseq {target} and name CA"")\nfor atom in atoms.atom:\n      if atom.ss == ' ...\n</code></pre>\n\n<h2>Uniprot</h2>\n\n<p>In Uniprot, if a protein is crystallised, there will be the feature entries corresponding to the secondary structure. It is sheet or helix. No turns, pi-helices or 3_10 helices. There is no Biopython method for parsing Uniprot XML. I wrote myself a parser and it is not too horrid thing to do —although I <a href=""https://blog.matteoferla.com/2019/02/uniprot-xml-and-python-elementtree.html"" rel=""nofollow noreferrer"">monkeypatched elementtree</a>. Alternatively, <code>xmlschema.XMLSchema.to_dict</code> can be used to convert the XML to a dictionary. The advantage of Uniprot is that the residues ids will not require correcting. The disadvantage is parsing XML.</p>\n\n<p>My parser is specific for my application and it is poorly documented (so would not recommend it) but but... I can share <a href=""https://github.com/matteoferla/MichelaNGLo-protein-module"" rel=""nofollow noreferrer"">it</a> and <a href=""https://github.com/matteoferla/MichelaNGLo-human-protein-data"" rel=""nofollow noreferrer"">human data</a>. I'd be used like:</p>\n\n<pre><code> import os\n from michelanglo_protein import ProteinCore\n ProteinCore.settings.startup(data_folder='protein-data')\n p = ProteinCore().gload(file=os.path.join('gpickle','Q86V25.gpz'))\n #print(p.asdict())\n if 'helix' in p.features:\n        n = p.sequence.find( ...\n        for ... p.features['helix'] #('helix', 'turn', 'strand')\n        if x &lt; n &lt; y ...\n</code></pre>\n","6322","","6322","2020-01-28T17:04:46.340","2020-01-28T17:04:46.340","","","","4","",""
"11179","2","","11166","2020-01-19T01:17:42.297","0","","<p>You have to use the accession name without the <code>.sra</code> suffix.</p>\n\n<p><code>abi-dump -A SRR1657115</code></p>\n","3051","","","","2020-01-19T01:17:42.297","","","","0","",""
"11180","2","","11162","2020-01-19T14:24:33.303","0","","<p>As it turns out, pseudotime algorithms are exceedingly difficult to parallelize. Pseudotime does not scale linearly to a subset of data compared to the full dataset, and as such a course-grained approach to parallelizing pseudotime calculations on random bins of the data, followed by bin merging, does not work. These values will never converge on the actual full-dataset solution.</p>\n\n<p>I put in a pretty good effort trying to parallelize slingshot, but with no success. I realize my code may be of limited utility without more explanation, but I'm not putting more time into this. FWIW:</p>\n\n<p>This code:</p>\n\n<ol>\n<li>Finds computationally optimal bin size for slingshot based on a number of genes and features</li>\n<li>Randomly splits the SingleCellExperiment object into that number of bins and runs slingshot on each bin</li>\n<li>Orients each bin so that a cell with a pseudotime of 1 in the first bin is similar to a pseudotime of 1 in every bin. Pseudotime can get flipped depending on the sampled subset.</li>\n<li>Merges all bins into a single matrix so each cell has a assigned pseudotime based on randomly sampled bins.</li>\n<li>Repeats steps #2 to #5 with a different random seed for binning every time.</li>\n<li>Measures convergence towards actual solution at three pseudotime values, where convergence is defined as correlation of cell indices along the trajectory.</li>\n</ol>\n\n<p>I abandoned the project after observing that more iterations to not change the convergence towards the actual solution. While pseudotime values close to 0 recapitulated the actual solution quite well, pseudotime values towards 100 failed to correlate at all with the actual solution. There was no convergence with more iterations.</p>\n\n<p>Maybe this will be helpful to somebody, which is why I'm posting.</p>\n\n<pre><code># Parallel Iterative Sampling with Slingshot\n# pissshot is a course-grained estimator of the actual slingshot solution, and iteratively converges towards the actual solution\n# pissshot randomly partitions cells in a large database into bins, bin size is determined by finding the computationally optimal number of cells\n# pissshot is run independently on each bin, pseudotime vectors are aligned across all bins, and then all bins are merged\n# pissshot repeats this process and the average of pseudotime assignments for each cell is taken at the end of each iteration. \n# The convergence of the blingshot model at the end of each iteration is calculated by running slingshot on a small sample of adjacent cells at a random pseudotime interval and measuring correlation with the predicted model\n# When satisfactory convergence is reached, blingshot returns a singlecellobject, just as slingshot would\n\npissshot &lt;- function(sce, num_iterations = 20){\n    cat(""Running blingshot on"", dim(sce)[2],""cells and"",dim(sce)[1],""features\n"")\n    cat(""  Step 1/5: Finding computationally optimal increment sizes\n   "")\n#    increment_size &lt;- FindOptimalIncrementSize(sce)\n    increment_size &lt;- 688\n    num_increments &lt;- ceiling(dim(sce)[2]/increment_size)-1\n    cat(""\n  ...Increment size of"",increment_size,""cells is computationally optimal\n"")\n    cat(""  Step 2/5: Course-grained pseudotime assignment across"",num_increments,""increments"")\n    ps &lt;- list()\n    convergenceArr &lt;- c()\n    for(iteration in seq(from=1,to=num_iterations,by=1)){\n        cat(""\n   ... iteration"",iteration,""...\n"")\n        cat(""         randomly assigning cells to"",num_increments,""bins\n         0%."")\n        bins &lt;- PartitionBins(sce, increment_size = increment_size, seed = 7*iteration)\n        cat(""100%\n         running slingshot on each bin\n         0%"")\n        cell_pseudotimes &lt;- c()\n        for(b in seq(from=1,to=length(bins),by=1)){\n            bin_with_pseudotime &lt;- suppressMessages(slingshot(bins[[b]]))\n            # Find right cell and left cell in pseudotime array (cell with highest/lowest pseudotime values)\n            pcells &lt;- as.data.frame(colData(bin_with_pseudotime)<span class=""math-container"">$slingPseudotime_1)\n            rownames(pcells) &lt;- colnames(bin_with_pseudotime)\n            pcells &lt;- data.frame(lapply(pcells, function(x) as.numeric(as.character(x))), check.names = F, row.names = rownames(pcells))\n            pcells.sorted &lt;- pcells[order(-pcells[,1]), , drop = FALSE]\n            right_cell_ID &lt;- rownames(pcells.sorted)[dim(pcells.sorted)[1]]\n            left_cell_ID &lt;- rownames(pcells.sorted)[1]\n            right_cell_pos &lt;- match(right_cell_ID,colnames(bins[[b]]))\n            left_cell_pos &lt;- match(left_cell_ID,colnames(bins[[b]]))\n            right_cell_logcounts &lt;- assays(bins[[b]])$</span>logcounts[,right_cell_pos]\n            left_cell_logcounts &lt;- assays(bins[[b]])<span class=""math-container"">$logcounts[,left_cell_pos]\n            if(b==1&amp;&amp;iteration==1){\n                ref_right_cell_logcounts &lt;- right_cell_logcounts\n                ref_left_cell_logcounts &lt;- left_cell_logcounts\n            }\n            # Figure out whether this pseudotime in this bin has flipped relative to reference (first bin), and if so, flip pseudotime values\n            cor_opposite_ends &lt;- mean(cor(right_cell_logcounts,ref_left_cell_logcounts),cor(left_cell_logcounts,ref_right_cell_logcounts))\n            cor_same_ends &lt;- mean(cor(right_cell_logcounts,ref_right_cell_logcounts),cor(left_cell_logcounts,ref_left_cell_logcounts))\n            flipped &lt;- FALSE\n            if(cor_opposite_ends &gt; cor_same_ends){\n                flipped &lt;- TRUE\n            }\n            if(flipped==TRUE){\n                # find max pseudotime in colData(bin_with_pseudotime)$slingPseudotime\n                # Recalculate pseudotime in bin_with_pseudotime as max-value\n                pseudotimes &lt;- colData(bin_with_pseudotime)$slingPseudotime_1\n                colData(bin_with_pseudotime)$slingPseudotime_1 &lt;- max(pseudotimes)-pseudotimes \n            }\n            cells_with_pseudotime &lt;- as.data.frame(colData(bin_with_pseudotime)$</span>slingPseudotime_1,colnames(bin_with_pseudotime))\n            cell_pseudotimes &lt;- rbind(cell_pseudotimes, cells_with_pseudotime)\n            cat(""."")\n        }\n        cat(""100%\n"")\n\n        ps[[iteration]] &lt;- cell_pseudotimes[order(row.names(cell_pseudotimes)), , drop = FALSE]\n\n        pss &lt;- data.frame(ps)\n        if(iteration&gt;1){\n            convergence10 &lt;- measureConvergence(pss, sce = sce, ptime = 10, increment_size = increment_size)\n            convergence50 &lt;- measureConvergence(pss, sce = sce, ptime = 50, increment_size = increment_size)\n            convergence99 &lt;- measureConvergence(pss, sce = sce, ptime = 99, increment_size = increment_size)\n            cat(""   convergence at 10:"",convergence10,""... at 50:"",convergence50,""...at 100:"",convergence99,""\n"")\n            convergenceArr &lt;- rbind(convergenceArr,c(iteration,convergence10,convergence50,convergence99))\n        }\n        # Iterate until the model converges towards the actual slingshot solution, within an indicated fraction (i.e. convergence = 0.02)\n        # The convergence measure compares how similar the model is to the actual slingshot solution, i.e. a convergence of 0.02 means that cell pseudotime values are within 2% of the actual solution\n        # Assess this by selecting a pseudotime at random, analyzing the following increment_size cells, and comparing accuracy of pseudotime measurement across that trajectory with the composite of the course-grained analysis\n\n        # need to study how convergence changes over pseudotime\n    }\n\n    # remove cells with really high standard deviations\n\n    return(pss)\n#    return(convergenceArr)\n}\n\n\nmeasureConvergence &lt;- function(mat, sce = sce, ptime = 45, increment_size = 500){\n    # this line is throwing an error\n    mat[,""avg""] &lt;- apply(mat[,1:dim(mat)[2]],1,mean)\n    mat &lt;- mat[order(mat[,""avg""]), , drop = FALSE]\n    # select a random pseudotime increment\n    mat &lt;- mat[mat[, ""avg""] &gt; ptime,]\n    mat &lt;- mat[1:increment_size,]\n    cell_IDs &lt;- rownames(mat)\n    # pull out this list of cell_IDs from the sce object and run slingshot\n    sce2 &lt;- suppressMessages(slingshot(sce[,cell_IDs]))\n    cpcells &lt;- as.data.frame(colData(sce2)$slingPseudotime_1)\n    rownames(cpcells) &lt;- colnames(sce2)\n    cpcells &lt;- data.frame(lapply(cpcells, function(x) as.numeric(as.character(x))), check.names = F, row.names = rownames(cpcells))\n    # sort cpcells by increasing pseudotime\n    cpcells &lt;- cpcells[order(cpcells[,1]), , drop = FALSE]\n    ordered_cells &lt;- cbind(rownames(mat),rownames(cpcells))\n    # get the index of each cell in the dataframe, run a correlation on how well the indices line up\n    cell_indices &lt;- c()\n    for(cell in cell_IDs){\n        new_indices &lt;- c(which(ordered_cells[,1] == cell), which(ordered_cells[,2] == cell))\n        cell_indices &lt;- rbind(cell_indices,new_indices)\n    }\n    convergence_measure &lt;- cor(cell_indices[,1],cell_indices[,2])\n    plot(cell_indices[,1],cell_indices[,2])\n    return(convergence_measure)\n}\n\nPartitionBins &lt;- function(sce, increment_size = 500, seed = 123){\n    bins &lt;- c()\n    while(dim(sce)[2]&gt;=increment_size){\n        if(dim(sce)[2]&lt;increment_size*2){\n            bins &lt;- c(bins, sce)\n            sce &lt;- sce[,1]\n        }\n        else{\n            set.seed(seed)\n            train_ind &lt;- sample(seq_len(ncol(sce)), size = increment_size)\n            bins &lt;- c(bins, sce[,train_ind])\n            sce &lt;- sce[,-train_ind]\n            cat(""."")\n        }\n    }\n    return(bins)\n}\n\nFindOptimalIncrementSize &lt;- function(sce){\n    # Find the computationally optimal increment_size for grainy alignments using 5 increment points (100, 500, 1000, 2500)\n    res &lt;- c(100,250,500,750,1000,1500,2500,5000)\n    runtimes &lt;- c()\n    for(i in res){\n        runtimes &lt;- c(runtimes, suppressMessages(system.time(slingshot(sce[,0:i])))[""elapsed""])\n    }\n    mat &lt;- as.data.frame(t(rbind(res,runtimes)))\n    c &lt;- summary(lm(mat<span class=""math-container"">$runtimes ~ poly(mat$</span>res, 2, raw=TRUE)))$coefficients[,""Estimate""]\n    predicted_runtimes &lt;- c()\n    for(i in seq(from=100, to=5000, by=1)){\n        predicted_runtime &lt;- (c[1] + c[2]*i + c[3]*i^2)*(dim(sce)[2]/i)\n        predicted_runtimes &lt;- rbind(predicted_runtimes,c(i,predicted_runtime))\n        cat(""."")\n    }\n    increment_size &lt;- predicted_runtimes[which.min(predicted_runtimes[,2]),1]\n    return(increment_size)\n}\n</code></pre>\n","6498","","","","2020-01-19T14:24:33.303","","","","0","",""
"11181","1","11182","","2020-01-19T14:59:30.530","-1","35","<p>I have a big data frame</p>\n\n<pre><code>&gt; dim(anno_maf)\n[1] 449892    132\n&gt; \n</code></pre>\n\n<p>In chromosome column I have chr 1 to 22 and another contigs like</p>\n\n<pre><code>&gt; unique(anno_maf$Chromosome)\n [1] ""chr1""                  ""chr6""                 \n [3] ""chr16""                 ""chr17""                \n [5] ""chr20""                 ""chr2""                 \n [7] ""chr3""                  ""chr4""                 \n [9] ""chr14""                 ""chr19""                \n[11] ""chr5""                  ""chr10""                \n[13] ""chr9""                  ""chr12""                \n[15] ""chr13""                 ""chr11""                \n[17] ""chr22""                 ""chr7""                 \n[19] ""chr15""                 ""chr8""                 \n[21] ""chr18""                 ""chr21""                \n[23] ""chrX""                  ""chrY""                 \n[25] ""chr4_gl000194_random""  ""chr17_gl000205_random""\n[27] ""chrUn_gl000241""        ""hs37d5""               \n[29] ""chrUn_gl000219""        ""chrUn_gl000234""       \n[31] ""chr1_gl000191_random""  ""chrUn_gl000211""       \n[33] ""chrUn_gl000224""        ""chrUn_gl000225""       \n[35] ""chr17_gl000203_random"" ""chrUn_gl000212""       \n[37] ""chrUn_gl000243""        ""chrUn_gl000214""       \n[39] ""chrM""                  ""chr1_gl000192_random"" \n[41] ""chr7_gl000195_random""  ""chrUn_gl000232""       \n[43] ""chr4_gl000193_random""  ""chr19_gl000208_random""\n[45] ""chrUn_gl000226""        ""chrUn_gl000218""       \n[47] ""chr9_gl000199_random""  ""chrUn_gl000217""       \n[49] ""chrUn_gl000229""        ""chrUn_gl000216""       \n[51] ""chrUn_gl000231""        ""chr9_gl000198_random"" \n[53] ""chr17_gl000204_random"" ""chrUn_gl000220""       \n[55] ""chrUn_gl000235""        ""chr11_gl000202_random""\n[57] ""chrUn_gl000222""        ""chrUn_gl000240""       \n[59] ""chrUn_gl000233""        ""chrUn_gl000230""       \n[61] ""chrUn_gl000213""        ""chrUn_gl000238""       \n[63] ""chr19_gl000209_random"" ""chrUn_gl000237""       \n[65] ""#CHROM"" \n\n  How I can only keep rows having chr1 to chr22 and removing the rest?\n</code></pre>\n","4595","","","","2020-01-19T15:06:35.853","Removing rows containing some strings","<r>","1","0","","1"
"11182","2","","11181","2020-01-19T15:06:35.853","2","","<p>You can create a vector consisting of values that you would like to keep (or discard):</p>\n\n<pre><code>to_keep &lt;- paste0(""chr"", seq(1,22))\n\n&gt; to_keep\n [1] ""chr1""  ""chr2""  ""chr3""  ""chr4""  ""chr5""  ""chr6""  ""chr7""  ""chr8""  ""chr9""  ""chr10"" ""chr11""\n[12] ""chr12"" ""chr13"" ""chr14"" ""chr15"" ""chr16"" ""chr17"" ""chr18"" ""chr19"" ""chr20"" ""chr21"" ""chr22""\n\nyour_df[your_df$Chromosome %in% to_keep,]\n</code></pre>\n\n<p>The <code>data.table</code> way, which should be faster for large data, would be:</p>\n\n<pre><code>your_df[Chromosome %in% to_keep]\n</code></pre>\n\n<p>I don't know what your goal is but you might want to keep chromosomes X and Y as well.</p>\n","3541","","","","2020-01-19T15:06:35.853","","","","0","",""
"11183","1","11185","","2020-01-19T21:38:29.317","-4","36","<p>I have a data frame like with more samples like</p>\n\n<pre><code>&gt; head(a[1:2,1:4])\n             ID               ids pre.or.post.treatment\n1 OCCAMS/AH/120 LP6005409-DNA_F01                   pre\n2 OCCAMS/AH/126 LP2000325-DNA_A01                   pre\n            TRG\n1 non-responder\n2 non-responder\n&gt; dim(a)\n[1] 160   4\n&gt; \n</code></pre>\n\n<p>Another data frame with part of my samples but more characteristics for each sample like</p>\n\n<pre><code>&gt; dim(b)\n[1] 60 50\n\n&gt; head(b[1:2,1:6])\n             ID       TRG pre.or.post.treatment          ID.1\n1 OCCAMS/AH/212 responder                   pre OCCAMS/AH/212\n2 OCCAMS/AH/279 responder                   pre OCCAMS/AH/279\n  DiagnosisDate FE.LastSeenDate\n1      8/8/2013        6/6/2014\n2     6/10/2014       11/8/2018\n&gt;  \n</code></pre>\n\n<p>How I can  merge these data frame getting a new data frame with my all 160 samples columns of both a and b</p>\n","4595","","","","2020-01-20T04:18:49.043","Merging two data frames","<r>","2","0","","1"
"11184","2","","2817","2020-01-19T23:17:45.590","0","","<p>If you have a very big fasta file(60 GB) why use SeqIO.parse and write the output file using biopython?</p>\n\n<p>Why not use SimpleFastaParser instead (you get a tuple(name, seq)) and you dont need to wait so long as using the parser and using Seqrecords.</p>\n\n<p>What do you think about it guys?</p>\n\n<p>Thanks</p>\n\n<p>Paulo</p>\n\n<p>Kind of:</p>\n\n<pre><code>from Bio.SeqIO.FastaIO import SimpleFastaParser\nfrom collections import defaultdict\n\ndedup_records = defaultdict(list)\nfor (name, seq) in SimpleFastaParser(filename):\n    # Use the sequence as the key and then have a list of id's as the value\n    dedup_records[seq)].append(name)\nwith open(filename, 'w') as output:\n    for seq, ids in dedup_records.items():\n        # Join the ids and write them out as the fasta\n        output.write(""&gt;{}\n"".format('|'.join(ids)))\n        output.write(seq + ""\n"")\n</code></pre>\n","4222","","","","2020-01-19T23:17:45.590","","","","0","",""
"11185","2","","11183","2020-01-20T03:11:31.550","2","","<p>Assuming your column ID in both dataframes <code>a</code> and <code>b</code> shared same IDs, and as your dataframe are not particularly heavy, you can have the use of the function <code>left_join</code> from <code>dplyr</code> package as this:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(dplyr)\nDF &lt;- left_join(a,b, by = ""ID"")\n</code></pre>\n","6437","","","","2020-01-20T03:11:31.550","","","","0","",""
"11186","2","","11183","2020-01-20T04:18:49.043","3","","<p>@dc37's dplyr solution is quite elegant, but you can also use base R's merge:</p>\n\n<pre><code>merge(x = df_a, y = df_b, by = ""ID"", all.x = TRUE, all.y = TRUE)\n</code></pre>\n\n<p>You can tweak various parameters, such as use <code>by.x</code> and <code>by.y</code> instead of just <code>by</code>, change <code>all.x</code> and <code>all.y</code> based on requirements to get your results.</p>\n\n<p>This is a pretty common R question by the way, and if you had googled ""merging data frames"", you'd see a lot of options.</p>\n","650","","","","2020-01-20T04:18:49.043","","","","0","",""
"11187","1","","","2020-01-20T09:16:13.010","0","23","<p>working on lung cancer project and need to find out which genes are deferentially expressed and which are not. for that i a have downloaded miRNA data set from GEO id is GSE110907. And the file i got has to variable gene_id and read counts. from that i have to derive p value from that data/file.<a href=""https://i.stack.imgur.com/osaNe.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/osaNe.jpg"" alt=""enter image description here""></a></p>\n","3659","","","","2020-01-20T09:37:12.140","how can i calculate p value of miRNA raw reads counts using R?","<r><p-values>","1","1","",""
"11188","2","","11187","2020-01-20T09:37:12.140","1","","<p>I would start by re-formulating the question in this post: Your end-goal seems to the differential expression analysis (DEA) of your samples (and you have to have to levels for this like treatment-control, mutant-WT, etc ...). P-values are context-dependent and should be calculated based on your research question/hypothesis.</p>\n\n<p>DEA has been addressed by many researchers and <code>R</code> has a lot of packages to help you with this; <code>DESeq2</code>, <code>edgeR</code>, <code>limma</code>, etc ...</p>\n\n<p>I would go to the <code>Biconductor</code> website, search for one of the three above, and go through their rich <strong>vignettes</strong> and would come back here for <strong>specific</strong> questions.</p>\n","3541","","","","2020-01-20T09:37:12.140","","","","0","",""
"11190","1","11191","","2020-01-20T13:25:31.967","-1","44","<p>I have a too big data frame</p>\n\n<pre><code>&gt; dim(anno_maf)\n[1] 2635299     132\n&gt;\n\n&gt; head(anno_maf[1:3,1:4])\n                     Tumor_Sample_Barcode Chromosome Start_Position End_Position\n1: LP2000107-DNA_A01_vs_LP2000102-DNA_A01       chr5      137536800    137536803\n2: LP2000330-DNA_A01_vs_LP2000316-DNA_A01      chr11       64012229     64012231\n3: LP2000330-DNA_A01_vs_LP2000316-DNA_A01      chr17       41720917     41720917\n&gt; \n\n&gt; unique(anno_maf$Tumor_Sample_Barcode)\n  [1] ""LP2000107-DNA_A01_vs_LP2000102-DNA_A01"" ""LP2000330-DNA_A01_vs_LP2000316-DNA_A01"" ""LP2000332-DNA_A01_vs_LP2000323-DNA_A01""\n  [4] ""LP2000333-DNA_A01_vs_LP2000324-DNA_A01"" ""LP6005334-DNA_A02_vs_LP6005333-DNA_A02"" ""LP6005334-DNA_C01_vs_LP6005333-DNA_C01""\n</code></pre>\n\n<p>I want to remove everything thing from <code>vs</code> onward in <code>anno_maf$Tumor_Sample_Barcode</code>, for example leaving <code>LP2000107-DNA_A01</code> from <code>LP2000107-DNA_A01_vs_LP2000102-DNA_A01</code> but I don't know how to do that</p>\n\n<p>Any help?</p>\n","4595","","","","2020-01-20T13:32:21.160","Chaning string in rows","<r>","1","0","","1"
"11191","2","","11190","2020-01-20T13:32:21.160","3","","<p><code>sub()</code> is the way to go.</p>\n\n<p>Your pattern is your regular expression: Check for ""_vs_"" followed by any character (provided with <code>.</code>) for an undefined length (provided with <code>*</code>).</p>\n\n<pre><code>my_text &lt;- ""LP2000107-DNA_A01_vs_LP2000102-DNA_A01""\n\nsub(pattern = ""_vs_.*"",\n    replacement = """",\n    x = my_text)\n\n[1] ""LP2000107-DNA_A01""\n</code></pre>\n","3541","","","","2020-01-20T13:32:21.160","","","","5","",""
"11192","1","11193","","2020-01-20T14:15:13.417","4","95","<p>I have a FASTA file which I would like to convert into FASTQ format as the tool I want to use my data in requires it in FASTQ format. So dummy quality scores are fine.</p>\n\n<p>Note: I am not using any functionality of the tool that will require the quality score.</p>\n","149","","149","2020-01-23T15:09:49.373","2020-01-23T15:09:49.373","Convert FASTA to FASTQ with dummy quality scores","<fasta><fastq>","4","0","",""
"11193","2","","11192","2020-01-20T14:15:13.417","2","","<p>One way of doing this is with two subcommands from the <a href=""https://github.com/sanger-pathogens/Fastaq"" rel=""nofollow noreferrer""><code>pyfastaq</code></a> suite.</p>\n\n<pre class=""lang-sh prettyprint-override""><code>fastaq to_fake_qual in.fasta - | fastaq fasta_to_fastq in.fasta - out.fastq\n</code></pre>\n\n<p>The first tool, <code>to_fake_qual</code>, creates fake quality scores (default 40) for each base and the <code>-</code> sends that file (<code>.qual</code>) to stdout. The second tool, <code>fasta_to_fastq</code>, consumes both the original fasta and the quality scores coming from stdin and turns these into a fastq file.</p>\n","149","","","","2020-01-20T14:15:13.417","","","","0","",""
"11194","2","","11192","2020-01-20T20:59:28.290","3","","<p>A simple Biopython solution:</p>\n\n<pre><code>from Bio import SeqIO\n\nfor r in SeqIO.parse(""myfile.fa"", ""fasta""):\n    r.letter_annotations[""solexa_quality""] = [40] * len(r)\n    print(r.format(""fastq""), end='')\n</code></pre>\n\n<p>Example:</p>\n\n<pre><code><span class=""math-container"">$ cat myfile.fa \n&gt;1\nATG\n&gt;2\nATGTAGA\n$</span> python3 myscript.py \n@1\nATG\n+\nIII\n@2\nATGTAGA\n+\nIIIIIII\n</code></pre>\n","104","","","","2020-01-20T20:59:28.290","","","","0","",""
"11195","2","","3206","2020-01-20T21:18:30.180","1","","<p>The <a href=""https://rdrr.io/bioc/scran/man/quickCluster.html"" rel=""nofollow noreferrer""><code>scran::quickCluster</code> method</a> has a <code>BPPARAM</code> (i.e., <code>BiocParallelParam</code>), so if one provides something like</p>\n\n<pre><code>clusters &lt;- quickCluster(sce, BPPARAM=BiocParallel::MulticoreParam())\n</code></pre>\n\n<p>it will use all available cores (a specific number of cores can also be given). See <a href=""https://bioconductor.org/packages/devel/bioc/vignettes/BiocParallel/inst/doc/Introduction_To_BiocParallel.pdf"" rel=""nofollow noreferrer"">the intro doc to <code>BiocParallel</code></a> for info on other options.</p>\n","900","","","","2020-01-20T21:18:30.180","","","","3","",""
"11196","1","11260","","2020-01-20T22:52:42.333","0","68","<p>Newbie R user here. I want to create a plot with boxplots of read length distribution on the Y-axis, and samples on the X-axis -- like this image from doi: 10.1038/srep16498 <a href=""https://i.stack.imgur.com/rHloO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rHloO.png"" alt=""enter image description here""></a></p>\n\n<p>However, I'm not sure how to plot my data in <code>ggplot2</code> to make this. My data is currently in this format:</p>\n\n<pre class=""lang-r prettyprint-override""><code>Length Sample1 Sample2\n20 10 5\n30 15 223\n40 16 4923\n50 293 239\n60 3290 23\n70 248 6\n80 23 1\n90 21 0\n100 5 0\n</code></pre>\n\n<p>Length is the read length bin, with the sample columns containing the number of reads of that length. I've tried using <code>ggplot2</code>, but don't think I can do it with the data in its current format.</p>\n\n<p>Thanks!</p>\n\n<p>Edit: Here are the number of reads for each individual read length (converted to long format):</p>\n\n<pre class=""lang-r prettyprint-override""><code>""Readlength"" ""name"" ""value""\n""1"" 25 ""Sample1"" 457\n""2"" 25 ""Sample2"" 580\n""3"" 26 ""Sample1"" 603\n""4"" 26 ""Sample2"" 648\n""5"" 27 ""Sample1"" 599\n""6"" 27 ""Sample2"" 715\n""7"" 28 ""Sample1"" 686\n""8"" 28 ""Sample2"" 914\n""9"" 29 ""Sample1"" 796\n""10"" 29 ""Sample2"" 1018\n""11"" 30 ""Sample1"" 1009\n""12"" 30 ""Sample2"" 1366\n""13"" 31 ""Sample1"" 1225\n""14"" 31 ""Sample2"" 1653\n""15"" 32 ""Sample1"" 1579\n""16"" 32 ""Sample2"" 2135\n""17"" 33 ""Sample1"" 1817\n""18"" 33 ""Sample2"" 2678\n""19"" 34 ""Sample1"" 2322\n""20"" 34 ""Sample2"" 3424\n""21"" 35 ""Sample1"" 2861\n""22"" 35 ""Sample2"" 4167\n""23"" 36 ""Sample1"" 3460\n""24"" 36 ""Sample2"" 5220\n""25"" 37 ""Sample1"" 4161\n""26"" 37 ""Sample2"" 6088\n""27"" 38 ""Sample1"" 4930\n""28"" 38 ""Sample2"" 7609\n""29"" 39 ""Sample1"" 5910\n""30"" 39 ""Sample2"" 9308\n""31"" 40 ""Sample1"" 7007\n""32"" 40 ""Sample2"" 11062\n""33"" 41 ""Sample1"" 8339\n""34"" 41 ""Sample2"" 13341\n""35"" 42 ""Sample1"" 10105\n""36"" 42 ""Sample2"" 16288\n""37"" 43 ""Sample1"" 11720\n""38"" 43 ""Sample2"" 19023\n""39"" 44 ""Sample1"" 13312\n""40"" 44 ""Sample2"" 22152\n""41"" 45 ""Sample1"" 15430\n""42"" 45 ""Sample2"" 25360\n""43"" 46 ""Sample1"" 17515\n""44"" 46 ""Sample2"" 28789\n""45"" 47 ""Sample1"" 20022\n""46"" 47 ""Sample2"" 32985\n""47"" 48 ""Sample1"" 22825\n""48"" 48 ""Sample2"" 38019\n""49"" 49 ""Sample1"" 26150\n""50"" 49 ""Sample2"" 42972\n""51"" 50 ""Sample1"" 28998\n""52"" 50 ""Sample2"" 49293\n""53"" 51 ""Sample1"" 33236\n""54"" 51 ""Sample2"" 56239\n""55"" 52 ""Sample1"" 37302\n""56"" 52 ""Sample2"" 62791\n""57"" 53 ""Sample1"" 41212\n""58"" 53 ""Sample2"" 69865\n""59"" 54 ""Sample1"" 45683\n""60"" 54 ""Sample2"" 76922\n""61"" 55 ""Sample1"" 49844\n""62"" 55 ""Sample2"" 82906\n""63"" 56 ""Sample1"" 53833\n""64"" 56 ""Sample2"" 90092\n""65"" 57 ""Sample1"" 58486\n""66"" 57 ""Sample2"" 98194\n""67"" 58 ""Sample1"" 64650\n""68"" 58 ""Sample2"" 105602\n""69"" 59 ""Sample1"" 69514\n""70"" 59 ""Sample2"" 114319\n""71"" 60 ""Sample1"" 76202\n""72"" 60 ""Sample2"" 123807\n""73"" 61 ""Sample1"" 83273\n""74"" 61 ""Sample2"" 133289\n""75"" 62 ""Sample1"" 90783\n""76"" 62 ""Sample2"" 146387\n""77"" 63 ""Sample1"" 98805\n""78"" 63 ""Sample2"" 159302\n""79"" 64 ""Sample1"" 107247\n""80"" 64 ""Sample2"" 168856\n""81"" 65 ""Sample1"" 115200\n""82"" 65 ""Sample2"" 180408\n""83"" 66 ""Sample1"" 123359\n""84"" 66 ""Sample2"" 191083\n""85"" 67 ""Sample1"" 134258\n""86"" 67 ""Sample2"" 200708\n""87"" 68 ""Sample1"" 142179\n""88"" 68 ""Sample2"" 212830\n""89"" 69 ""Sample1"" 155549\n""90"" 69 ""Sample2"" 226788\n""91"" 70 ""Sample1"" 165425\n""92"" 70 ""Sample2"" 239063\n""93"" 71 ""Sample1"" 178681\n""94"" 71 ""Sample2"" 255338\n""95"" 72 ""Sample1"" 194293\n""96"" 72 ""Sample2"" 272725\n""97"" 73 ""Sample1"" 211417\n""98"" 73 ""Sample2"" 288512\n""99"" 74 ""Sample1"" 225639\n""100"" 74 ""Sample2"" 307752\n""101"" 75 ""Sample1"" 242618\n""102"" 75 ""Sample2"" 324919\n""103"" 76 ""Sample1"" 259391\n""104"" 76 ""Sample2"" 338429\n""105"" 77 ""Sample1"" 274041\n""106"" 77 ""Sample2"" 354866\n""107"" 78 ""Sample1"" 295365\n""108"" 78 ""Sample2"" 373456\n""109"" 79 ""Sample1"" 315802\n""110"" 79 ""Sample2"" 389945\n""111"" 80 ""Sample1"" 337826\n""112"" 80 ""Sample2"" 411033\n""113"" 81 ""Sample1"" 363906\n""114"" 81 ""Sample2"" 437069\n""115"" 82 ""Sample1"" 388803\n""116"" 82 ""Sample2"" 460419\n""117"" 83 ""Sample1"" 416728\n""118"" 83 ""Sample2"" 487959\n""119"" 84 ""Sample1"" 446797\n""120"" 84 ""Sample2"" 518295\n""121"" 85 ""Sample1"" 474343\n""122"" 85 ""Sample2"" 538191\n""123"" 86 ""Sample1"" 496615\n""124"" 86 ""Sample2"" 558646\n""125"" 87 ""Sample1"" 524080\n""126"" 87 ""Sample2"" 583043\n""127"" 88 ""Sample1"" 549720\n""128"" 88 ""Sample2"" 601107\n""129"" 89 ""Sample1"" 573864\n""130"" 89 ""Sample2"" 623715\n""131"" 90 ""Sample1"" 604559\n""132"" 90 ""Sample2"" 648209\n""133"" 91 ""Sample1"" 632324\n""134"" 91 ""Sample2"" 668883\n""135"" 92 ""Sample1"" 657924\n""136"" 92 ""Sample2"" 698291\n""137"" 93 ""Sample1"" 686875\n""138"" 93 ""Sample2"" 727313\n""139"" 94 ""Sample1"" 712778\n""140"" 94 ""Sample2"" 745380\n""141"" 95 ""Sample1"" 732786\n""142"" 95 ""Sample2"" 766930\n""143"" 96 ""Sample1"" 755081\n""144"" 96 ""Sample2"" 784337\n""145"" 97 ""Sample1"" 772734\n""146"" 97 ""Sample2"" 793358\n""147"" 98 ""Sample1"" 787861\n""148"" 98 ""Sample2"" 804737\n""149"" 99 ""Sample1"" 807004\n""150"" 99 ""Sample2"" 820443\n""151"" 100 ""Sample1"" 823211\n""152"" 100 ""Sample2"" 826114\n""153"" 101 ""Sample1"" 836362\n""154"" 101 ""Sample2"" 839164\n""155"" 102 ""Sample1"" 853124\n""156"" 102 ""Sample2"" 853144\n""157"" 103 ""Sample1"" 865378\n""158"" 103 ""Sample2"" 861670\n""159"" 104 ""Sample1"" 874578\n""160"" 104 ""Sample2"" 875069\n""161"" 105 ""Sample1"" 887322\n""162"" 105 ""Sample2"" 883285\n""163"" 106 ""Sample1"" 888977\n""164"" 106 ""Sample2"" 875065\n""165"" 107 ""Sample1"" 889700\n""166"" 107 ""Sample2"" 877648\n""167"" 108 ""Sample1"" 893138\n""168"" 108 ""Sample2"" 875875\n""169"" 109 ""Sample1"" 887592\n""170"" 109 ""Sample2"" 865221\n""171"" 110 ""Sample1"" 888684\n""172"" 110 ""Sample2"" 863268\n""173"" 111 ""Sample1"" 887037\n""174"" 111 ""Sample2"" 863513\n""175"" 112 ""Sample1"" 887121\n""176"" 112 ""Sample2"" 855519\n""177"" 113 ""Sample1"" 883085\n""178"" 113 ""Sample2"" 854312\n""179"" 114 ""Sample1"" 886536\n""180"" 114 ""Sample2"" 855445\n""181"" 115 ""Sample1"" 875703\n""182"" 115 ""Sample2"" 844795\n""183"" 116 ""Sample1"" 870749\n""184"" 116 ""Sample2"" 837992\n""185"" 117 ""Sample1"" 863969\n""186"" 117 ""Sample2"" 830157\n""187"" 118 ""Sample1"" 849188\n""188"" 118 ""Sample2"" 809476\n""189"" 119 ""Sample1"" 840690\n""190"" 119 ""Sample2"" 798965\n""191"" 120 ""Sample1"" 834472\n""192"" 120 ""Sample2"" 791200\n""193"" 121 ""Sample1"" 822558\n""194"" 121 ""Sample2"" 776191\n""195"" 122 ""Sample1"" 811864\n""196"" 122 ""Sample2"" 765913\n""197"" 123 ""Sample1"" 805300\n""198"" 123 ""Sample2"" 761200\n""199"" 124 ""Sample1"" 794602\n""200"" 124 ""Sample2"" 747483\n""201"" 125 ""Sample1"" 784650\n""202"" 125 ""Sample2"" 739639\n""203"" 126 ""Sample1"" 775107\n""204"" 126 ""Sample2"" 730152\n""205"" 127 ""Sample1"" 763282\n""206"" 127 ""Sample2"" 713285\n""207"" 128 ""Sample1"" 748652\n""208"" 128 ""Sample2"" 699446\n""209"" 129 ""Sample1"" 736764\n""210"" 129 ""Sample2"" 686165\n""211"" 130 ""Sample1"" 723134\n""212"" 130 ""Sample2"" 671281\n""213"" 131 ""Sample1"" 710686\n""214"" 131 ""Sample2"" 658314\n""215"" 132 ""Sample1"" 701834\n""216"" 132 ""Sample2"" 650564\n""217"" 133 ""Sample1"" 689531\n""218"" 133 ""Sample2"" 635331\n""219"" 134 ""Sample1"" 678070\n""220"" 134 ""Sample2"" 628475\n""221"" 135 ""Sample1"" 668757\n""222"" 135 ""Sample2"" 617210\n""223"" 136 ""Sample1"" 654506\n""224"" 136 ""Sample2"" 601336\n""225"" 137 ""Sample1"" 644720\n""226"" 137 ""Sample2"" 592700\n""227"" 138 ""Sample1"" 630305\n""228"" 138 ""Sample2"" 579707\n""229"" 139 ""Sample1"" 615571\n""230"" 139 ""Sample2"" 562022\n""231"" 140 ""Sample1"" 603896\n""232"" 140 ""Sample2"" 550812\n""233"" 141 ""Sample1"" 597816\n""234"" 141 ""Sample2"" 540016\n""235"" 142 ""Sample1"" 584121\n""236"" 142 ""Sample2"" 526066\n""237"" 143 ""Sample1"" 571689\n""238"" 143 ""Sample2"" 516774\n""239"" 144 ""Sample1"" 564142\n""240"" 144 ""Sample2"" 509033\n""241"" 145 ""Sample1"" 552281\n""242"" 145 ""Sample2"" 496389\n""243"" 146 ""Sample1"" 543184\n""244"" 146 ""Sample2"" 487283\n""245"" 147 ""Sample1"" 532683\n""246"" 147 ""Sample2"" 477035\n""247"" 148 ""Sample1"" 520019\n""248"" 148 ""Sample2"" 461698\n""249"" 149 ""Sample1"" 510484\n""250"" 149 ""Sample2"" 451095\n""251"" 150 ""Sample1"" 507832\n""252"" 150 ""Sample2"" 446390\n""253"" 151 ""Sample1"" 495343\n""254"" 151 ""Sample2"" 431015\n""255"" 152 ""Sample1"" 484169\n""256"" 152 ""Sample2"" 417551\n""257"" 153 ""Sample1"" 471422\n""258"" 153 ""Sample2"" 404674\n""259"" 154 ""Sample1"" 460944\n""260"" 154 ""Sample2"" 390280\n""261"" 155 ""Sample1"" 448227\n""262"" 155 ""Sample2"" 380134\n""263"" 156 ""Sample1"" 437738\n""264"" 156 ""Sample2"" 369703\n""265"" 157 ""Sample1"" 428359\n""266"" 157 ""Sample2"" 354902\n""267"" 158 ""Sample1"" 417445\n""268"" 158 ""Sample2"" 343343\n""269"" 159 ""Sample1"" 408065\n""270"" 159 ""Sample2"" 331193\n""271"" 160 ""Sample1"" 395244\n""272"" 160 ""Sample2"" 315775\n""273"" 161 ""Sample1"" 386510\n""274"" 161 ""Sample2"" 305048\n""275"" 162 ""Sample1"" 377028\n""276"" 162 ""Sample2"" 292336\n""277"" 163 ""Sample1"" 366563\n""278"" 163 ""Sample2"" 277876\n""279"" 164 ""Sample1"" 357659\n""280"" 164 ""Sample2"" 267642\n""281"" 165 ""Sample1"" 348242\n""282"" 165 ""Sample2"" 255871\n""283"" 166 ""Sample1"" 337553\n""284"" 166 ""Sample2"" 242636\n""285"" 167 ""Sample1"" 327824\n""286"" 167 ""Sample2"" 231054\n""287"" 168 ""Sample1"" 317584\n""288"" 168 ""Sample2"" 219066\n""289"" 169 ""Sample1"" 306454\n""290"" 169 ""Sample2"" 204829\n""291"" 170 ""Sample1"" 295255\n""292"" 170 ""Sample2"" 192423\n""293"" 171 ""Sample1"" 284770\n""294"" 171 ""Sample2"" 180440\n""295"" 172 ""Sample1"" 274444\n""296"" 172 ""Sample2"" 166529\n""297"" 173 ""Sample1"" 264469\n""298"" 173 ""Sample2"" 155605\n""299"" 174 ""Sample1"" 253923\n""300"" 174 ""Sample2"" 145166\n""301"" 175 ""Sample1"" 243462\n""302"" 175 ""Sample2"" 134932\n""303"" 176 ""Sample1"" 233021\n""304"" 176 ""Sample2"" 124474\n""305"" 177 ""Sample1"" 223360\n""306"" 177 ""Sample2"" 114975\n""307"" 178 ""Sample1"" 213341\n""308"" 178 ""Sample2"" 105267\n""309"" 179 ""Sample1"" 202191\n""310"" 179 ""Sample2"" 95632\n""311"" 180 ""Sample1"" 193325\n""312"" 180 ""Sample2"" 87027\n""313"" 181 ""Sample1"" 182022\n""314"" 181 ""Sample2"" 78802\n""315"" 182 ""Sample1"" 173011\n""316"" 182 ""Sample2"" 71116\n""317"" 183 ""Sample1"" 164207\n""318"" 183 ""Sample2"" 64758\n""319"" 184 ""Sample1"" 154985\n""320"" 184 ""Sample2"" 57082\n""321"" 185 ""Sample1"" 147070\n""322"" 185 ""Sample2"" 51728\n""323"" 186 ""Sample1"" 138927\n""324"" 186 ""Sample2"" 46295\n""325"" 187 ""Sample1"" 130203\n""326"" 187 ""Sample2"" 40908\n""327"" 188 ""Sample1"" 122935\n""328"" 188 ""Sample2"" 36275\n""329"" 189 ""Sample1"" 115565\n""330"" 189 ""Sample2"" 31825\n""331"" 190 ""Sample1"" 107969\n""332"" 190 ""Sample2"" 28028\n""333"" 191 ""Sample1"" 100995\n""334"" 191 ""Sample2"" 24447\n""335"" 192 ""Sample1"" 94510\n""336"" 192 ""Sample2"" 21125\n""337"" 193 ""Sample1"" 88365\n""338"" 193 ""Sample2"" 18489\n""339"" 194 ""Sample1"" 82247\n""340"" 194 ""Sample2"" 15878\n""341"" 195 ""Sample1"" 77315\n""342"" 195 ""Sample2"" 13881\n""343"" 196 ""Sample1"" 71657\n""344"" 196 ""Sample2"" 12183\n""345"" 197 ""Sample1"" 66920\n""346"" 197 ""Sample2"" 10305\n""347"" 198 ""Sample1"" 61298\n""348"" 198 ""Sample2"" 9096\n""349"" 199 ""Sample1"" 57765\n""350"" 199 ""Sample2"" 7759\n""351"" 200 ""Sample1"" 53811\n""352"" 200 ""Sample2"" 6645\n""353"" 201 ""Sample1"" 48906\n""354"" 201 ""Sample2"" 5532\n""355"" 202 ""Sample1"" 44847\n""356"" 202 ""Sample2"" 4642\n""357"" 203 ""Sample1"" 41546\n""358"" 203 ""Sample2"" 3997\n""359"" 204 ""Sample1"" 38125\n""360"" 204 ""Sample2"" 3360\n""361"" 205 ""Sample1"" 35039\n""362"" 205 ""Sample2"" 2858\n""363"" 206 ""Sample1"" 31902\n""364"" 206 ""Sample2"" 2447\n""365"" 207 ""Sample1"" 29076\n""366"" 207 ""Sample2"" 2185\n""367"" 208 ""Sample1"" 26271\n""368"" 208 ""Sample2"" 1811\n""369"" 209 ""Sample1"" 24037\n""370"" 209 ""Sample2"" 1494\n""371"" 210 ""Sample1"" 21487\n""372"" 210 ""Sample2"" 1313\n""373"" 211 ""Sample1"" 19187\n""374"" 211 ""Sample2"" 1028\n""375"" 212 ""Sample1"" 17317\n""376"" 212 ""Sample2"" 954\n""377"" 213 ""Sample1"" 15741\n""378"" 213 ""Sample2"" 819\n""379"" 214 ""Sample1"" 14000\n""380"" 214 ""Sample2"" 693\n""381"" 215 ""Sample1"" 12494\n""382"" 215 ""Sample2"" 614\n""383"" 216 ""Sample1"" 10963\n""384"" 216 ""Sample2"" 556\n""385"" 217 ""Sample1"" 9928\n""386"" 217 ""Sample2"" 549\n""387"" 218 ""Sample1"" 8652\n""388"" 218 ""Sample2"" 421\n""389"" 219 ""Sample1"" 7604\n""390"" 219 ""Sample2"" 404\n""391"" 220 ""Sample1"" 6515\n""392"" 220 ""Sample2"" 394\n""393"" 221 ""Sample1"" 5735\n""394"" 221 ""Sample2"" 325\n""395"" 222 ""Sample1"" 4783\n""396"" 222 ""Sample2"" 301\n""397"" 223 ""Sample1"" 4108\n""398"" 223 ""Sample2"" 308\n""399"" 224 ""Sample1"" 3611\n""400"" 224 ""Sample2"" 290\n""401"" 225 ""Sample1"" 3224\n""402"" 225 ""Sample2"" 307\n""403"" 226 ""Sample1"" 2676\n""404"" 226 ""Sample2"" 304\n""405"" 227 ""Sample1"" 2261\n""406"" 227 ""Sample2"" 273\n""407"" 228 ""Sample1"" 1936\n""408"" 228 ""Sample2"" 303\n""409"" 229 ""Sample1"" 1664\n""410"" 229 ""Sample2"" 302\n""411"" 230 ""Sample1"" 1529\n""412"" 230 ""Sample2"" 291\n""413"" 231 ""Sample1"" 1180\n""414"" 231 ""Sample2"" 313\n""415"" 232 ""Sample1"" 975\n""416"" 232 ""Sample2"" 288\n""417"" 233 ""Sample1"" 897\n""418"" 233 ""Sample2"" 274\n""419"" 234 ""Sample1"" 812\n""420"" 234 ""Sample2"" 281\n""421"" 235 ""Sample1"" 712\n""422"" 235 ""Sample2"" 318\n""423"" 236 ""Sample1"" 587\n""424"" 236 ""Sample2"" 288\n""425"" 237 ""Sample1"" 555\n""426"" 237 ""Sample2"" 299\n""427"" 238 ""Sample1"" 524\n""428"" 238 ""Sample2"" 314\n""429"" 239 ""Sample1"" 443\n""430"" 239 ""Sample2"" 336\n""431"" 240 ""Sample1"" 411\n""432"" 240 ""Sample2"" 354\n""433"" 241 ""Sample1"" 382\n""434"" 241 ""Sample2"" 339\n""435"" 242 ""Sample1"" 344\n""436"" 242 ""Sample2"" 342\n""437"" 243 ""Sample1"" 340\n""438"" 243 ""Sample2"" 366\n""439"" 244 ""Sample1"" 362\n""440"" 244 ""Sample2"" 353\n""441"" 245 ""Sample1"" 325\n""442"" 245 ""Sample2"" 373\n""443"" 246 ""Sample1"" 290\n""444"" 246 ""Sample2"" 340\n""445"" 247 ""Sample1"" 319\n""446"" 247 ""Sample2"" 345\n""447"" 248 ""Sample1"" 293\n""448"" 248 ""Sample2"" 341\n""449"" 249 ""Sample1"" 304\n""450"" 249 ""Sample2"" 365\n""451"" 250 ""Sample1"" 354\n""452"" 250 ""Sample2"" 381\n""453"" 251 ""Sample1"" 296\n""454"" 251 ""Sample2"" 366\n""455"" 252 ""Sample1"" 315\n""456"" 252 ""Sample2"" 415\n""457"" 253 ""Sample1"" 315\n""458"" 253 ""Sample2"" 399\n""459"" 254 ""Sample1"" 331\n""460"" 254 ""Sample2"" 377\n""461"" 255 ""Sample1"" 321\n""462"" 255 ""Sample2"" 410\n""463"" 256 ""Sample1"" 326\n""464"" 256 ""Sample2"" 421\n""465"" 257 ""Sample1"" 349\n""466"" 257 ""Sample2"" 433\n""467"" 258 ""Sample1"" 370\n""468"" 258 ""Sample2"" 429\n""469"" 259 ""Sample1"" 318\n""470"" 259 ""Sample2"" 439\n""471"" 260 ""Sample1"" 344\n""472"" 260 ""Sample2"" 433\n""473"" 261 ""Sample1"" 346\n""474"" 261 ""Sample2"" 400\n""475"" 262 ""Sample1"" 341\n""476"" 262 ""Sample2"" 419\n""477"" 263 ""Sample1"" 345\n""478"" 263 ""Sample2"" 499\n""479"" 264 ""Sample1"" 348\n""480"" 264 ""Sample2"" 418\n""481"" 265 ""Sample1"" 371\n""482"" 265 ""Sample2"" 454\n""483"" 266 ""Sample1"" 405\n""484"" 266 ""Sample2"" 441\n""485"" 267 ""Sample1"" 380\n""486"" 267 ""Sample2"" 476\n""487"" 268 ""Sample1"" 365\n""488"" 268 ""Sample2"" 464\n""489"" 269 ""Sample1"" 366\n""490"" 269 ""Sample2"" 461\n""491"" 270 ""Sample1"" 351\n""492"" 270 ""Sample2"" 483\n""493"" 271 ""Sample1"" 431\n""494"" 271 ""Sample2"" 481\n""495"" 272 ""Sample1"" 340\n""496"" 272 ""Sample2"" 503\n""497"" 273 ""Sample1"" 405\n""498"" 273 ""Sample2"" 493\n""499"" 274 ""Sample1"" 370\n""500"" 274 ""Sample2"" 529\n""501"" 275 ""Sample1"" 374\n""502"" 275 ""Sample2"" 469\n""503"" 276 ""Sample1"" 385\n""504"" 276 ""Sample2"" 503\n""505"" 277 ""Sample1"" 361\n""506"" 277 ""Sample2"" 496\n""507"" 278 ""Sample1"" 410\n""508"" 278 ""Sample2"" 491\n""509"" 279 ""Sample1"" 410\n""510"" 279 ""Sample2"" 521\n""511"" 280 ""Sample1"" 368\n""512"" 280 ""Sample2"" 514\n""513"" 281 ""Sample1"" 397\n""514"" 281 ""Sample2"" 501\n""515"" 282 ""Sample1"" 379\n""516"" 282 ""Sample2"" 546\n""517"" 283 ""Sample1"" 384\n""518"" 283 ""Sample2"" 502\n""519"" 284 ""Sample1"" 389\n""520"" 284 ""Sample2"" 513\n""521"" 285 ""Sample1"" 419\n""522"" 285 ""Sample2"" 571\n""523"" 286 ""Sample1"" 403\n""524"" 286 ""Sample2"" 519\n""525"" 287 ""Sample1"" 423\n""526"" 287 ""Sample2"" 536\n""527"" 288 ""Sample1"" 480\n""528"" 288 ""Sample2"" 653\n""529"" 289 ""Sample1"" 478\n""530"" 289 ""Sample2"" 564\n\n</code></pre>\n","6797","","73","2020-01-21T22:42:57.427","2020-01-28T15:13:07.623","Boxplot of read length distributions from multiple samples","<r><ggplot2>","3","0","",""
"11197","1","","","2020-01-20T23:17:09.230","0","46","<p>I want to create a stacked bar-graph with different cell cycles for each cell type within each condition. I have uploaded the file for it. Don't know how to go about it as now I have another condition (cell cycle). I was able to do the bar graph comparing cell cycle per condition but now I want to compare cell type and cell cycle in each condition</p>\n\n<p>output of dput(head(all.combined@metadata))</p>\n\n<pre><code>structure(list(orig.ident = c(""treated"", ""treated"", ""treated"", \n    ""treated"", ""treated"", ""treated""), nCount_RNA = c(1892, 307, 1348, \n    3699, 4205, 4468), nFeature_RNA = c(960L, 243L, 765L, 1612L, \n    1341L, 1644L), percent.mt = c(0.211416490486258, 1.62866449511401, \n    4.45103857566766, 4.4065963773993, 0.0713436385255648, 3.87197851387645\n    ), RNA_snn_res.0.5 = structure(c(11L, 11L, 5L, 6L, 11L, 13L), .Label = c(""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", \n    ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19""), class = ""factor""), seurat_clusters = structure(c(11L, 11L, 5L, 6L, 11L, 13L), .Label = c(""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19""), class = ""factor""), S.Score = c(0.476893835992198, -0.0200784617568548, -0.0335915198305002, -0.0247184276246385, 0.010785196602457, 0.0190008903712199), G2M.Score = c(0.204441469200986, 0.173804859670862, -0.0313235510969097, -0.0376796363661889, -0.0559526905696905, -0.0122031631356698), Phase = structure(c(3L, 2L, 1L, 1L, 3L, 3L), .Label = c(""G1"", ""G2M"", ""S""), class = ""factor""), old.ident = structure(c(7L,7L, 1L, 4L, 7L, 9L), .Label = c(""Fibroblast"", ""T cell"", ""Macrophage"", ""Stellate"", ""Acinar"", ""Endothelial"", ""Tumor"", ""B cell"", ""Mast cell"", ""Ductal"", ""Islets of Langerhans""), class = ""factor"")), row.names = c(""treated_AAACGCTAGCGGGTTA-1"", ""treated_AAAGGTAAGTACAGAT-1"", ""treated_AAAGTGAGTTTGATCG-1"", ""treated_AAATGGACAAAGTGTA-1"", \n    ""treated_AACAAAGGTCGACTTA-1"", ""treated_AACAGGGTCCTAGCCT-1""), class = ""data.frame"")\n</code></pre>\n\n<p>output of dput(tail(all.combined@metadata))</p>\n\n<pre><code>structure(list(orig.ident = c(""untreated"", ""untreated"", ""untreated"", \n""untreated"", ""untreated"", ""untreated""), nCount_RNA = c(901, 823, \n1184, 1835, 1147, 1407), nFeature_RNA = c(482L, 479L, 649L, 1043L, \n604L, 709L), percent.mt = c(1.77580466148724, 2.91616038882138, \n4.22297297297297, 3.86920980926431, 2.0052310374891, 4.05117270788913\n), RNA_snn_res.0.5 = structure(c(7L, 7L, 7L, 14L, 7L, 7L), .Label = c(""0"", \n""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", \n""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19""), class = ""factor""), \n    seurat_clusters = structure(c(7L, 7L, 7L, 14L, 7L, 7L), .Label = c(""0"", \n    ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", \n    ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19""), class = ""factor""), \n    S.Score = c(-0.0320858200243315, 0.0304725660342869, 0.0215996091745327, \n    0.0384166213301423, 0.144956251122548, -0.0242770509986111\n    ), G2M.Score = c(0.0904224391544142, 0.050148242050667, -0.0178041670730754, \n    -0.0112596867977946, -0.0519554524339088, -0.0136533184257381\n    ), Phase = structure(c(2L, 2L, 3L, 3L, 3L, 1L), .Label = c(""G1"", \n    ""G2M"", ""S""), class = ""factor""), old.ident = structure(c(5L, \n    5L, 5L, 5L, 5L, 5L), .Label = c(""Fibroblast"", ""T cell"", ""Macrophage"", \n    ""Stellate"", ""Acinar"", ""Endothelial"", ""Tumor"", ""B cell"", ""Mast cell"", \n    ""Ductal"", ""Islets of Langerhans""), class = ""factor"")), row.names = c(""untreated_TTTGGTTGTCTAATCG-18"", \n""untreated_TTTGGTTTCCCGAGGT-18"", ""untreated_TTTGTTGAGAACTGAT-18"", \n""untreated_TTTGTTGAGCTCGGCT-18"", ""untreated_TTTGTTGAGTGCCTCG-18"", \n""untreated_TTTGTTGCACGGTGCT-18""), class = ""data.frame"")\n</code></pre>\n\n<p>Been using this code to generate the previous graph.</p>\n\n<pre><code>ggplot(CC, aes(x = Condition, y = Percent, fill = Cell_Cycle))+\n  geom_bar(stat = ""identity"")+\n  geom_text(aes(label = paste(round(Percent,2),""%"")), position = position_stack(vjust =  0.5))\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/fkecc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fkecc.png"" alt=""cell cycle by condition""></a></p>\n","6774","","6774","2020-01-20T23:47:46.657","2020-01-21T00:11:07.753","Stacked barplot for single cell analysis","<single-cell><ggplot2>","1","4","",""
"11198","2","","11196","2020-01-20T23:29:28.737","1","","<p>My first recommendation would be to use (i.e. ask for) the indidividual-level data, which is what ggplot2 expects.</p>\n\n<p><strong>Binned Data</strong></p>\n\n<p>If that's not possible, binned data can be shoehorned into a ggplot2 boxplot by forcing it back into individual values. My example here adds new unbinned rows to the table with a count of -1, then removes rows with a positive count:</p>\n\n<pre><code>library(tidyverse)\nlibrary(ggplot2)\n\nread_table2(""11196_input.txt"") %&gt;%\n    pivot_longer(cols=c(""Sample1"", ""Sample2""),\n                 names_to=""Sample"", values_to=""Count"") %&gt;%\n    bind_rows(tibble(Length=rep(.<span class=""math-container"">$Length, .$</span>Count),\n                     Sample=rep(.<span class=""math-container"">$Sample, .$</span>Count),\n                     Count=rep(.<span class=""math-container"">$Length * 0 - 1, .$</span>Count))) %&gt;%\n    filter(Count &lt; 0) %&gt;%\n    select(-Count) -&gt; data.tbl\n\ndata.tbl %&gt;%\n    ggplot() +\n    aes(x = Sample, y = Length) +\n    geom_boxplot()\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/iIgah.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iIgah.png"" alt=""boxplot of binned data""></a></p>\n\n<p>It would be possible to make the boxplot look more normal by modifying the <code>rep</code> functions to create values based on a distribution (e.g. using <code>runif</code> or <code>rnorm</code>), but that gets quite tricky because (for example) binned normal data would have different probability distributions for each bin.</p>\n\n<p><strong>Individual Values</strong></p>\n\n<p>If, on the other hand, individual values are available in long format, creating a boxplot becomes much easier because the data fits the format expected by ggplot2:</p>\n\n<pre><code>library(tidyverse)\nlibrary(ggplot2)\n\nread.table(""11196_input2.txt"") -&gt; data.tbl\n\ndata.tbl %&gt;%\n    ggplot() +\n    aes(x = name, y = Readlength) +\n    geom_boxplot()\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/FeYEH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FeYEH.png"" alt=""boxplot of raw data""></a></p>\n","73","","73","2020-01-21T22:42:19.643","2020-01-21T22:42:19.643","","","","2","",""
"11199","2","","11196","2020-01-20T23:33:52.690","1","","<p>The plot you pasted in your question has two layers of information: i) fragment (or read) length, ii) sample.</p>\n\n<p>To be able to draw the same plot with your data using <code>ggplot2</code>, you will need your data to be in the ""long format"" (your data as you presented now is in the ""wide format""):</p>\n\n<pre><code>library(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nmy_data &lt;- ""\nLength Sample1 Sample2\n20 10 5\n30 15 223\n40 16 4923\n50 293 239\n60 3290 23\n70 248 6\n80 23 1\n90 21 0\n100 5 0\n""\n\nmy_data &lt;- read_delim(my_data, delim = "" "")\n\nmy_data_long &lt;- pivot_longer(my_data, cols = c(""Sample1"", ""Sample2""))\n\n&gt; my_data_long\n# A tibble: 18 x 3\n   Length name    value\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1     20 Sample1    10\n 2     20 Sample2     5\n 3     30 Sample1    15\n 4     30 Sample2   223\n 5     40 Sample1    16\n 6     40 Sample2  4923\n 7     50 Sample1   293\n 8     50 Sample2   239\n 9     60 Sample1  3290\n10     60 Sample2    23\n11     70 Sample1   248\n12     70 Sample2     6\n13     80 Sample1    23\n14     80 Sample2     1\n15     90 Sample1    21\n16     90 Sample2     0\n17    100 Sample1     5\n18    100 Sample2     0\n</code></pre>\n\n<p>Moreover, boxplots are meaningful when you would like to visualize distributions, in your case you don't have all of your data points but only a summary of those, counts of read lengths. The best I could think of was using a third aesthetic to reflect the counts (after removing read lengths that were not observed):</p>\n\n<pre><code># remove read lengths that are not observed\nmy_data_long &lt;- my_data_long[my_data_long$value != 0,]\n\nggplot(my_data_long,\n       aes(x = name, y = Length)) +\n  geom_boxplot() +\n  geom_point(aes(size = value))\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/McA5k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/McA5k.png"" alt=""enter image description here""></a></p>\n\n<p>EDIT: In a boxplot, the quantiles reflect the number of data points but the way I draw the boxplot (using <code>Length</code> and <code>name</code> columns) do not make use of such information. A better way of visualising the data would be the following (and is not that different from what @gringer suggested):</p>\n\n<pre><code>ggplot(my_data_long,\n       aes(x = name, y = Length)) +\n  geom_point(aes(size = value))\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/8HbzT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8HbzT.png"" alt=""enter image description here""></a></p>\n","3541","","3541","2020-01-20T23:50:39.763","2020-01-20T23:50:39.763","","","","0","",""
"11200","2","","11197","2020-01-21T00:11:07.753","1","","<p>Based on your example, you can use count for each phase of cell cycle for each cell types in each condition. Here, I'm using <code>dplyr</code> package to do that but most likely, you could have the same output using various methods:</p>\n\n<pre class=""lang-r prettyprint-override""><code>DF &lt;- rbind(dfhead, dftail)\n\nlibrary(dplyr)\nDF_Count &lt;- DF %&gt;%group_by(orig.ident,Phase,old.ident) %&gt;%\n  count() %&gt;%\n  ungroup() %&gt;%\n  group_by(orig.ident,old.ident) %&gt;%\n  mutate(Freq = n/sum(n)*100)\n\n# A tibble: 8 x 5\n# Groups:   orig.ident, old.ident [5]\n  orig.ident Phase old.ident      n  Freq\n  &lt;chr&gt;      &lt;fct&gt; &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;\n1 treated    G1    Fibroblast     1 100  \n2 treated    G1    Stellate       1 100  \n3 treated    G2M   Tumor          1  33.3\n4 treated    S     Tumor          2  66.7\n5 treated    S     Mast cell      1 100  \n6 untreated  G1    Acinar         1  16.7\n7 untreated  G2M   Acinar         2  33.3\n8 untreated  S     Acinar         3  50  \n</code></pre>\n\n<p>As you can see, <code>DF_Count</code> has the frequency for each cell types of each phase of the cell cycle in function of the condition. We can use <code>DF_Count</code> to get the following plot. Using <code>facet_wrap</code>, you can create two panels based on the condition column and thus represent the cell cycle of each cell types in function of the treatment condition:</p>\n\n<pre class=""lang-r prettyprint-override""><code>ggplot(DF_Count, aes(x = old.ident, y = Freq, fill = Phase))+\n  geom_col()+\n  geom_text(aes(label = paste(round(Freq, 2),""%"")),position = position_stack(vjust = 0.5))+\n  facet_wrap(~orig.ident)\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/hcjcz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hcjcz.png"" alt=""enter image description here""></a></p>\n\n<p>Does it look what you are expecting ? </p>\n\n<p>NB: <code>DF</code> here is the addition of your head and tail of your <code>all.combined</code> dataframe.</p>\n","6437","","","","2020-01-21T00:11:07.753","","","","2","",""
"11201","1","11204","","2020-01-21T09:11:14.040","0","37","<p>I know that with GRCh38, it is possible to extract the matching RefSeq transcripts for Ensembl genes/transcripts/proteins using Biomart from Ensembl. </p>\n\n<p>However, for GRCh37, this option does not exists. Using the following MySQL command returns Ensembl transcript IDs for matching RefSeq transcripts:</p>\n\n<pre><code>mysql -u anonymous -h ensembldb.ensembl.org -A -P 3337 -D homo_sapiens_core_99_37 -e ""SELECT transcript.stable_id, xref.display_label FROM transcript, object_xref, xref,external_db WHERE transcript.transcript_id = object_xref.ensembl_id AND object_xref.ensembl_object_type = 'Transcript' AND object_xref.xref_id = xref.xref_id AND xref.external_db_id = external_db.external_db_id AND external_db.db_name = 'RefSeq_mRNA';""\n</code></pre>\n\n<p>Also, I was trying to play around with the <a href=""https://www.ensembl.org/info/docs/api/core/diagrams/Core.svg"" rel=""nofollow noreferrer"">databases schema</a> but I am not able to find any solution to problem.</p>\n\n<p>Ultimately, I wish to obtain a table like the following:</p>\n\n<pre><code>Gene stable ID      Transcript stable ID    Protein stable ID   RefSeq match transcript\nENSG00000120329     ENST00000239451         ENSP00000239451     NM_031947.4\nENSG00000164219     ENST00000419445         ENSP00000404676     NM_005023.4\nENSG00000150594     ENST00000280155         ENSP00000280155     NM_000681.4\nENSG00000113578     ENST00000337706         ENSP00000338548     NM_000800.5\n</code></pre>\n\n<p>Thank you in advance.</p>\n","4413","","","","2020-01-22T20:47:01.650","Is it possible to obtain RefSeq transcripts from Ensembl database for GRCh37 to match Ensembl Genes?","<ensembl><refseq>","2","0","",""
"11203","2","","11192","2020-01-21T10:39:06.400","2","","<p>I think the python package <a href=""https://bioconvert.readthedocs.io/en/master/user_guide.html#explicit-conversion"" rel=""nofollow noreferrer"">bioconvert</a> does what you want if you don't provide qualities: <a href=""https://github.com/bioconvert/bioconvert/blob/master/bioconvert/fasta2fastq.py"" rel=""nofollow noreferrer"">https://github.com/bioconvert/bioconvert/blob/master/bioconvert/fasta2fastq.py</a></p>\n\n<p>The following outputs lots of warnings for me, but does generate a fastq file <code>test.fq</code> from an existing fasta file <code>test.fa</code> (using dummy quality ""I"", and adding ""None"" to sequence headers that do not have a comment part in the original fasta):</p>\n\n<pre><code>bioconvert fasta2fastq test.fa test.fq\n</code></pre>\n\n<p>(This requires installing bioconvert using <code>pip</code> beforehand: <code>pip install bioconvert</code>.)</p>\n","292","","292","2020-01-22T11:45:44.253","2020-01-22T11:45:44.253","","","","0","",""
"11204","2","","11201","2020-01-21T15:44:20.883","3","","<p>The RefSeq match option in BioMart is from the <a href=""http://www.ensembl.org/info/genome/genebuild/mane.html"" rel=""nofollow noreferrer"">Matched Annotation from NCBI and EBI (MANE)</a> collaboration between RefSeq and Ensembl. It has only been calculated for the up-to-date gene annotation on GRCh38 so cannot be obtained on GRCh37. You can get mapping from Ensembl to RefSeq transcripts through BioMart as RefSeq mRNA ID (refseq_mrna in R) but this is not a perfect match like the MANE, it is a mapping based on sequence similarity and similar genomic location, and there can be mismatches between them.</p>\n","1332","","","","2020-01-21T15:44:20.883","","","","2","",""
"11205","2","","11079","2020-01-21T19:29:07.280","1","","<p><code>for i in $(cat text.txt); do cp -v *${i}*vcf mydir; done</code></p>\n","6343","","","","2020-01-21T19:29:07.280","","","","0","",""
"11206","1","","","2020-01-22T03:46:06.183","1","28","<p>I wrote a script which should changes IDs in a GFF3 file. Unfortunately, the below script has two problems.</p>\n\n<ol>\n<li>It attaches the new ID to <code>Parent</code> which leads that <code>Parent</code> contains the old and new id. How is it possible to keep only the new one?</li>\n<li>How is it possible to access the chromosome name?</li>\n</ol>\n\n<p>This how to run the script <code>python renameIDgff3.py --gff3 braker_utr.gff3 --prefix ACTG --output braker_utr-newID.gff3</code></p>\n\n<pre><code>#!/usr/bin/python3\nimport click\nfrom BCBio.GFF import GFFExaminer\nfrom BCBio import GFF\n\n@click.command()\n@click.option('--gff3', help=""Provide GFF3 file"", required=True)\n@click.option('--prefix', help=""e.g. ASSCTG"", required=True)\n@click.option('--output', help=""Keep GFF3 file"", required=True)\ndef run(gff3, prefix, output):\n    print(""Hello"")\n    with open(output, ""w"") as out_handle:\n        for rec in GFF.parse(gff3):\n\n            for count, feature in enumerate(rec.features):\n                print(""count"", count)\n                print(feature)\n                print(feature.qualifiers.get(""Name""))\n                print(feature.sub_features)\n                print(feature.sub_features[0].qualifiers.get(""Name""))\n                print(""!!!!change"")\n                feature.qualifiers[""ID""] = prefix + str(count).zfill(6)\n\n\n                print(feature.sub_features[0].qualifiers[""ID""])\n                id_extension =  feature.sub_features[0].qualifiers[""ID""][0].split('.')[1]\n                feature.sub_features[0].qualifiers[""ID""] = prefix + str(count).zfill(6) + '.' + id_extension\n                print(feature.sub_features[0].qualifiers[""Parent""])\n                print(""-----------"")\n\n            GFF.write([rec], out_handle)\n\nif __name__ == '__main__':\n    run()\n</code></pre>\n\n<p>Input file:</p>\n\n<pre><code>NbV1Ch08    AUGUSTUS    gene    7015    29794   0.01    -   .   ID=g1;\nNbV1Ch08    AUGUSTUS    mRNA    7015    29794   0.01    -   .   ID=g1.t1;Parent=g1\nNbV1Ch08    AUGUSTUS    transcription_end_site  7015    7015    .   -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    three_prime_utr 7015    8531    0.2 -   .   ID=g1.t1.3UTR1;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    7015    8747    .   -   .   ID=g1.t1.exon1;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    stop_codon  8532    8534    .   -   0   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 8532    8747    0.31    -   0   ID=g1.t1.CDS1;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    intron  8748    9191    0.49    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 9192    9342    0.66    -   1   ID=g1.t1.CDS2;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    9192    9342    .   -   .   ID=g1.t1.exon2;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  9343    9915    0.58    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 9916    10006   0.71    -   2   ID=g1.t1.CDS3;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    9916    10006   .   -   .   ID=g1.t1.exon3;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  10007   10101   0.74    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 10102   10201   0.78    -   0   ID=g1.t1.CDS4;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    10102   10201   .   -   .   ID=g1.t1.exon4;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  10202   10712   0.8 -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 10713   11107   0.11    -   2   ID=g1.t1.CDS5;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    10713   11107   .   -   .   ID=g1.t1.exon5;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  11108   11569   0.07    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 11570   12151   0.09    -   2   ID=g1.t1.CDS6;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    11570   12151   .   -   .   ID=g1.t1.exon6;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  12152   12588   0.34    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 12589   12717   0.39    -   2   ID=g1.t1.CDS7;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    12589   12717   .   -   .   ID=g1.t1.exon7;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  12718   12789   0.42    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 12790   13075   0.39    -   0   ID=g1.t1.CDS8;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    12790   13075   .   -   .   ID=g1.t1.exon8;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  13076   14832   0.51    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 14833   15009   0.39    -   0   ID=g1.t1.CDS9;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    14833   15009   .   -   .   ID=g1.t1.exon9;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  15010   15278   0.59    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 15279   15415   0.56    -   2   ID=g1.t1.CDS10;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    15279   15415   .   -   .   ID=g1.t1.exon10;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  15416   15487   0.58    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 15488   15612   0.96    -   1   ID=g1.t1.CDS11;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    15488   15612   .   -   .   ID=g1.t1.exon11;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    intron  15613   15706   0.96    -   .   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    CDS 15707   15957   0.98    -   0   ID=g1.t1.CDS12;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    15707   15958   .   -   .   ID=g1.t1.exon12;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    start_codon 15955   15957   .   -   0   Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    five_prime_utr  15958   15958   0.99    -   .   ID=g1.t1.5UTR1;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    five_prime_utr  27458   28250   0.37    -   .   ID=g1.t1.5UTR2;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    27458   28250   .   -   .   ID=g1.t1.exon13;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    five_prime_utr  29272   29794   0.08    -   .   ID=g1.t1.5UTR3;Parent=g1.t1\nNbV1Ch08    AUGUSTUS    exon    29272   29794   .   -   .   ID=g1.t1.exon14;Parent=g1.t1;\nNbV1Ch08    AUGUSTUS    transcription_start_site    29794   29794   .   -   .   Parent=g1.t1;\n</code></pre>\n\n<p>Output file:</p>\n\n<pre><code>##gff-version 3\n##sequence-region NbV1Ch08 1 129222376\nNbV1Ch08    AUGUSTUS    gene    7015    29794   0.01    -   .   ID=ACTG000000\nNbV1Ch08    AUGUSTUS    mRNA    7015    29794   0.01    -   .   ID=ACTG000000.t1;Parent=g1,ACTG000000\nNbV1Ch08    AUGUSTUS    transcription_end_site  7015    7015    .   -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    three_prime_utr 7015    8531    0.2 -   .   ID=g1.t1.3UTR1;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    7015    8747    .   -   .   ID=g1.t1.exon1;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    stop_codon  8532    8534    .   -   0   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 8532    8747    0.31    -   0   ID=g1.t1.CDS1;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  8748    9191    0.49    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 9192    9342    0.66    -   1   ID=g1.t1.CDS2;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    9192    9342    .   -   .   ID=g1.t1.exon2;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  9343    9915    0.58    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 9916    10006   0.71    -   2   ID=g1.t1.CDS3;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    9916    10006   .   -   .   ID=g1.t1.exon3;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  10007   10101   0.74    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 10102   10201   0.78    -   0   ID=g1.t1.CDS4;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    10102   10201   .   -   .   ID=g1.t1.exon4;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  10202   10712   0.8 -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 10713   11107   0.11    -   2   ID=g1.t1.CDS5;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    10713   11107   .   -   .   ID=g1.t1.exon5;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  11108   11569   0.07    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 11570   12151   0.09    -   2   ID=g1.t1.CDS6;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    11570   12151   .   -   .   ID=g1.t1.exon6;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  12152   12588   0.34    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 12589   12717   0.39    -   2   ID=g1.t1.CDS7;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    12589   12717   .   -   .   ID=g1.t1.exon7;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  12718   12789   0.42    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 12790   13075   0.39    -   0   ID=g1.t1.CDS8;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    12790   13075   .   -   .   ID=g1.t1.exon8;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  13076   14832   0.51    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 14833   15009   0.39    -   0   ID=g1.t1.CDS9;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    14833   15009   .   -   .   ID=g1.t1.exon9;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  15010   15278   0.59    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 15279   15415   0.56    -   2   ID=g1.t1.CDS10;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    15279   15415   .   -   .   ID=g1.t1.exon10;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  15416   15487   0.58    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 15488   15612   0.96    -   1   ID=g1.t1.CDS11;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    15488   15612   .   -   .   ID=g1.t1.exon11;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    intron  15613   15706   0.96    -   .   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    CDS 15707   15957   0.98    -   0   ID=g1.t1.CDS12;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    15707   15958   .   -   .   ID=g1.t1.exon12;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    start_codon 15955   15957   .   -   0   Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    five_prime_utr  15958   15958   0.99    -   .   ID=g1.t1.5UTR1;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    five_prime_utr  27458   28250   0.37    -   .   ID=g1.t1.5UTR2;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    27458   28250   .   -   .   ID=g1.t1.exon13;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    five_prime_utr  29272   29794   0.08    -   .   ID=g1.t1.5UTR3;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    exon    29272   29794   .   -   .   ID=g1.t1.exon14;Parent=g1.t1,ACTG000000.t1\nNbV1Ch08    AUGUSTUS    transcription_start_site    29794   29794   .   -   .   Parent=g1.t1,ACTG000000.t1\n</code></pre>\n\n<p>Thank you in advance</p>\n","2477","","","","2020-02-06T02:08:25.840","renaming IDs in gff3 with BCBio.GFF","<annotation><biopython><gff3>","1","0","",""
"11207","2","","11201","2020-01-22T04:48:40.857","1","","<p>Try the GRCh37 version of BioMart, and the External reference ""RefSeq mRNA ID"" / ""RefSeq ncRNA ID"":</p>\n\n<p><a href=""https://grch37.ensembl.org/biomart"" rel=""nofollow noreferrer"">https://grch37.ensembl.org/biomart</a></p>\n\n<p>From the Perl code provided by BioMart, it looks like the field name should be ""refseq_mrna"" or ""refseq_ncrna"". I'm not sure how that translates to the SQL code.</p>\n\n<p>I feel compelled to add a caution to this, though. GRCh37 is a very old build, having been <em>replaced</em> by GRCh38 in <a href=""https://en.wikipedia.org/wiki/Reference_genome#Human_reference_genome"" rel=""nofollow noreferrer"">December 2013</a>. If possible, please find a way to use GRCh38 for your analysis instead, and don't create any <em>new</em> software that depends on GRCh37. For example, gnomAD v3 supports GRCh38 (see <a href=""https://bioconductor.org/packages/release/data/annotation/html/MafDb.gnomAD.r3.0.GRCh38.html"" rel=""nofollow noreferrer"">bioconductor</a>, or <a href=""https://macarthurlab.org/2019/10/16/gnomad-v3-0/"" rel=""nofollow noreferrer"">here</a>).</p>\n","73","","73","2020-01-22T20:47:01.650","2020-01-22T20:47:01.650","","","","1","",""
"11208","2","","321","2020-01-22T10:32:17.390","0","","<p>In case BQSR is not an option (i.e. non-model organisms) it would be best to use some internal control sequence such as PhiX for illumina platform. Although this is supposed to be common practice some facilities ignore it. In principle, the machines should use these sequences as reference so that the scoring would be more accurate. In my experience the first 10-15 bases of the illumina reads always had lower quality. This can easily be seen in the nucleotide distributions. I would advise trimming the first 10-15 bases and quality based end-trimmingiIf the quality of the individual reads are important, such as low coverage resequencing or de-novo genome assembly applications.</p>\n","6802","","6802","2020-01-22T11:10:16.963","2020-01-22T11:10:16.963","","","","0","",""
"11209","1","11211","","2020-01-22T11:16:01.023","-1","48","<p>I have 3 data frames for treatment of 3 different drugs and in each of them I have the number of mutation types like insertion, deletion, SNP and total of mutations for each patients. In each group I have different number of patients like below</p>\n\n<pre><code>&gt; head(dat1)\n                patient  DEL  INS   SNP MNP total\n1:    LP6008337-DNA_H06  927  773 40756   0 42456\n2:    LP6008334-DNA_D02 1049  799 31009   0 32857\n&gt; dim(dat1)\n[1] 21  6\n&gt; \n\n head(dat2)\n                Patient   DEL  INS   SNP MNP total\n1:    LP6008031-DNA_E01 13552 3374 62105   0 79031\n2:    LP6005500-DNA_G01   539  500 43451   0 44490\n&gt; dim(dat2)\n[1] 33  6\n&gt; \n\n&gt; head(dat3)\n                Patient   DEL   INS    SNP MNP  total\n1:    LP6005935-DNA_F03 39168 16739  58095   0 114002\n2:    LP6008269-DNA_D08   849   910 103501  11 105271\n\n&gt; dim(dat3)\n[1] 106   6\n</code></pre>\n\n<p>I want to show if the number of each mutational category and total is different for each drug by boxplot, violinplot or any type of visualization</p>\n\n<p>For example this picture is very nice <a href=""https://i.stack.imgur.com/Kss8C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kss8C.png"" alt=""enter image description here""></a></p>\n\n<p>Can you help me?</p>\n","4595","","6723","2020-01-23T11:02:31.247","2020-01-23T11:09:04.033","Making a boxplot or violinplot for several dataframe with different number of samples","<r><visualization><ggplot2><wgs>","2","0","",""
"11210","2","","11209","2020-01-22T12:47:52.560","1","","<p>If you use a violin plot instead and ggplot2 to make it then:</p>\n\n<pre><code>geom_violin(...not relevant stuff..., scale=""count"")\n</code></pre>\n\n<p>will scale each violin plot according to the number of actual entries in it. I often find this nice so small groups look super skinny and people don't focus on random changes in them so much.</p>\n","77","","","","2020-01-22T12:47:52.560","","","","2","",""
"11211","2","","11209","2020-01-22T13:13:08.300","2","","<p>To help us help you, please make your data available with <code>dput()</code> next time.</p>\n\n<p>To achieve your goal with <code>ggplot2</code>, you would need all of your data in one data frame and in the ""long format"".</p>\n\n<pre><code>dat1 &lt;- ""\nPatient DEL INS SNP MNP total\nLP6008337-DNA_H06 927 773 40756 0 42456\nLP6008334-DNA_D02 1049 799 31009 0 32857\n""\n\ndat2 &lt;- ""\nPatient DEL INS SNP MNP total\nLP6008031-DNA_E01 13552 3374 62105 0 79031\nLP6005500-DNA_G01 539 500 43451 0 44490\n""\n\ndat3 &lt;- ""\nPatient DEL INS SNP MNP total\nLP6005935-DNA_F03 39168 16739 58095 0 114002\nLP6008269-DNA_D08 849 910 103501 11 105271\n""\n\n# read data and add a column indicating data frame of origin (drug in this case)\ndat1 &lt;- readr::read_delim(dat1, delim = "" "")\ndat1<span class=""math-container"">$drug &lt;- ""drug1""\ndat2 &lt;- readr::read_delim(dat2, delim = "" "")\ndat2$</span>drug &lt;- ""drug2""\ndat3 &lt;- readr::read_delim(dat3, delim = "" "")\ndat3$drug &lt;- ""drug3""\n\n# bind rows\ndat &lt;- rbind(dat1, dat2, dat3)\n\n&gt; dat\n# A tibble: 6 x 7\n  Patient             DEL   INS    SNP   MNP  total drug \n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 LP6008337-DNA_H06   927   773  40756     0  42456 drug1\n2 LP6008334-DNA_D02  1049   799  31009     0  32857 drug1\n3 LP6008031-DNA_E01 13552  3374  62105     0  79031 drug2\n4 LP6005500-DNA_G01   539   500  43451     0  44490 drug2\n5 LP6005935-DNA_F03 39168 16739  58095     0 114002 drug3\n6 LP6008269-DNA_D08   849   910 103501    11 105271 drug3\n\n# convert data to long format, ggplot() expects that\ndat_long &lt;- tidyr::pivot_longer(dat, cols = 2:6)\n\n&gt; dat_long\n# A tibble: 30 x 4\n   Patient           drug  name  value\n   &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 LP6008337-DNA_H06 drug1 DEL     927\n 2 LP6008337-DNA_H06 drug1 INS     773\n 3 LP6008337-DNA_H06 drug1 SNP   40756\n 4 LP6008337-DNA_H06 drug1 MNP       0\n 5 LP6008337-DNA_H06 drug1 total 42456\n 6 LP6008334-DNA_D02 drug1 DEL    1049\n\nggplot(dat_long,\n                aes(x = name, y = value)) +\n  geom_boxplot() +\n  facet_wrap(~drug)\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/xURLG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xURLG.png"" alt=""enter image description here""></a></p>\n\n<p>EDIT: setting <code>scales</code> argument of <code>facet_wrap()</code> to ""free"" could be helpful</p>\n\n<pre><code>ggplot(dat_long,\n       aes(x = name, y = value)) +\n  geom_boxplot() +\n  facet_wrap(~drug, scales = ""free"")\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/SVenb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SVenb.png"" alt=""enter image description here""></a></p>\n","3541","","3541","2020-01-23T11:09:04.033","2020-01-23T11:09:04.033","","","","3","",""
"11212","2","","2378","2020-01-22T13:26:27.567","0","","<p>I've got the RIP-seq analysis outputs. We used an isotype control antibody VS antibody against an RBP to pull down corresponding RNAs! </p>\n\n<p>I did the following pipeline: Hisat2 (mapping the reads after QC) -> htseq-count (getting read counts) -> DESeq2 (getting enriched RNAs in test VS isotype control) -> volcano plot shows up-regulated and down-regulated genes in the test sample. </p>\n\n<p>Logically, I should have seen just enriched RNAs (up-regulated genes), but surprisingly there are some RNAs down-regulated in the test sample (or up-regulated in the isotype control sample)!!!!!\nHow can we explain and interpret this???? it seems some genes are pull down in isotype control sample as well. Are these background? Is my computational pipeline right? </p>\n\n<p>BTW, it's worth to mention that all enriched RNA (called up-regulated genes in DESeq2 pipeline) are significantly relevant and meaningful to my experiment. However, I am not sure how much confident I can be on this output? And how should I explain down-regulated RNAs? because normally we just expect enriched RNAs in RIP-seq experiment.</p>\n\n<p>Any thoughts?</p>\n","6804","","","","2020-01-22T13:26:27.567","","","","0","",""
"11213","2","","11192","2020-01-22T14:42:30.503","0","","<p>Here's a quick one-liner with no dependencies other than the UNIX core utils and Perl.</p>\n\n<pre><code>cat seqs.fasta | paste - - | perl -ne 'chomp; s/^&gt;/@/; @v = split /\t/; printf(""%s\n%s\n+\n%s\n"", $v[0], $v[1], ""B""x length($v[1]))' &gt; seqs.fastq\n</code></pre>\n\n<p>Caveat: assumes your Fasta sequences are not wrapped, i.e. two lines per record.</p>\n","96","","","","2020-01-22T14:42:30.503","","","","0","",""
"11214","1","","","2020-01-22T17:12:39.807","1","24","<p>I searched the Python documentation for libSBML but couldn't find a staightforward answer to this question. For a given SBML model, how can I use Python libSBML to retrieve the values of the <code>id</code> and <code>volume</code> attributes for a given compartment?</p>\n","4894","","4913","2020-02-11T06:35:22.087","2020-02-11T06:35:22.087","How do I get the compartment Id and volume from SBML using libSBML Python?","<sbml>","1","0","",""
"11215","2","","11214","2020-01-22T17:22:44.707","1","","<p>There aren't any examples in the python docs on how to do this. The best bet is to search the C/C++ docs which are more extensive. There is a python example in the docs which shows how to retrieve the number of compartments in a model but it doesn't go further. The code below shows how you can retrieve the information requested, assuming you have a model called testmodel.xml:</p>\n\n<pre><code>import libsbml\n\ndocument = libsbml.readSBML(""testmodel.xml"")\n\nif document.getNumErrors() &gt; 0:\n   print(""Encountered the following SBML errors:"" + ""\n"")\n   document.printErrors()\nelse:       \n   model = document.getModel()\n\n   nCompartments = model.getNumCompartments()\n\n   print (""Number of compartments: "", nCompartments)\n   for i in range (nCompartments):\n       comp = model.getCompartment(i)\n\n       print (""Id = "", comp.getId())\n       print (""Volume = "", comp.getVolume())\n</code></pre>\n","4894","","","","2020-01-22T17:22:44.707","","","","0","",""
"11216","2","","8908","2020-01-22T18:31:42.317","1","","<p>What I wanted to do is called ""Consensus Sequence"". Two steps were needed:</p>\n\n<pre><code>$ bcftools mpileup -Ou -f ref.fa input.bam | bcftools call -Ou -mv | bcftools norm -f ref.fa Oz -o output.vcf.gz\n</code></pre>\n\n<p>After that you can create the consensus:</p>\n\n<pre><code>$ bcftools consensus -f ref.fa output.vcf.gz &gt; out.fa\n</code></pre>\n","4362","","","","2020-01-22T18:31:42.317","","","","0","",""
"11217","2","","7307","2020-01-22T22:58:48.517","0","","<p>The nomenclature is no longer being followed as NCBI is moving all the files to S3/Google cloud. <a href=""https://github.com/saketkc/pysradb"" rel=""nofollow noreferrer"">pysradb</a> allows you to fetch the metadata and get the URLs for each individual SRR. See examples in <a href=""https://colab.research.google.com/drive/1tOVBg7vPnhTYNjyFQTvN0K9jGT3MyrVb"" rel=""nofollow noreferrer"">this notebook</a>.</p>\n\n<p>You can also download an entire project by doing:</p>\n\n<pre><code>pysradb download -p &lt;SRP&gt;\n</code></pre>\n","161","","","","2020-01-22T22:58:48.517","","","","0","",""
"11218","2","","7027","2020-01-22T23:00:28.657","0","","<p>You can get metadata for an entire project using <a href=""https://github.com/saketkc/pysradb"" rel=""nofollow noreferrer"">pysradb</a>:</p>\n\n<pre><code>&gt; pysradb metadata SRP063732 --detailed\n\nstudy_accession experiment_accession experiment_title                                                       experiment_desc                                                        organism_taxid  organism_name   library_strategy library_source library_selection sample_accession sample_title instrument           total_spots total_size   run_accession run_total_spots run_total_bases run_alias             sra_url                                                                                 experiment_alias isolate                 dev_stage sex   tissue             treatment     BioSampleModel           \n SRP063732       SRX1249861           Bisulfite-seq of rhesus macaque 26168_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 26168_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071080                    Illumina HiSeq 2000  89051187    11830170462  SRR2420581    89051187        17988339774     26168_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420581/SRR2420581.1                   26168_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249860           Bisulfite-seq of rhesus macaque 26148_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 26148_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071081                    Illumina HiSeq 2000  87335402    11763248173  SRR2420580    87335402        17641751204     26148_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420580/SRR2420580.1                   26148_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249859           Bisulfite-seq of rhesus macaque 25822_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 25822_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071082                    Illumina HiSeq 2000  83544130    9481342835   SRR2420579    83544130        13784781450     25822_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420579/SRR2420579.1                   25822_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249858           Bisulfite-seq of rhesus macaque 25154_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 25154_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071083                    Illumina HiSeq 2000  115476876   13003165573  SRR2420578    115476876       19053684540     25154_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420578/SRR2420578.1                   25154_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249857           Bisulfite-seq of rhesus macaque 23784_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 23784_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071084                    Illumina HiSeq 2000  101539017   13697691917  SRR2420577    101539017       20510881434     23784_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420577/SRR2420577.1                   23784_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249856           Bisulfite-seq of rhesus macaque 23779_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 23779_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071085                    Illumina HiSeq 2000  98394582    13326784419  SRR2420576    98394582        19875705564     23779_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420576/SRR2420576.1                   23779_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249855           Bisulfite-seq of rhesus macaque 23773_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 23773_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071086                    Illumina HiSeq 2000  84442355    11218164664  SRR2420575    84442355        17057355710     23773_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420575/SRR2420575.1                   23773_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249854           Bisulfite-seq of rhesus macaque 23764_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 23764_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071087                    Illumina HiSeq 2000  87197006    11291883405  SRR2420574    87197006        17613795212     23764_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420574/SRR2420574.1                   23764_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249853           Bisulfite-seq of rhesus macaque 23762_HeavyDrinker: Nucleus accumbens  Bisulfite-seq of rhesus macaque 23762_HeavyDrinker: Nucleus accumbens  9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071088                    Illumina HiSeq 2000  89455608    11917710767  SRR2420573    89455608        18070032816     23762_NaC_HD_files    https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420573/SRR2420573.1                   23762_NucleusAccumbens  Adult     male  Nucleus accumbens  HeavyEthanol  Model organism or animal\n SRP063732       SRX1249852           Bisulfite-seq of rhesus macaque 25811_LowDrinker: Nucleus accumbens    Bisulfite-seq of rhesus macaque 25811_LowDrinker: Nucleus accumbens    9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071089                    Illumina HiSeq 2000  88216999    10285962574  SRR2420572    88216999        14555804835     25811_NaC_LMD _files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420572/SRR2420572.1                   25811_NucleusAccumbens  Adult     male  Nucleus accumbens  LightEthanol  Model organism or animal\n SRP063732       SRX1249851           Bisulfite-seq of rhesus macaque 25790_LowDrinker: Nucleus accumbens    Bisulfite-seq of rhesus macaque 25790_LowDrinker: Nucleus accumbens    9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071090                    Illumina HiSeq 2000  94731241    10748979142  SRR2420571    94731241        15630654765     25790_NaC_LMD _files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420571/SRR2420571.1                   25790_NucleusAccumbens  Adult     male  Nucleus accumbens  LightEthanol  Model organism or animal\n SRP063732       SRX1249850           Bisulfite-seq of rhesus macaque 25742_LowDrinker: Nucleus accumbens    Bisulfite-seq of rhesus macaque 25742_LowDrinker: Nucleus accumbens    9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071091                    Illumina HiSeq 2000  113512952   17943763654  SRR2420570    113512952       22929616304     25742_NaC_LMD _files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420570/SRR2420570.1                   25742_NucleusAccumbens  Adult     male  Nucleus accumbens  LightEthanol  Model organism or animal\n SRP063732       SRX1249849           Bisulfite-seq of rhesus macaque 25240_LowDrinker: Nucleus accumbens    Bisulfite-seq of rhesus macaque 25240_LowDrinker: Nucleus accumbens    9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071092                    Illumina HiSeq 2000  81002663    9232986327   SRR2420569    81002663        13365439395     25240_NaC_LMD _files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420569/SRR2420569.1                   25240_NucleusAccumbens  Adult     male  Nucleus accumbens  LightEthanol  Model organism or animal\n SRP063732       SRX1249848           Bisulfite-seq of rhesus macaque 25184_LowDrinker: Nucleus accumbens    Bisulfite-seq of rhesus macaque 25184_LowDrinker: Nucleus accumbens    9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071093                    Illumina HiSeq 2000  110228135   12594727479  SRR2420568    110228135       18187642275     25184_NaC_LMD _files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420568/SRR2420568.1                   25184_NucleusAccumbens  Adult     male  Nucleus accumbens  LightEthanol  Model organism or animal\n SRP063732       SRX1249847           Bisulfite-seq of rhesus macaque 25157_LowDrinker: Nucleus accumbens    Bisulfite-seq of rhesus macaque 25157_LowDrinker: Nucleus accumbens    9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071094                    Illumina HiSeq 2000  97249804    11026452312  SRR2420567    97249804        16046217660     25157_NaC_LMD _files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420567/SRR2420567.1                   25157_NucleusAccumbens  Adult     male  Nucleus accumbens  LightEthanol  Model organism or animal\n SRP063732       SRX1249846           Bisulfite-seq of rhesus macaque 21119_LowDrinker: Nucleus accumbens    Bisulfite-seq of rhesus macaque 21119_LowDrinker: Nucleus accumbens    9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071095                    Illumina HiSeq 2000  68899204    9241244635   SRR2420566    68899204        13917639208     21119_NaC_LMD _files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420566/SRR2420566.1                   21119_NucleusAccumbens  Adult     male  Nucleus accumbens  LightEthanol  Model organism or animal\n SRP063732       SRX1249845           Bisulfite-seq of rhesus macaque 26104_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 26104_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071096                    Illumina HiSeq 2000  84859783    9604170375   SRR2420565    84859783        14001864195     26104_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420565/SRR2420565.1                   26104_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n SRP063732       SRX1249844           Bisulfite-seq of rhesus macaque 26082_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 26082_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071097                    Illumina HiSeq 2000  73346886    8248399265   SRR2420564    73346886        12102236190     26082_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420564/SRR2420564.1                   26082_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n SRP063732       SRX1249843           Bisulfite-seq of rhesus macaque 25207_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 25207_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071098                    Illumina HiSeq 2000  77306973    12235827725  SRR2420563    77306973        15616008546     25207_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420563/SRR2420563.1                   25207_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n SRP063732       SRX1249842           Bisulfite-seq of rhesus macaque 25407_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 25407_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071099                    Illumina HiSeq 2000  83985968    13355795218  SRR2420562    83985968        16965165536     25407_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420562/SRR2420562.1                   25407_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n SRP063732       SRX1249841           Bisulfite-seq of rhesus macaque 24818_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 24818_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071100                    Illumina HiSeq 2000  72782928    8484613123   SRR2420561    72782928        12009183120     24818_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420561/SRR2420561.1                   24818_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n SRP063732       SRX1249840           Bisulfite-seq of rhesus macaque 24698_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 24698_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071101                    Illumina HiSeq 2000  79414016    12627204820  SRR2420560    79414016        16041631232     24698_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420560/SRR2420560.1                   24698_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n SRP063732       SRX1249839           Bisulfite-seq of rhesus macaque 24340_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 24340_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071102                    Illumina HiSeq 2000  84772576    13436053136  SRR2420559    84772576        17124060352     24340_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420559/SRR2420559.1                   24340_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n SRP063732       SRX1249838           Bisulfite-seq of rhesus macaque 22770_Control: Nucleus accumbens       Bisulfite-seq of rhesus macaque 22770_Control: Nucleus accumbens       9544            Macaca mulatta  Bisulfite-Seq    GENOMIC        other             SRS1071103                    Illumina HiSeq 2000  83310630    13224887149  SRR2420558    83310630        16828747260     22770_NaC_Ctrl_files  https://sra-download.st-va.ncbi.nlm.nih.gov/sos2/sra-pub-run-3/SRR2420558/SRR2420558.1                   22770_NucleusAccumbens  Adult     male  Nucleus accumbens  Control       Model organism or animal\n\n</code></pre>\n","161","","161","2020-01-22T23:06:16.333","2020-01-22T23:06:16.333","","","","0","",""
"11220","1","11221","","2020-01-23T14:03:12.697","-2","36","<p>I have this expression matrix</p>\n\n<pre><code>&gt; head(dat[1:8,1:4])\n  Gene.Symbol        p1        p2         p3\n1       STAT1     7.6159     7.6784     7.4430\n2       STAT1     7.4435     7.2850     7.1250\n3       STAT1     7.8925     7.9269     7.4956\n4       STAT1     7.5867     7.9285     7.6741\n5       GAPDH     8.4861     8.3189     9.0913\n6       GAPDH     9.0206     8.0813     9.2360\n&gt; dim(dat)\n[1] 58324   276\n&gt;\n</code></pre>\n\n<p>As you are seeing, I have repeated genes, so how I take average over expression values for redundant genes finishing with one unique expression value for each gene for each sample in columns? </p>\n","4595","","4595","2020-01-23T14:09:32.170","2020-01-23T14:22:18.627","Calculating mean accross rows with repeated entries in R","<r><microarray>","1","0","",""
"11221","2","","11220","2020-01-23T14:16:15.140","3","","<p><strong>Using <code>group_by</code> from <code>dplyr</code></strong></p>\n\n<p>You can use <code>group_by</code> function from <code>dplyr</code> to calculate the mean of each gene symbol in each samples:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(dplyr)\ndat %&gt;% \n  group_by(Gene.Symbol) %&gt;% \n  summarise_at(vars(contains(""p"")), funs(mean(., na.rm = TRUE)))\n\n# A tibble: 2 x 4\n  Gene.Symbol    p1    p2    p3\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 GAPDH        8.75  8.20  9.16\n2 STAT1        7.63  7.70  7.43\n</code></pre>\n\n<hr>\n\n<p><strong>Using <code>data.table</code></strong></p>\n\n<p>Using <code>data.table</code> to speed up the calculation, you can do:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(data.table)\nsetDT(dat)\ndat[, lapply(.SD, mean), by = Gene.Symbol]\n\n   Gene.Symbol      p1     p2       p3\n1:       STAT1 7.63465 7.7047 7.434425\n2:       GAPDH 8.75335 8.2001 9.163650\n<span class=""math-container"">```</span>\n</code></pre>\n","6437","","6437","2020-01-23T14:22:18.627","2020-01-23T14:22:18.627","","","","0","",""
"11222","1","","","2020-01-23T16:37:35.753","0","58","<p>I'm attempting to plot a stacked barplot with ggplot2 with this code:</p>\n\n<pre class=""lang-r prettyprint-override""><code>ggplot(CC, aes(x = Condition, y = Percent, fill = Cell_Cycle))+\n  geom_bar(stat = ""identity"")+\n  geom_text(aes(label = paste(round(Percent,2),""%"")), position = position_stack(vjust =  0.5))\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/YTjJJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YTjJJ.png"" alt=""stacked bar graph""></a></p>\n\n<p>Question: How do I go about adding error bars to the graph?</p>\n\n<p>I tried this code:</p>\n\n<pre class=""lang-r prettyprint-override""><code>ggplot(DF, aes(x = Condition, y = Percent, fill = Cell_Type))+\n     geom_bar(stat = ""identity"")+\n     geom_text(aes(label = paste(round(Percent,2),""%"")), position = position_stack(vjust =  0.5) + geom_errorbar(aes(ymin=Condition, ymax=Condition), width = .2))\n</code></pre>\n\n<p>But got this error: </p>\n\n<blockquote>\n  <p>Error: Cannot add ggproto objects together.</p>\n</blockquote>\n\n<p>output of dput(DF)</p>\n\n<pre><code>structure(list(Cell_Type = structure(c(1L, 2L, 3L, 4L, 5L, 6L, \n7L, 1L, 2L, 3L, 4L, 5L, 6L, 7L), .Label = c(""Fibroblast"", ""T cell"", \n""Macrophage"", ""Tumor"", ""Islets of Langerhans"", ""Endothelial"", \n""B cell""), class = ""factor""), Condition = structure(c(1L, 1L, \n1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""treated"", \n""untreated""), class = ""factor""), Freq = c(6051L, 1892L, 1133L, \n657L, 116L, 868L, 832L, 5331L, 3757L, 1802L, 835L, 287L, 704L, \n256L), Percent = c(52.3941466793662, 16.3823707680319, 9.8103731924842, \n5.68880422547407, 1.00441596675037, 7.51580223395965, 7.20408693393367, \n41.0962072155412, 28.9623805118717, 13.8914585260561, 6.43694110391613, \n2.21245760098674, 5.42707369719396, 1.97348134443417)), class = c(""grouped_df"", \n""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -14L), groups = structure(list(\n    Condition = structure(1:2, .Label = c(""treated"", ""untreated""\n    ), class = ""factor""), .rows = list(1:7, 8:14)), row.names = c(NA, \n-2L), class = c(""tbl_df"", ""tbl"", ""data.frame""), .drop = TRUE))\n</code></pre>\n","6810","","6810","2020-01-23T19:17:47.933","2020-01-23T20:00:13.053","stacked bargraph with error bars","<ggplot2>","1","10","",""
"11223","1","11252","","2020-01-23T18:51:25.313","0","47","<p>I'm using GATK4 to build a somatic variant calling pipeline. The pipeline uses MultiQC to aggregate quality control data, and one of the QC measures reported is base quality score recalibration from GATK4's <code>BaseRecalibraton</code>. The description in MultiQC says</p>\n\n<blockquote>\n  <p>This plot shows the distribution of base quality scores in each sample before and after base quality score recalibration (BQSR). Applying BQSR should broaden the distribution of base quality scores.</p>\n</blockquote>\n\n<p><a href=""https://i.stack.imgur.com/rxcP5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rxcP5.png"" alt=""enter image description here""></a></p>\n\n<p>The black line is for my tumor sample and blue line is my normal sample. I don't understand how the before and after distributions are <strong>both</strong> shown in the plot as I only see 1 distribution per sample. Am I missing something, or should there be a ""Post-calibration Count"" plot that isn't shown? If nothing is missing, how should I be interpreting the plot?</p>\n","6674","","6674","2020-01-23T19:07:32.927","2020-01-27T21:20:53.797","Interpreting MultiQC plot of GATK BaseRecalibration data?","<fasta><sequencing><variant-calling><gatk><qc>","1","3","",""
"11224","2","","11222","2020-01-23T19:52:00.803","1","","<p>If you want to indicate the error or the uncertainty for each cell type, you need to replicate your experiments, ccount each cell type in each replicate, calculate the average percentage of each cell type in each condition and the associated standard deviation.</p>\n\n<p>To answer your question about how to plot a stacked bargraph with error bar, first this option is not integrated into <code>ggplot2</code> (see this discussion from the creator of ggplot2: <a href=""https://github.com/tidyverse/ggplot2/issues/1079"" rel=""nofollow noreferrer"">https://github.com/tidyverse/ggplot2/issues/1079</a>)</p>\n\n<p>So, you need to calculate the position of each error bar by hand. You can do that by using <code>dplyr</code>. Here, I set the same error bar for everyone as you don't know what is the standard deviation in your data but at least you can observe the structure of the dataframe:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(dplyr)\nlibrary(ggplot2)\nDF %&gt;% mutate(SD = 2) %&gt;%\n  mutate(Cell_Type = factor(Cell_Type, levels = c(""B cell"",""Endothelial"",""Islets of Langerhans"",""Tumor"",""Macrophage"",""T cell"",""Fibroblast""))) %&gt;%\n  group_by(Condition) %&gt;%\n  mutate(SDPos = cumsum(Percent)) %&gt;%\n  ggplot(aes(x = Condition, y = Percent, fill = Cell_Type))+\n  geom_bar(stat = ""identity"")+\n  geom_text(aes(label = paste(round(Percent,2),""%"")), position = position_stack(vjust =  0.5))+\n  geom_errorbar(aes(ymin = SDPos-SD, ymax = SDPos+SD), width = 0.3, position = ""identity"")\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/Bn98R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bn98R.png"" alt=""enter image description here""></a></p>\n\n<p>As you can see, it's pretty ugly... so, if you need to add error bar, I will rather advised you to do a dodged barchart as the following:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(dplyr)\nlibrary(ggplot2)\nDF %&gt;% mutate(SD = 2) %&gt;%\n  ggplot(aes(x = Cell_Type, y = Percent, fill = Condition))+\n  geom_bar(stat = ""identity"", position = position_dodge())+\n  geom_text(aes(label = paste(round(Percent,2),""%"")), position = position_dodge(0.9), vjust =-2, size = 3)+\n  geom_errorbar(aes(ymin = Percent-SD, ymax = Percent+SD), width = 0.3, position = position_dodge(0.9))+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  scale_y_continuous(limits = c(-2,60))\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/M22VR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M22VR.png"" alt=""enter image description here""></a></p>\n\n<p>To my opinion, it makes easier the comparison of the distribution of each cell type between conditions. </p>\n\n<p>Hope that it answers your question.</p>\n","6437","","6437","2020-01-23T20:00:13.053","2020-01-23T20:00:13.053","","","","6","",""
"11225","1","","","2020-01-24T12:10:22.327","0","34","<p>I am working with an in silico model to measure the distances between drugs targets and disease proteins of a disease. I would want to compare these distances with the distances of random networks but I don't know which is the best way to do the comparison. For instance, if I have 300 drug targets and 500 disease proteins mapped in the interactome and I want to measure the distance between them and then compare to random networks with the same features. </p>\n\n<p>How do I generate these random networks? Choosing the same number of drug targets (300) and disease genes (500) randomly in my referral network? Choosing the same number of nodes (300 and 500) and the same number of links?</p>\n\n<p>I'd appreciate your help!</p>\n","6815","","","","2020-02-05T20:35:36.013","Random networks in PPi","<networks>","1","4","",""
"11226","2","","7007","2020-01-24T14:22:49.057","1","","<p>This one uses JQ for parsing and querying conda generated json output (<a href=""https://stedolan.github.io/jq/"" rel=""nofollow noreferrer"">https://stedolan.github.io/jq/</a>)</p>\n\n<pre><code>conda list | awk '{if(NR&gt;3) printf(""%s=%s\n"", <span class=""math-container"">$1, $</span>2)}' | xargs -I{} conda search --info --json ""{}"" | jq --raw-output '.[][0] | ""\(.name)\t\(.version)\t\(.license)""'\n</code></pre>\n\n<p>builds the list, asks conda search for all info on package as json, feeds json to JQ, which selects the first item and outputs its name, version and license as tsv.</p>\n","6816","","","","2020-01-24T14:22:49.057","","","","1","",""
"11227","1","11229","","2020-01-25T00:55:19.673","139","72195","<p>The Wuhan coronavirus's genome was released, and is now available on Genbank.  Looking at it...</p>\n\n<blockquote>\n<pre><code>    1 attaaaggtt tataccttcc caggtaacaa accaaccaac tttcgatctc ttgtagatct\n   61 gttctctaaa cgaactttaa aatctgtgtg gctgtcactc ggctgcatgc ttagtgcact\n  121 cacgcagtat aattaataac taattactgt cgttgacagg acacgagtaa ctcgtctatc\n  ...\n29761 acagtgaaca atgctaggga gagctgccta tatggaagag ccctaatgtg taaaattaat\n29821 tttagtagtg ctatccccat gtgattttaa tagcttctta ggagaatgac aaaaaaaaaa\n29881 aaaaaaaaaa aaaaaaaaaa aaa\n</code></pre>\n  \n  <p><sub><a href=""https://www.ncbi.nlm.nih.gov/nuccore/MN908947"" rel=""noreferrer"">Wuhan seafood market pneumonia virus isolate Wuhan-Hu-1, complete genome</a>, Genbank</sub></p>\n</blockquote>\n\n<p>Geeze, that's a lot of <a href=""https://en.wikipedia.org/wiki/Adenine"" rel=""noreferrer"">a</a> nucleotides---I don't think that's just random.  I would guess that it's either an artifact of the sequencing process, or there is some underlying biological reason.</p>\n\n<p><strong>Question</strong>: Why does the Wuhan coronavirus genome end in 33 a's?</p>\n","1451","","","","2020-02-06T23:38:16.630","Why does the Wuhan coronavirus genome end in aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (33 a's)?","<sequencing>","4","8","","44"
"11228","2","","11227","2020-01-25T02:09:15.500","12","","<p>Not an expert, but some searching on eukaryotic positive-strand RNA viruses seems to show that polyadenylation is not uncommon.  For example, <a href=""https://jvi.asm.org/content/84/6/2843"" rel=""noreferrer""><em>Steil, et al., 2010</em></a>.</p>\n","900","","","","2020-01-25T02:09:15.500","","","","0","",""
"11229","2","","11227","2020-01-25T11:48:59.720","125","","<p>Good observation! The 3' poly(A) tail is actually a very common feature of positive-strand RNA viruses, including coronaviruses and picornaviruses.</p>\n\n<p>For coronaviruses in particular, we know that the poly(A) tail is required for replication, functioning in conjunction with the 3' untranslated region (UTR) as a cis-acting signal for negative strand synthesis and attachment to the ribosome during translation. Mutants lacking the poly(A) tail are severely compromised in replication. Jeannie Spagnolo and Brenda Hogue report:</p>\n\n<blockquote>\n  <p>The 3′ poly (A) tail plays an important, but as yet undefined role in Coronavirus genome replication. To further examine the requirement for the Coronavirus poly(A) tail, we created truncated poly(A) mutant defective interfering (DI) RNAs and observed the effects on replication. Bovine Coronavirus (BCV) and mouse hepatitis Coronavirus A59 (MHV-A59) DI RNAs with tails of 5 or 10 A residues were replicated, albeit at delayed kinetics as compared to DI RNAs with wild type tail lengths (>50 A residues). A BCV DI RNA lacking a poly(A) tail was unable to replicate; however, a MHV DI lacking a tail did replicate following multiple virus passages. Poly(A) tail extension/repair was concurrent with robust replication of the tail mutants. Binding of the host factor poly(A)- binding protein (PABP) appeared to correlate with the ability of DI RNAs to be replicated. Poly(A) tail mutants that were compromised for replication, or that were unable to replicate at all exhibited less in vitro PABP interaction. The data support the importance of the poly(A) tail in Coronavirus replication and further delineate the minimal requirements for viral genome propagation.</p>\n  \n  <p><a href=""https://link.springer.com/chapter/10.1007/978-1-4615-1325-4_68"" rel=""noreferrer"">Spagnolo J.F., Hogue B.G. (2001) Requirement of the Poly(A) Tail in Coronavirus Genome Replication. In: Lavi E., Weiss S.R., Hingley S.T. (eds) The Nidoviruses. Advances in Experimental Medicine and Biology, vol 494. Springer, Boston, MA</a></p>\n</blockquote>\n\n<p>Yu-Hui Peng <em>et al.</em> also report that the length of the poly(A) tail is regulated during infection:</p>\n\n<blockquote>\n  <p>Similar to eukaryotic mRNA, the positive-strand coronavirus genome of ~30 kilobases is 5’-capped and 3’-polyadenylated. It has been demonstrated that the length of the coronaviral poly(A) tail is not static but regulated during infection; however, little is known regarding the factors involved in coronaviral polyadenylation and its regulation. Here, we show that during infection, the level of coronavirus poly(A) tail lengthening depends on the initial length upon infection and that the minimum length to initiate lengthening may lie between 5 and 9 nucleotides. By mutagenesis analysis, it was found that (i) the hexamer AGUAAA and poly(A) tail are two important elements responsible for synthesis of the coronavirus poly(A) tail and may function in concert to accomplish polyadenylation and (ii) the function of the hexamer AGUAAA in coronaviral polyadenylation is position dependent. Based on these findings, we propose a process for how the coronaviral poly(A) tail is synthesized and undergoes variation. Our results provide the first genetic evidence to gain insight into coronaviral polyadenylation.</p>\n  \n  <p><a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0165077"" rel=""noreferrer"">Peng Y-H, Lin C-H, Lin C-N, Lo C-Y, Tsai T-L, Wu H-Y (2016) Characterization of the Role of Hexamer AGUAAA and Poly(A) Tail in Coronavirus Polyadenylation. PLoS ONE 11(10): e0165077</a></p>\n</blockquote>\n\n<p>This builds upon prior work by Hung-Yi Wu <em>et al</em>, which showed that the coronaviral 3' poly(A) tail is approximately 65 nucleotides in length in both genomic and sgmRNAs at peak viral RNA synthesis, and also observed that the precise length varied throughout infection. Most interestingly, they report:</p>\n\n<blockquote>\n  <p>Functional analyses of poly(A) tail length on specific viral RNA species, furthermore, revealed that translation, in vivo, of RNAs with the longer poly(A) tail was enhanced over those with the shorter poly(A). Although the mechanisms by which the tail lengths vary is unknown, experimental results together suggest that the length of the poly(A) and poly(U) tails is regulated. One potential function of regulated poly(A) tail length might be that for the coronavirus genome a longer poly(A) favors translation. The regulation of coronavirus translation by poly(A) tail length resembles that during embryonal development suggesting there may be mechanistic parallels.</p>\n  \n  <p><a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3726627/"" rel=""noreferrer"">Wu HY, Ke TY, Liao WY, Chang NY. Regulation of coronaviral poly(A) tail length during infection. PLoS One. 2013;8(7):e70548. Published 2013 Jul 29. doi:10.1371/journal.pone.0070548</a></p>\n</blockquote>\n\n<p>It's also worth pointing out that poly(A) tails at the 3' end of RNA are not an unusual feature of viruses. Eukaryotic mRNA almost always contains poly(A) tails, which are added post-transcriptionally in a process known as polyadenylation. It should not therefore be surprising that positive-strand RNA viruses would have poly(A) tails as well. In eukaryotic mRNA, the central sequence motif for identifying a polyadenylation region is AAUAAA, identified way back in the 1970s, with more recent research confirming its ubiquity. <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3175714/"" rel=""noreferrer"">Proudfoot 2011</a> is a nice review article on poly(A) signals in eukaryotic mRNA.</p>\n","6823","","","","2020-01-25T11:48:59.720","","","","5","",""
"11230","2","","11227","2020-01-25T19:33:51.023","23","","<p>This question is quite general, so I'm going to attempt to tie it back to bioinformatics.</p>\n\n<p><strong>Background</strong>\nThe tree for the current coronavirus is <a href=""https://www.ecohealthalliance.org/2020/01/phylogenetic-analysis-shows-novel-wuhan-coronavirus-clusters-with-sars"" rel=""noreferrer"">here</a>, showing it is closely related to bat-coronavirus and in particular SARS. </p>\n\n<p><strong>Question</strong>\nThe bioinformatics question for the current coronavirus is why this virus appears to be able to infect humans and transmit to human.</p>\n\n<p><strong>Genome size</strong>\nFirstly, you said that 30kb was large ... this is a standard size for a coronavirus genome, albeit it is unusual in that the family Coronaviridae are the largest genomes for a single stranded RNA virus, for example flaviviruses are 10kb. Thus, all coronaviruses are all approximately 30Kb. Some coronaviruses don't infect humans (zero symptoms), some cause very mild symptoms, others are MERS and SARS with 40-60% and 10% mortality rates, respectively. So, genome size is of little bioinformatics interest in my opinion.</p>\n\n<p><strong>Polyadenylation</strong>\nPolyadenylation and capping (5' methylation) enable the RNA to be trafficked and transcribed by ribosomes and the mechanism is widely used by viruses. Methylation would also prevent the innate immune response from the shredding the vRNA. <a href=""https://www.pnas.org/content/107/8/3283#ref-1"" rel=""noreferrer"">Koonin and Moss (2010)</a>, interpreted a given capping mechanism as being common to the Mononegavirales - a viral Order including measles, mumps, Ebolavirus. Its a big statement, but regardless poly-A and capping are simply mimicking the host mRNA which a lot of viruses use. Poly-A and capping per se are not really interesting.</p>\n\n<p><strong><a href=""https://bioinformatics.stackexchange.com/questions/11250/ancestry-of-the-coronavirus-2019-ncov-wuhan-city-china"">Evolution and SARS</a></strong> A more detailed examination the evolution of 2019-nCov and its epidemiology in relation to SARS can be found <a href=""https://bioinformatics.stackexchange.com/questions/11250/ancestry-of-the-coronavirus-2019-ncov-wuhan-city-china"">here</a></p>\n\n<p><strong>Conclusion</strong>\nThe bioinformatics question is the genome size wierd - no, its standard for a coronavirus, is the poly-A weird - no its generic amongst lots of viruses as is capping. Is the length of the poly-A excessive (33 As), it looks odd  but a human genecist/bioinformaticist needs to answer that ... so is it (potentially) linked with its epidemiology/clinical symptoms?</p>\n\n<p>I don't think 33 poly-As are linked with anything bioinformatically interesting. This is because it will likely vary dramatically between genomes (not simply epidemic vs. non-epidemic strains). I don't know the mechanism for poly-adenylation, but I think slippage is a likely mutation resulting in large variations between individual genomes, particularly for poly-A - which notorious for slippage. </p>\n\n<p>So ultimately could poly-As be linked with the ability of the new coronavirus to infect/transmit and could we therefore explore that bioinformatically? I personally think slippage mutations would prevent a clonal lineage emerging, i.e. that the size of the poly-As is not stable between genomes, but that would assume a given given mechanism of polyadenylation. Thus as a bioinformatics question I wouldn't pursue it, because I don't think there is sufficient biological rationale. I agree weird stuff should be questioned and that bit of the genome jumps out ... but I doubt it would go anywhere.</p>\n\n<p><strong>Slippage</strong>\nThe definition of a slippage mutation is <a href=""https://en.m.wikipedia.org/wiki/Slipped_strand_mispairing"" rel=""noreferrer"">here</a>, but basically it means this genome has 33 poly-As, however another isolate from the same epidemic could say have 30 poly-As (just an example), another might have 25 poly-As and so on.</p>\n\n<p>Just my 2 cents</p>\n","3848","","3848","2020-01-30T14:21:35.930","2020-01-30T14:21:35.930","","","","2","",""
"11231","1","11232","","2020-01-25T19:35:34.567","2","95","<p>I have generated a volcano plot with a differential expression file.</p>\n\n<p>Code for inputing file:</p>\n\n<p><code>macrophage_list &lt;- read.table(""differential_expression_macrophage.csv"", header = T, sep = "","")</code></p>\n\n<pre><code>library(tidyr)\nfinal_df &lt;- df %&gt;% pivot_longer(., -c(Feature.ID, Feature.Name), names_to = c(""set"","".value""), names_pattern = ""(.+)_(.+)"")\n\n# A tibble: 80 x 6\n   Feature.ID Feature.Name set       Mean.Counts Log2.fold.change Adjusted.p.value\n   &lt;fct&gt;      &lt;fct&gt;        &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 a          A            Cluster.1    0.000961            0.292           1     \n 2 a          A            Cluster.2    0.000902            0.793           1     \n 3 a          A            Cluster.3    0.00181             1.46            0.758 \n 4 a          A            Cluster.4    0.000642            0.269           1     \n 5 b          B            Cluster.1    0.000320            1.95            0.910 \n 6 b          B            Cluster.2    0.00180             4.77            0.154 \n 7 b          B            Cluster.3    0                   2.19            1     \n 8 b          B            Cluster.4    0                   1.66            1     \n 9 c          C            Cluster.1    0.00128            -2.01            0.0467\n10 c          C            Cluster.2    0.00632             0.352           1     \n# … with 70 more rows\n</code></pre>\n\n<p>output of head(final_tumor)</p>\n\n<pre><code># A tibble: 6 x 6\n  Feature.ID        Feature.Name set       Mean.Counts Log2.fold.change Adjusted.p.value\n  &lt;fct&gt;             &lt;fct&gt;        &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 ENSG00000227232.5 WASH7P       Cluster.1     0                   1.50            1    \n2 ENSG00000227232.5 WASH7P       Cluster.2     0                   1.73            1    \n3 ENSG00000227232.5 WASH7P       Cluster.3     0                   1.77            1    \n4 ENSG00000227232.5 WASH7P       Cluster.4     0.00114             4.30            0.293\n5 ENSG00000227232.5 WASH7P       Cluster.5     0                   2.15            1    \n6 ENSG00000227232.5 WASH7P       Cluster.6     0                   1.22            1 \n</code></pre>\n\n<p>output of tail(final_tumor)</p>\n\n<pre><code># A tibble: 6 x 6\n  Feature.ID        Feature.Name set        Mean.Counts Log2.fold.change Adjusted.p.value\n  &lt;fct&gt;             &lt;fct&gt;        &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 ENSG00000210196.2 MT-TP        Cluster.6       0.0699          -0.202           0.790  \n2 ENSG00000210196.2 MT-TP        Cluster.7       0.0801           0.0386          1      \n3 ENSG00000210196.2 MT-TP        Cluster.8       0.0711           0.0875          1      \n4 ENSG00000210196.2 MT-TP        Cluster.9       0.0152          -2.31            0.00127\n5 ENSG00000210196.2 MT-TP        Cluster.10      0.0147          -2.30            0.00612\n6 ENSG00000210196.2 MT-TP        Cluster.11      0.122            0.762           1  \n</code></pre>\n\n<p>Code for generating volcano plot:</p>\n\n<pre><code>library(ggplot2)\nlibrary(ggrepel)\nggplot(final_tumor, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+\n  geom_text_repel(data = subset(final_tumor, Adjusted.p.value &lt; 0.05), \n                  aes(label = Feature.Name))\n</code></pre>\n\n<p>Now, I want to pull out a certain gene, Casp14, from the list and box it on the plot. How do I do that?</p>\n\n<p><a href=""https://i.stack.imgur.com/PCi7U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PCi7U.png"" alt=""ggplot of rep_final""></a></p>\n","6827","","6827","2020-01-26T15:05:41.927","2020-01-26T15:05:41.927","Pulling out a certain gene in a volcano plot","<r><ggplot2>","1","0","",""
"11232","2","","11231","2020-01-25T20:12:20.550","4","","<p>As you did for labeling genes with an adjusted p value below 0.05, you can subset your dataset for keeping only rows corresponding to ""Casp14"":</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(ggplot2)\nlibrary(ggrepel)\nggplot(final_tumor, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+\n  geom_text_repel(data = subset(final_tumor, Adjusted.p.value &lt; 0.05), \n                  aes(label = Feature.Name))+\n  geom_text_repel(data = subset(final_tumor, Feature.Name == ""Casp14""), \n                  aes(label = Feature.Name), color = ""red"")\n</code></pre>\n\n<p>With this code, you should see now the labeling of the gene of interest (Casp14) in red on your volcano plot. </p>\n","6437","","","","2020-01-25T20:12:20.550","","","","11","",""
"11235","1","","","2020-01-26T15:37:10.033","0","18","<p>I am reading ""<code>Statistical methods in bioinformatics</code>""</p>\n\n<p>I came across this formula:\n<a href=""https://i.stack.imgur.com/eSebp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eSebp.png"" alt=""enter image description here""></a></p>\n\n<p>As explained in the book <strong>O</strong> are the observations:\n<a href=""https://i.stack.imgur.com/DnIgr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DnIgr.png"" alt=""enter image description here""></a> </p>\n\n<p>My question is why the author only specified <span class=""math-container"">$prob(O)$</span> for a sequence of length=1 ?, I assume he is only talking about one observation, but generally we have a sequence <span class=""math-container"">$O_1 ,O_2,O_3,...$</span>. </p>\n\n<p>I assume that <span class=""math-container"">$Prob(sequence)= P(O_1).P(O_2)...$</span> but I am not sure.</p>\n","6829","","","","2020-01-26T15:37:10.033","probability of an entire sequence, not just one value","<sequencing><hidden-markov-models>","0","0","",""
"11236","1","11239","","2020-01-26T16:54:25.403","0","27","<p>I want to create a dodged barchart with different cell cycles for each cell type within each condition. </p>\n\n<p><strong>dput(DF_count)</strong></p>\n\n<pre><code>structure(list(orig.ident = c(""treated"", ""treated"", ""treated"", \n""treated"", ""treated"", ""treated"", ""treated"", ""treated"", ""treated"", \n""treated"", ""treated"", ""treated"", ""treated"", ""treated"", ""treated"", \n""treated"", ""treated"", ""treated"", ""treated"", ""treated"", ""treated"", \n""treated"", ""treated"", ""treated"", ""treated"", ""treated"", ""treated"", \n""treated"", ""treated"", ""treated"", ""treated"", ""treated"", ""treated"", \n""untreated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n""untreated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n""untreated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n""untreated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n""untreated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n""untreated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n""untreated"", ""untreated"", ""untreated""), Phase = structure(c(1L, \n1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, \n2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, \n1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, \n2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, \n3L), .Label = c(""G1"", ""G2M"", ""S""), class = ""factor""), old.ident = structure(c(1L, \n2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, \n6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, \n10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, \n3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, \n7L, 8L, 9L, 10L, 11L), .Label = c(""Fibroblast"", ""T cell"", ""Macrophage"", \n""Stellate"", ""Acinar"", ""Endothelial"", ""Tumor"", ""B cell"", ""Mast cell"", \n""Ductal"", ""Islets of Langerhans""), class = ""factor""), n = c(3257L, \n666L, 560L, 446L, 91L, 464L, 192L, 506L, 162L, 207L, 90L, 603L, \n608L, 238L, 98L, 22L, 110L, 215L, 95L, 63L, 97L, 10L, 2191L, \n618L, 335L, 349L, 35L, 294L, 250L, 231L, 142L, 153L, 16L, 2910L, \n1259L, 853L, 559L, 1134L, 348L, 306L, 133L, 116L, 12L, 192L, \n706L, 1186L, 374L, 112L, 616L, 106L, 277L, 32L, 47L, 6L, 27L, \n1715L, 1312L, 575L, 390L, 1036L, 250L, 252L, 91L, 66L, 8L, 68L\n), Freq = c(53.8258139150554, 35.2008456659619, 49.4263018534863, \n49.9440089585666, 61.4864864864865, 53.4562211981567, 29.2237442922374, \n60.8173076923077, 44.141689373297, 45.2954048140044, 77.5862068965517, \n9.96529499256321, 32.1353065539112, 21.0061782877317, 10.9742441209406, \n14.8648648648649, 12.6728110599078, 32.7245053272451, 11.4182692307692, \n17.1662125340599, 21.2253829321663, 8.62068965517241, 36.2088910923814, \n32.6638477801269, 29.567519858782, 39.0817469204927, 23.6486486486486, \n33.8709677419355, 38.0517503805175, 27.7644230769231, 38.6920980926431, \n33.4792122538293, 13.7931034482759, 54.5863815419246, 33.5107798775619, \n47.3362930077692, 52.6861451460886, 40.7035175879397, 49.4318181818182, \n36.6467065868263, 51.953125, 50.6550218340611, 46.1538461538462, \n66.8989547038328, 13.2432939410992, 31.5677402182592, 20.7547169811321, \n10.5560791705938, 22.1105527638191, 15.0568181818182, 33.1736526946108, \n12.5, 20.5240174672489, 23.0769230769231, 9.40766550522648, 32.1703245169762, \n34.9214799041789, 31.9089900110988, 36.7577756833176, 37.1859296482412, \n35.5113636363636, 30.1796407185629, 35.546875, 28.82096069869, \n30.7692307692308, 23.6933797909408)), class = c(""grouped_df"", \n""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -66L), groups = structure(list(\n    orig.ident = c(""treated"", ""treated"", ""treated"", ""treated"", \n    ""treated"", ""treated"", ""treated"", ""treated"", ""treated"", ""treated"", \n    ""treated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n    ""untreated"", ""untreated"", ""untreated"", ""untreated"", ""untreated"", \n    ""untreated"", ""untreated""), old.ident = structure(c(1L, 2L, \n    3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, \n    6L, 7L, 8L, 9L, 10L, 11L), .Label = c(""Fibroblast"", ""T cell"", \n    ""Macrophage"", ""Stellate"", ""Acinar"", ""Endothelial"", ""Tumor"", \n    ""B cell"", ""Mast cell"", ""Ductal"", ""Islets of Langerhans""), class = ""factor""), \n    .rows = list(c(1L, 12L, 23L), c(2L, 13L, 24L), c(3L, 14L, \n    25L), c(4L, 15L, 26L), c(5L, 16L, 27L), c(6L, 17L, 28L), \n        c(7L, 18L, 29L), c(8L, 19L, 30L), c(9L, 20L, 31L), c(10L, \n        21L, 32L), c(11L, 22L, 33L), c(34L, 45L, 56L), c(35L, \n        46L, 57L), c(36L, 47L, 58L), c(37L, 48L, 59L), c(38L, \n        49L, 60L), c(39L, 50L, 61L), c(40L, 51L, 62L), c(41L, \n        52L, 63L), c(42L, 53L, 64L), c(43L, 54L, 65L), c(44L, \n        55L, 66L))), row.names = c(NA, -22L), class = c(""tbl_df"", \n""tbl"", ""data.frame""), .drop = TRUE))\n</code></pre>\n\n<p>Used this code to generate a stacked bar graph: </p>\n\n<pre><code>ggplot(DF_Count, aes(x = old.ident, y = Freq, fill = Phase))+\n  geom_col()+\n  geom_text(aes(label = paste(round(Freq, 2),""%"")),position = position_stack(vjust = 0.5))+\n  facet_wrap(~orig.ident)\n</code></pre>\n\n<p>But want the 2 conditions to be next to each other on the same graph. How do I use this dataset and create a dodged bar chart to have both conditions together?</p>\n","6827","","","","2020-01-26T19:39:44.060","Dodged bar chart","<r><ggplot2>","1","0","",""
"11237","1","11240","","2020-01-26T17:37:58.763","0","48","<p>I am trying to plot a box plot/stacked bar graph with error bars.</p>\n\n<p><strong>output of dput(DF)</strong></p>\n\n<pre><code>`structure(list(Cell_Type = structure(c(1L, 2L, 3L, 4L, 5L, 6L, \n7L, 1L, 2L, 3L, 4L, 5L, 6L, 7L), .Label = c(""Fibroblast"", ""T cell"", \n""Macrophage"", ""Tumor"", ""Islets of Langerhans"", ""Endothelial"", \n""B cell""), class = ""factor""), Condition = structure(c(1L, 1L, \n1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""treated"", \n""untreated""), class = ""factor""), Freq = c(6051L, 1892L, 1133L, \n657L, 116L, 868L, 832L, 5331L, 3757L, 1802L, 835L, 287L, 704L, \n256L), Percent = c(52.3941466793662, 16.3823707680319, 9.8103731924842, \n5.68880422547407, 1.00441596675037, 7.51580223395965, 7.20408693393367, \n41.0962072155412, 28.9623805118717, 13.8914585260561, 6.43694110391613, \n2.21245760098674, 5.42707369719396, 1.97348134443417)), class = c(""grouped_df"", \n""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -14L), groups = structure(list(\n    Condition = structure(1:2, .Label = c(""treated"", ""untreated""\n    ), class = ""factor""), .rows = list(1:7, 8:14)), row.names = c(NA, \n-2L), class = c(""tbl_df"", ""tbl"", ""data.frame""), .drop = TRUE))`\n</code></pre>\n\n<p>Gives me the following dataset.</p>\n\n<pre><code>       Var1      Var2 Freq\n1   Fibroblast   treated 6051\n2       T cell   treated 1892\n3   Macrophage   treated 1133\n4     Stellate   treated  893\n5       Acinar   treated  148\n6  Endothelial   treated  868\n7   Fibroblast untreated 5331\n8       T cell untreated 3757\n9   Macrophage untreated 1802\n10    Stellate untreated 1061\n11      Acinar untreated 2786\n12 Endothelial untreated  704\n\nlibrary(dplyr)\nDF &lt;- df %&gt;% rename(Cell_Type = Var1, Condition = Var2) %&gt;%\n  group_by(Condition) %&gt;% \n  mutate(Percent = Freq / sum(Freq)*100)\n\n# A tibble: 12 x 4\n# Groups:   Condition [2]\n   Cell_Type   Condition  Freq Percent\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1 Fibroblast  treated    6051   55.1 \n 2 T cell      treated    1892   17.2 \n 3 Macrophage  treated    1133   10.3 \n 4 Stellate    treated     893    8.13\n 5 Acinar      treated     148    1.35\n 6 Endothelial treated     868    7.90\n 7 Fibroblast  untreated  5331   34.5 \n 8 T cell      untreated  3757   24.3 \n 9 Macrophage  untreated  1802   11.7 \n10 Stellate    untreated  1061    6.87\n11 Acinar      untreated  2786   18.0 \n12 Endothelial untreated   704    4.56\n</code></pre>\n\n<p>Calculated SD/Mean with this:</p>\n\n<pre><code>my_sum_1 &lt;- DF %&gt;%\n    group_by(Cell_Type) %&gt;%\n    summarise( \n        n=n(),\n        mean=mean(Freq),\n        sd=sd(Freq)\n    ) %&gt;%\n    mutate( se=sd/sqrt(n))  %&gt;%\n    mutate( ic=se * qt((1-0.05)/2 + .5, n-1))\n</code></pre>\n\n<p>Then plotted with this:</p>\n\n<pre><code>ggplot(my_sum_1) +\n    geom_bar( aes(x=Cell_Type, y=mean), stat=""identity"", fill=""forestgreen"", alpha=0.5) +\n    geom_errorbar( aes(x=Cell_Type, ymin=mean-ic, ymax=mean+ic), width=0.4, colour=""orange"", alpha=0.9, size=1.5) +\n    ggtitle(""Cell Types"")\n</code></pre>\n\n<p>I want to plot based on condition. Not sure how I would do that. I tried the <code>fill</code> command but doesn't seem to work as the object is not found, I can't seem to add it back to the data set. How do I include the condition in the bar chart?</p>\n\n<p>When I'm calculating sd, I think it's combining the cell type value into one, rather than taking it individually. How do I change that?</p>\n\n<p><strong>dput(head(all@meta.data))</strong></p>\n\n<pre><code>structure(list(orig.ident = c(""PDAC"", ""PDAC"", ""PDAC"", ""PDAC"", \n""PDAC"", ""PDAC""), nCount_RNA = c(7945, 7616, 7849, 5499, 853, \n1039), nFeature_RNA = c(2497L, 2272L, 2303L, 2229L, 509L, 588L\n), percent.mt = c(2.63058527375708, 3.7421218487395, 4.9433048796025, \n0.490998363338789, 0.234466588511137, 0.192492781520693), RNA_snn_res.0.5 = structure(c(5L, \n5L, 5L, 5L, 1L, 6L), .Label = c(""0"", ""1"", ""2"", ""3"", ""4"", ""5"", \n""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", \n""17"", ""18"", ""19""), class = ""factor""), seurat_clusters = structure(c(5L, \n5L, 5L, 5L, 1L, 6L), .Label = c(""0"", ""1"", ""2"", ""3"", ""4"", ""5"", \n""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", \n""17"", ""18"", ""19""), class = ""factor""), S.Score = c(0.0171539298241524, \n-0.00599651468836135, 0.011058574403656, -0.0424413740104151, \n0.03549521163095, -0.0243347616753686), G2M.Score = c(-0.0591387992008088, \n0.0253275642795205, -0.0512402869839816, -0.0239076248512967, \n-0.0263164126515597, -0.0339086517371782), Phase = structure(c(3L, \n2L, 3L, 1L, 3L, 1L), .Label = c(""G1"", ""G2M"", ""S""), class = ""factor""), \n    old.ident = structure(c(1L, 1L, 1L, 1L, 1L, 3L), .Label = c(""Fibroblast"", \n    ""T Cell"", ""Endothelial"", ""Tumor"", ""Stellate"", ""Macrophage"", \n    ""B Cell"", ""Mast Cell"", ""Acinar"", ""Endocrine"", ""Exocrine""), class = ""factor"")), row.names = c(""PDAC_AAACCCAGTCGGCTAC-1"", \n""PDAC_AAACGAAGTCCAGGTC-1"", ""PDAC_AAAGAACAGCAAGTGC-1"", ""PDAC_AAAGAACAGGATGAGA-1"", \n""PDAC_AAAGTGACATCCGTTC-1"", ""PDAC_AACAACCAGGAAGTAG-1""), class = ""data.frame"")\n</code></pre>\n","6827","","6827","2020-01-27T23:25:38.250","2020-01-27T23:25:38.250","Bar plot with error bars","<r><ggplot2>","1","0","",""
"11238","1","","","2020-01-26T18:24:18.040","0","81","<p>I have generated a volcano plot with a differential expression file.</p>\n\n<p>Code for inputing file:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(tidyr)\nfinal_tumor &lt;- df %&gt;% pivot_longer(., -c(Feature.ID, Feature.Name), names_to = c(""set"","".value""), names_pattern = ""(.+)_(.+)"")\n\n# A tibble: 80 x 6\nFeature.ID Feature.Name set       Mean.Counts Log2.fold.change Adjusted.p.value\n&lt;fct&gt;      &lt;fct&gt;        &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n  1 a          A            Cluster.1    0.000961            0.292           1     \n2 a          A            Cluster.2    0.000902            0.793           1     \n3 a          A            Cluster.3    0.00181             1.46            0.758 \n4 a          A            Cluster.4    0.000642            0.269           1     \n5 b          B            Cluster.1    0.000320            1.95            0.910 \n6 b          B            Cluster.2    0.00180             4.77            0.154 \n7 b          B            Cluster.3    0                   2.19            1     \n8 b          B            Cluster.4    0                   1.66            1     \n9 c          C            Cluster.1    0.00128            -2.01            0.0467\n10 c          C            Cluster.2    0.00632             0.352           1     \n# … with 70 more rows\n</code></pre>\n\n<p>Here the output of head(final_tumor)</p>\n\n<pre class=""lang-r prettyprint-override""><code># A tibble: 6 x 6\nFeature.ID        Feature.Name set       Mean.Counts Log2.fold.change Adjusted.p.value\n&lt;fct&gt;             &lt;fct&gt;        &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n  1 ENSG00000227232.5 WASH7P       Cluster.1     0                   1.50            1    \n2 ENSG00000227232.5 WASH7P       Cluster.2     0                   1.73            1    \n3 ENSG00000227232.5 WASH7P       Cluster.3     0                   1.77            1    \n4 ENSG00000227232.5 WASH7P       Cluster.4     0.00114             4.30            0.293\n5 ENSG00000227232.5 WASH7P       Cluster.5     0                   2.15            1    \n6 ENSG00000227232.5 WASH7P       Cluster.6     0                   1.22            1 \n</code></pre>\n\n<p>And here is the output of tail(final_tumor):</p>\n\n<pre class=""lang-r prettyprint-override""><code># A tibble: 6 x 6\nFeature.ID        Feature.Name set        Mean.Counts Log2.fold.change Adjusted.p.value\n&lt;fct&gt;             &lt;fct&gt;        &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n  1 ENSG00000210196.2 MT-TP        Cluster.6       0.0699          -0.202           0.790  \n2 ENSG00000210196.2 MT-TP        Cluster.7       0.0801           0.0386          1      \n3 ENSG00000210196.2 MT-TP        Cluster.8       0.0711           0.0875          1      \n4 ENSG00000210196.2 MT-TP        Cluster.9       0.0152          -2.31            0.00127\n5 ENSG00000210196.2 MT-TP        Cluster.10      0.0147          -2.30            0.00612\n6 ENSG00000210196.2 MT-TP        Cluster.11      0.122            0.762           1\n</code></pre>\n\n<p>Here is the code for generating volcano plot:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(ggplot2)\nlibrary(ggrepel)\nggplot(final_tumor, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+\n  geom_text_repel(data = subset(final_tumor, Adjusted.p.value &lt; 0.05), \n                  aes(label = Feature.Name))+\n  geom_text_repel(data = subset(final_tumor, Feature.Name == ""Casp14""), \n                  aes(label = Feature.Name), color = ""red"")\n</code></pre>\n\n<p>Generating the ggplot with all the data was taking forever, it was suggested to select random rows of the dataframe: </p>\n\n<pre><code>rep_final = final_tumor[sample(1:nrow(final_tumor), size = 1000), ]\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/nt36a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nt36a.png"" alt=""plot""></a></p>\n\n<p>Then I did:</p>\n\n<pre class=""lang-r prettyprint-override""><code>ggplot(rep_final, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+ \n  geom_text_repel(data = subset(rep_final, Adjusted.p.value &lt; 0.05), aes(label = Feature.Name))+ \n  geom_text_repel(data = subset(final_tumor, Feature.Name == ""Casp14""), aes(label = Feature.Name), color = ""red"")\n</code></pre>\n\n<p>Everything is very tight and cluttered. I want to show the top differentially expressed genes. How do I go about simplifying the plot? </p>\n\n<p>Is there a way to do this on EnhancedVolcano, pulling out a certain gene?</p>\n\n<pre><code>EnhancedVolcano(final_tumor, lab = as.character(final_tumor$FeatureName), x = 'Log2.fold.change', y = 'Adjusted.p.value', xlim = c(-8,8), title = 'Tumor', pCutoff = 10e-5, FCcutoff = 1.5, pointSize = 3.0, labSize = 3.0) \n</code></pre>\n","6827","","6437","2020-01-27T11:40:06.850","2020-01-27T13:59:56.470","Making a volcano plot look less cluttered","<r><bioconductor><ggplot2>","2","0","",""
"11239","2","","11236","2020-01-26T19:39:44.060","1","","<p>Here is a rather straight-forward work-around for your problem:</p>\n\n<pre><code>ggplot(DF_Count, aes(x = orig.ident, y = Freq, fill = Phase))+\n  geom_col()+\n  geom_text(aes(label = paste(round(Freq, 2),""%"")),position = position_stack(vjust = 0.5))+\n  facet_wrap(~old.ident)\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/3lpqw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3lpqw.png"" alt=""enter image description here""></a></p>\n\n<p>It does not give a single plot or a dodged bar chart representing the two conditions you would like to visualize. For an exact solution to your problem, see <a href=""https://stackoverflow.com/a/46597859/7236472"">this</a> (actually the answer below the linked answer is analogous to what I have suggested above).</p>\n","3541","","","","2020-01-26T19:39:44.060","","","","0","",""
"11240","2","","11237","2020-01-27T04:11:58.247","0","","<p>Your issue is coming by the fact that when you calculate <code>mean</code>, <code>se</code>, ... you removed the <code>Condition</code> column, here is your <code>my_sum_1</code> results:</p>\n\n<pre class=""lang-r prettyprint-override""><code># A tibble: 7 x 6\n  Cell_Type                n  mean    sd    se     ic\n  &lt;fct&gt;                &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Fibroblast               2 5691   509. 360    4574.\n2 T cell                   2 2824. 1319. 932.  11849.\n3 Macrophage               2 1468.  473. 334.   4250.\n4 Tumor                    2  746   126.  89.0  1131.\n5 Islets of Langerhans     2  202.  121.  85.5  1086.\n6 Endothelial              2  786   116.  82    1042.\n7 B cell                   2  544   407. 288    3659.\n</code></pre>\n\n<p>To calculate the <code>mean</code>, <code>sd</code>, ... from each cell types in each conditions, you need to group by <code>Cell_Type</code> and <code>Condition</code> by writing:</p>\n\n<pre class=""lang-r prettyprint-override""><code>my_sum_1 &lt;- DF %&gt;%\n  group_by(Cell_Type, Condition) %&gt;%\n  summarise( \n    n=n(),\n    mean=mean(Freq),\n    sd=sd(Freq)\n  ) %&gt;%\n  mutate( se=sd/sqrt(n))  %&gt;%\n  mutate( ic=se * qt((1-0.05)/2 + .5, n-1))\n</code></pre>\n\n<p>But now, your table looks like:</p>\n\n<pre class=""lang-r prettyprint-override""><code># A tibble: 14 x 7\n# Groups:   Cell_Type [7]\n   Cell_Type            Condition     n  mean    sd    se    ic\n   &lt;fct&gt;                &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Fibroblast           treated       1  6051    NA    NA    NA\n 2 Fibroblast           untreated     1  5331    NA    NA    NA\n 3 T cell               treated       1  1892    NA    NA    NA\n 4 T cell               untreated     1  3757    NA    NA    NA\n 5 Macrophage           treated       1  1133    NA    NA    NA\n 6 Macrophage           untreated     1  1802    NA    NA    NA\n 7 Tumor                treated       1   657    NA    NA    NA\n 8 Tumor                untreated     1   835    NA    NA    NA\n 9 Islets of Langerhans treated       1   116    NA    NA    NA\n10 Islets of Langerhans untreated     1   287    NA    NA    NA\n11 Endothelial          treated       1   868    NA    NA    NA\n12 Endothelial          untreated     1   704    NA    NA    NA\n13 B cell               treated       1   832    NA    NA    NA\n14 B cell               untreated     1   256    NA    NA    NA\n</code></pre>\n\n<p>Because in your initial dataset, you only have one value for each cell type in each condition, so you can calculate <code>sd</code>, <code>sem</code>. You need to first merge your replicates into your dataframe in order to be able to calculate your <code>sem</code>. </p>\n\n<p>Then, you will be able to do:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(ggplot2)\nggplot(my_sum_1, aes(x=Cell_Type, y=mean, fill = Condition)) +\n  geom_col(alpha=0.5) +\n  geom_errorbar( aes(ymin=mean-ic, ymax=mean+ic), width=0.4, colour=""orange"", alpha=0.9, size=1.5) +\n  ggtitle(""Cell Types"")\n</code></pre>\n\n<p>Hope it helps you to figure it out what is your issue with your current data.</p>\n","6437","","","","2020-01-27T04:11:58.247","","","","6","",""
"11241","2","","11238","2020-01-27T04:33:43.677","0","","<p>As we discussed in comments in your previous post: <a href=""https://bioinformatics.stackexchange.com/questions/11231/pulling-out-a-certain-gene-in-a-volcano-plot/11232#11232"">Pulling out a certain gene in a volcano plot</a>, in order to have less labeled genes on your volcano plot, you can specify more stringent cutoff. </p>\n\n<p>For example, you can decide to display only genes with an absolute fold change superior to 2 by selecting the appropriate subset of your data:</p>\n\n<pre class=""lang-r prettyprint-override""><code>ggplot(rep_final, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+ \n  geom_text_repel(data = subset(rep_final, abs(Log2.fold.change) &gt; 2), aes(label = Feature.Name))+ \n  geom_text_repel(data = subset(final_tumor, Feature.Name == ""Casp14""), aes(label = Feature.Name), \n                  color = ""red"", size = 5)\n</code></pre>\n\n<p>Alternatively, you can maybe decide that a p value inferior to 0.05 is not enough stringent and decide to labeled only genes with an adjusted p value inferior to 0.00001 by doing:</p>\n\n<pre class=""lang-r prettyprint-override""><code>ggplot(rep_final, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+ \n  geom_text_repel(data = subset(rep_final, Adjusted.p.value &lt; 0.00001), aes(label = Feature.Name))+ \n  geom_text_repel(data = subset(final_tumor, Feature.Name == ""Casp14""), aes(label = Feature.Name), \n                  color = ""red"", size = 5)\n</code></pre>\n\n<p>Or, you can want to do a combination of both criteria by doing:</p>\n\n<pre class=""lang-r prettyprint-override""><code>ggplot(rep_final, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+ \n  geom_text_repel(data = subset(rep_final, abs(Log2.fold.change) &gt; 2 &amp; Adjusted.p.value &lt; 0.00001), aes(label = Feature.Name))+ \n  geom_text_repel(data = subset(final_tumor, Feature.Name == ""Casp14""), aes(label = Feature.Name), \n                  color = ""red"", size = 5)\n</code></pre>\n\n<p>At the end, I think it depends of what you are trying to demonstrate by presenting this volcano plot. Do you want to highlight some particular genes ? or do you want to display genes that are the most differentially expressed ? If so, how many ? </p>\n\n<p>All of this depends of the scientific question underlying your experiments and why you came to make this volcano plot.</p>\n\n<p>Hope this couple of example helps you to figure it out how to customize your volcano plot.</p>\n","6437","","","","2020-01-27T04:33:43.677","","","","0","",""
"11242","2","","11227","2020-01-27T09:37:05.457","14","","<p>Some of the other answers here seem quite good; at the same time I think the core answer to the OP's question is maybe a bit hard to tease out of them, so I'd like to try to state it more plainly. It's worth noting that a truly complete answer to this question seems to be beyond current research, but any kind of ""Why?"" is inevitably a hard or even impossible sort of question to answer fully in biology. We have some ideas about it though.</p>\n\n<p>mRNA is used as a template for protein synthesis within a cell. A single mRNA is used repeatedly, but is eventually ""used up"" and taken apart. In eukaryotes, poly(A) tails are almost always found on mRNAs produced in the nucleus. The poly(A) tail is ultimately shortened during the transcription process, and this shortening contributes to the mRNA being degraded. (See <a href=""https://www.ncbi.nlm.nih.gov/books/NBK6413/"" rel=""noreferrer"">here</a> for more.)</p>\n\n<p>Coronaviruses also have a poly(A) tail, similarly to eukaryote mRNA. The precise mechanical functions of this poly(A) tail and the means of its synthesis are objects of ongoing research, but research has shown that <a href=""https://link.springer.com/chapter/10.1007/978-1-4615-1325-4_68"" rel=""noreferrer"">its presence greatly increases the degree to which Coronavirus RNA is replicated by the host cell</a>. Research has also shown that <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3726627/"" rel=""noreferrer"">longer tails increase replication compared to shorter tails</a>. It's quite likely that the presence of the tail assists in recruiting the cell's protein synthesis machinery and allows the RNA to last longer within the host cell, just as it does in the cell's own mRNA.</p>\n\n<p>Interestingly, the pattern in which Coronavirus poly(A) tail length is regulated during infection, in which it starts out shorter, gets longer, then gets much shorter, resembles <a href=""https://elifesciences.org/articles/16955"" rel=""noreferrer"">poly(A) tail length regulation of mRNA during eukaryote embryogenesis</a>, suggesting parallels (see the paper in the ""longer tails"" link for more on this as well). Longer poly(A) tail length is closely tied to greater translational efficiency in that context.</p>\n\n<p>There has been some speculation in the comments as to whether or not the Coronavirus poly(A) tail resembles a NOP sled in computer programming. I think the resemblance is mostly coincidental. NOP sleds are used in exploits because a processor, encountering a NOP, moves to the next instruction without taking any other actions. A long chain of NOPs, if entered by the processor at any point within it, will lead it to the instructions at the ""bottom,"" after the NOPs. This is advantageous to use if you can't get the processor to go exactly where you want but you know it will end up somewhere close by, because it increases your chances of having your payload executed.</p>\n\n<p>It's unusual to see a lengthy NOP sled in legitimate code, to the point that people writing them usually have to disguise their function in order to avoid automatic detection. (see pg. 183 <a href=""https://link.springer.com/content/pdf/10.1007/s10994-009-5143-5.pdf"" rel=""noreferrer"">here</a>) In contrast, a poly(A) tail is <a href=""https://www.cell.com/fulltext/S0092-8674(02)01137-6"" rel=""noreferrer"">almost universally found on nuclear eukaryote mRNA (and on some mRNAs of almost all organisms to some capacity, even mitochondria)</a>. Furthermore, the functions of the poly(A) tail are complex enough that it's still an object of ongoing research decades after its initial discovery, whereas a NOP sled does one very mechanical thing. Since the environment inside a cell is so different from the environment of a processor interacting with memory, I think it's hard to make comparisons that are so granular as to deal with a specific set of machine instructions, at least in this kind of context—a processor is a very straightforward kind of machine compared to a cell.</p>\n","6928","","","","2020-01-27T09:37:05.457","","","","6","",""
"11243","1","11247","","2020-01-27T12:04:57.213","1","59","<p>I have 3 data frames for three groups of patients and in each of them I have the number of mutation types like insertion, deletion, SNP and total of mutations for each patients. In each group I have different number of patients like below</p>\n\n<pre><code>&gt; head(dat1)\n                patient  DEL  INS   SNP   total\n1:    LP6008337-DNA_H06  927  773 40756   42456\n2:    LP6008334-DNA_D02 1049  799 31009   32857\n&gt; dim(dat1)\n[1] 21  6\n&gt; \n\n head(dat2)\n                Patient   DEL  INS   SNP total\n1:    LP6008031-DNA_E01 13552 3374 62105   79031\n2:    LP6005500-DNA_G01   539  500 43451   44490\n&gt; dim(dat2)\n[1] 33  6\n&gt; \n\n&gt; head(dat3)\n                Patient   DEL   INS    SNP    total\n1:    LP6005935-DNA_F03 39168 16739  58095   114002\n2:    LP6008269-DNA_D08   849   910 103501  105271\n\n&gt; dim(dat3)\n[1] 106   6\n</code></pre>\n\n<p>I want to show if the number of each mutational category and total is different between these three groups by chi-square test</p>\n\n<p>Actual question is if total number of mutations, SNP, DEL and INS are statistically significant among groups. I used pairwise t test but I afraid this test is not a suitable test for the distribution of data moreover I don't know how to visualize the p-value</p>\n\n<p>Imagining the comparison of total number of mutations between three groups, this picture is a good example\n<a href=""https://i.stack.imgur.com/ABR04.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ABR04.jpg"" alt=""enter image description here""></a></p>\n\n<p>Or in this plot they compare several features but only two groups </p>\n\n<p><a href=""https://i.stack.imgur.com/Sy4cn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sy4cn.jpg"" alt=""enter image description here""></a></p>\n\n<p>Can you help ?</p>\n\n<p>Thanks </p>\n","4595","","4595","2020-01-27T14:46:17.413","2020-01-27T15:36:10.237","Visualizing the difference among three groups","<r><ggplot2>","1","2","",""
"11244","2","","11238","2020-01-27T13:59:56.470","1","","<p><strong>The element that is clustering your plot the most is the ggrepel labels</strong>, so that's what you need to work on. </p>\n\n<p>To get to a readable number of labels, <strong>I would not remove random genes</strong>: you risk losing important info, be it genes that could be of interest to you or some of the most regulated. Incidentally that seems to happen on your example: you try plotting in red the label for Casp14, but it does not show on the plot, probably because the gene was removed in your sampling.</p>\n\n<p>What you can do however is, as dc37 suggests, <strong>increase the thresholds for the labels</strong> (you can probably play around with the thresholds to see when it does become readable). This should also help with the plotting: you mention that generating the plot was taking a long time, but with the number of genes you are looking at (~10k-30k?), the geom_points() should not be a real problem (maybe a few seconds), however the ggrepel is the most computationnally expensive so that's what you should try to reduce.</p>\n\n<p>This should get you started:</p>\n\n<pre><code>ggplot(final_tumor, aes(x = Log2.fold.change,y = -log10(Adjusted.p.value), label = Feature.Name))+ \n  geom_point()+ \n  geom_text_repel(data = subset(final_tumor, Adjusted.p.value &lt; 0.001 &amp; abs(Log2.fold.change) &gt; 1 ), aes(label = Feature.Name))+ \n  geom_text_repel(data = subset(final_tumor, Feature.Name == ""Casp14""), aes(label = Feature.Name), color = ""red"")\n</code></pre>\n\n<p>Notice everything plots the ""final_tumor"" object and not the ""rep_final"". Feel free to be even more stringent with the cutoffs if it's still taking too long to plot.</p>\n\n<p>A bit outside of the score of your question, but I also wanted to mention that you should never be trying to label too many genes on a volcano plot: this type of graph is useful to visualize your data's p-value/FC distribution and highlight a subset of genes in this context (most regulated, genes regulated in another experiment, pathway of interest...), but if you label too many of them it will never be readable.</p>\n\n<p>If you really want to be able to indentify all the genes on your graph, I would try an interactive visualization (like with <a href=""https://plot.ly/r/"" rel=""nofollow noreferrer"">plotly</a>). I'm not going to detail the code here but you could look in that direction! (However, as with ggrepel, you might need to remove some genes so that it does not ""take forever"").</p>\n\n<p>Have fun :)</p>\n","1452","","","","2020-01-27T13:59:56.470","","","","4","",""
"11245","1","","","2020-01-27T14:54:59.273","1","32","<p>I have a RNA aligned file without any header info. \nI need to calculate the coverage of each mapped base for per chromosome. Is it possible to do it with perl ? </p>\n\n<pre><code>NS500377:28:H16A7BGXX:2:21205:16038:7658        99      chr2   3000482 255     46M     =       3000502 56      GTCCTACAGTGTGCATTTCTCGTTTTCCACGTTTCT    AAAA)FFFFFFFFFAFFFFFFFFFFF.F.FFFFFAF    XA:i:2  MD:Z:21T4T7T1   NM:i:3\nNS500377:28:H16A7BGXX:3:11503:24953:10427       99      chr2   3000482 255     50M     =       3000513 71      GTCCTACAGTGTGCATTTCTCATTTTTCAAGTTTTTCAGT        AAAAAFFFFFFFFFFFFAFFFFFF7AAFFFFFFFFFFFFF\n        XA:i:1  MD:Z:21T7C10    NM:i:2\nNS500377:28:H16A7BGXX:3:12404:23929:9392        163     chr2   3000482 255     49M     =       3000483 40      GTCCTACAGTGTGCATTTCCCTTTTTTCACGTTTTTTAG AAFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF XA:i:1  MD:Z:19T16C2    NM:i:2\nNS500377:28:H16A7BGXX:3:12404:23929:9392        83      chr2   3000483 255     49M     =       3000482 -40     TCCTACAGTGTGCATTTCCCTTTTTTCACGTTTTTTAGT AFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF XA:i:2  MD:Z:18T16C3    NM:i:2\nNS500377:28:H16A7BGXX:3:21606:15675:1639        83      chr2   3000483 255     50M     =       3000480 -43     TCTTACAGTGTGCATTTCTCTTTTTTCACGTTTTTCAGTG        AFFFFFFFFFAFFFFFFFFFFFFFFFFFFFFFFFFAAAAA\n        XA:i:0  MD:Z:2C37       NM:i:1\nNS500377:28:H16A7BGXX:4:12605:18505:11088       83      chr2   3000486 255     50M     =       3000481 -45     TACAATGTGCATTTCTCTTTTTTCACGTTTTTCAGTATTT        FFFFFFFAFFF)FFFFFFFFF&lt;FFFF&lt;AFFFFFFFAA&lt;AA\n        XA:i:1  MD:Z:4G31G3     NM:i:2\nNS500377:28:H16A7BGXX:2:13303:20634:12085       99      chr2   3000487 255     49M     =       3000521 73      ACAGTGTACATTTCTCATTTTTCACGTTTTTCAGTGTTA AAAAAFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF. XA:i:2  MD:Z:7G8T21T0   NM:i:3\n</code></pre>\n","6958","","6958","2020-01-27T15:10:25.740","2020-01-27T17:39:13.583","how to calculate coverage of each base of mapped reads?","<rna><coverage><awk>","1","0","","1"
"11247","2","","11243","2020-01-27T15:36:10.237","3","","<p>Here a way to do it is to start first by creating a dataset containing your three different group, based on the few lines you display:</p>\n\n<pre class=""lang-r prettyprint-override""><code>dat1<span class=""math-container"">$Condition = ""dat1""\ndat2$</span>Condition = ""dat2""\ndat3$Condition = ""dat3""\ncolnames(dat1)[1] = ""Patient""\nDF &lt;- rbind(dat1,dat2,dat3)\n\n             Patient   DEL   INS    SNP  total Condition\n1: LP6008337-DNA_H06   927   773  40756  42456      dat1\n2: LP6008334-DNA_D02  1049   799  31009  32857      dat1\n3: LP6008031-DNA_E01 13552  3374  62105  79031      dat2\n4: LP6005500-DNA_G01   539   500  43451  44490      dat2\n5: LP6005935-DNA_F03 39168 16739  58095 114002      dat3\n6: LP6008269-DNA_D08   849   910 103501 105271      dat3\n</code></pre>\n\n<p>Then, you can reshape <code>DF</code> in order to put all values DEL, INS, SNP and Total into a single column and create a categorical column. Doing this will be helpful for later producing the facets of your plot. To do that, you can use <code>pivot_longer</code> from <code>tidyr</code> package:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(tidyr)\nDF &lt;- DF %&gt;% pivot_longer(.,cols = c(DEL,INS,SNP, total), names_to = ""var"", values_to = ""val"")\n\n# A tibble: 24 x 4\n   Patient           Condition var     val\n   &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n 1 LP6008337-DNA_H06 dat1      DEL     927\n 2 LP6008337-DNA_H06 dat1      INS     773\n 3 LP6008337-DNA_H06 dat1      SNP   40756\n 4 LP6008337-DNA_H06 dat1      total 42456\n 5 LP6008334-DNA_D02 dat1      DEL    1049\n 6 LP6008334-DNA_D02 dat1      INS     799\n 7 LP6008334-DNA_D02 dat1      SNP   31009\n 8 LP6008334-DNA_D02 dat1      total 32857\n 9 LP6008031-DNA_E01 dat2      DEL   13552\n10 LP6008031-DNA_E01 dat2      INS    3374\n# … with 14 more rows\n</code></pre>\n\n<p>Now, you can plot your data and use the function <code>stat_compare_means</code> from <code>ggpubr</code> package to get the significance display on each tab. Based on the size of your group, you may want to adjust for the type of statistical test to be used (take a look at <code>?stat_compare_means</code> to find the statistical test adequate to your dataset). You can pass various conditions to test as a list by using <code>comparisons</code> argument. </p>\n\n<p>For facetting your graph, you can use <code>facet_grid</code> (or<code>facet_wrap</code>, try the one you preferred). Altogether, you can get the plot by writing:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(ggplot2)\nlibrary(ggpubr)\nggplot(DF, aes(x = Condition, y = val, fill = Condition))+\n  geom_boxplot()+\n  stat_compare_means(comparisons = list(c(""dat1"",""dat2""), c(""dat1"",""dat3""), c(""dat2"",""dat3"")))+\n  stat_compare_means()+\n  facet_grid(var~., scales = ""free"")\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/LAuWd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LAuWd.png"" alt=""enter image description here""></a></p>\n\n<p>Hope it helps you to figure it out how to represent your data.</p>\n","6437","","","","2020-01-27T15:36:10.237","","","","4","",""
"11249","2","","11245","2020-01-27T17:39:13.583","1","","<p>You are looking for <code>bedtools genomecov</code>, specifically <a href=""https://bedtools.readthedocs.io/en/latest/content/tools/genomecov.html#d-reporting-per-base-genome-coverage"" rel=""nofollow noreferrer""><code>-d</code></a> option. See <a href=""https://bedtools.readthedocs.io/en/latest/content/tools/genomecov.html"" rel=""nofollow noreferrer"">https://bedtools.readthedocs.io/en/latest/content/tools/genomecov.html</a> , for example:</p>\n\n<pre><code>bedtools genomecov -i A.bed -g my.genome -d\n</code></pre>\n\n<hr>\n\n<p>I recommend not reinventing the wheel: use <code>bedtools</code> instead of reimplementing the same functionality in Perl.</p>\n","6251","","","","2020-01-27T17:39:13.583","","","","3","",""
"11250","1","11268","","2020-01-27T18:38:00.190","5","581","<p>In one of the answers to <a href=""https://bioinformatics.stackexchange.com/questions/11227/why-does-the-wuhan-coronavirus-genome-end-in-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"">another question about the corona virus</a> a link was given to this <a href=""https://www.ecohealthalliance.org/2020/01/phylogenetic-analysis-shows-novel-wuhan-coronavirus-clusters-with-sars"" rel=""nofollow noreferrer"">phylogenetic analysis</a> of the virus.</p>\n\n<p>Can somebody assist a non-bio type here? It seems to show that the current corona virus split from a virus in bats. And that the same ancestral virus is also ancestral to several other viruses, some in humans and some in bats. </p>\n\n<p>Is my understanding basically correct?</p>\n","6971","","3848","2020-01-28T11:41:39.423","2020-02-28T14:40:04.963","Ancestry of the coronavirus 2019-nCov, WuHan city, China","<phylogenetics><phylogeny>","2","1","",""
"11251","1","","","2020-01-27T21:17:59.723","4","202","<p>This is the BLAST tree of the latest coronavirus out of China. It seems strange that there is so much divergence from all the other coronaviruses. Is this expected of new diseases?</p>\n\n<p><a href=""https://i.stack.imgur.com/zIawo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zIawo.png"" alt=""Wuhan BLAST cornoavirus tree""></a></p>\n","6985","","3848","2020-01-30T15:32:24.247","2020-01-30T15:32:24.247","Interpretion of my coronavirus 2019-nCov, Wuhan, China BLAST tree?","<phylogenetics><blast><phylogeny>","1","0","",""
"11252","2","","11223","2020-01-27T21:20:53.797","0","","<p>Turns out two recalibration tables are needed to show pre-recalibration and post-recalibration plots. I previously only had generated the pre-calibration table by running <code>BaseRecalibrator</code> on the BAM that hadn't undergone <code>ApplyBQSR</code> to adjust base scores. Here are the steps to show pre and post recalibration plots in <code>MultiQC</code>:</p>\n\n<ol>\n<li>Run <code>BaseRecalibrator</code> on your initial input BAM. This will create a recalibration table with the original base scores. </li>\n<li>Run <code>ApplyBQSR</code> with the recalibration table from step 1 to create a new BAM with the adjusted base quality scores.</li>\n<li>Run <code>BaseRecalibrator</code> on the adjusted BAM generated in step 2 to create a new recalibration table with the adjusted base quality scores. </li>\n<li>Use both recalibration tables as inputs to <code>MultiQC</code>. This should create both plots. </li>\n</ol>\n","6674","","","","2020-01-27T21:20:53.797","","","","0","",""
"11253","1","11267","","2020-01-27T23:35:10.380","0","45","<p>I want to create a column plot for my single cell analysis that has 2 sample types, a normal and PDAC. I want to show, the percentage of different cell types per sample type.</p>\n\n<p>How do I go about determining the percent of each cell type in each condition and then go about plotting the column plot? Also, is it possible to determine the SD from the dataset and add it?</p>\n\n<p><strong>dput(head(all@meta.data))</strong></p>\n\n<pre><code>structure(list(orig.ident = c(""PDAC"", ""PDAC"", ""PDAC"", ""PDAC"", \n""PDAC"", ""PDAC""), nCount_RNA = c(7945, 7616, 7849, 5499, 853, \n1039), nFeature_RNA = c(2497L, 2272L, 2303L, 2229L, 509L, 588L\n), percent.mt = c(2.63058527375708, 3.7421218487395, 4.9433048796025, \n0.490998363338789, 0.234466588511137, 0.192492781520693), RNA_snn_res.0.5 = structure(c(5L, \n5L, 5L, 5L, 1L, 6L), .Label = c(""0"", ""1"", ""2"", ""3"", ""4"", ""5"", \n""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", \n""17"", ""18"", ""19""), class = ""factor""), seurat_clusters = structure(c(5L, \n5L, 5L, 5L, 1L, 6L), .Label = c(""0"", ""1"", ""2"", ""3"", ""4"", ""5"", \n""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", \n""17"", ""18"", ""19""), class = ""factor""), S.Score = c(0.0171539298241524, \n-0.00599651468836135, 0.011058574403656, -0.0424413740104151, \n0.03549521163095, -0.0243347616753686), G2M.Score = c(-0.0591387992008088, \n0.0253275642795205, -0.0512402869839816, -0.0239076248512967, \n-0.0263164126515597, -0.0339086517371782), Phase = structure(c(3L, \n2L, 3L, 1L, 3L, 1L), .Label = c(""G1"", ""G2M"", ""S""), class = ""factor""), \n    old.ident = structure(c(1L, 1L, 1L, 1L, 1L, 3L), .Label = c(""Fibroblast"", \n    ""T Cell"", ""Endothelial"", ""Tumor"", ""Stellate"", ""Macrophage"", \n    ""B Cell"", ""Mast Cell"", ""Acinar"", ""Endocrine"", ""Exocrine""), class = ""factor"")), row.names = c(""PDAC_AAACCCAGTCGGCTAC-1"", \n""PDAC_AAACGAAGTCCAGGTC-1"", ""PDAC_AAAGAACAGCAAGTGC-1"", ""PDAC_AAAGAACAGGATGAGA-1"", \n""PDAC_AAAGTGACATCCGTTC-1"", ""PDAC_AACAACCAGGAAGTAG-1""), class = ""data.frame"")\n</code></pre>\n","6827","","","","2020-02-03T11:11:19.227","Clustered column chart for single cell data","<r><ggplot2>","2","3","",""
"11254","1","","","2020-01-28T03:02:39.093","0","57","<p>I would like to construct a mathematical graph: <span class=""math-container"">$G = (E,V, w(E))$</span> a graph with vertices, edges with weight on them <a href=""https://www.biostars.org/p/85763/"" rel=""nofollow noreferrer"">based on a KEGG pathway</a>. </p>\n\n<p>Is there a way to do that with existing tools? If not I will start to implement a python module for that. My main question is what is the the mathematical graph representation of pathways? Assuming the possible node types and the different connections?</p>\n\n<p>I would like to transform a pathway to a graph because then one can run min-cut-max-flow algorithms and stuff like that as part of my <a href=""https://en.wikipedia.org/wiki/Pathway_analysis"" rel=""nofollow noreferrer"">pathway analysis</a>.</p>\n","734","","734","2020-02-01T15:32:44.817","2020-02-01T15:32:44.817","How to represent pathways as mathematical graphs?","<python><pathway><kegg>","1","0","",""
"11255","1","","","2020-01-28T06:38:06.180","0","45","<p>Although the Kallisto software was designed for RNA, it has also been very reliable in counting selected FASTA sequences in DNA sequencing. For example, given a  WGS sample and my FASTA file. Kallisto is able to accurately return the counts for each sequence in my FASTA file.</p>\n\n<p>Now, I'd like to do something similar on Nanopore long reads. Is that a similar software?</p>\n","174","","","","2020-01-28T07:14:56.147","Kallisto for Nanopore?","<nanopore>","1","0","",""
"11256","1","","","2020-01-28T07:01:13.693","1","62","<p>How can we retrieve the file offsets of the zipped-chunks in a block-gzipped file that contain records for a region/window of interest (e.g. Chr01:100-Chr02:145)? </p>\n\n<p>It seems like the CLI of <code>samtools</code>, or <code>tabix</code> should support that, but I can't find a way to just get the offsets in the datafile, without having the accompanying datafile at hand. I couldn't find a formal specification for those index files either.</p>\n\n<p>Perhaps one can recommend a library that can parse those index files, in python, C, or golang? It's hard to accept that there isn't a reusable block-gzipped index parser somewhere, other than the one buried deep in samtools/htslib/gatk code.</p>\n\n<p><strong>edit</strong></p>\n\n<p>I'd initially given a small motivating example making use of file offsets to access a subportion of file on a remote server. Commenters were quick to point out that samtools supported a series of protocols for doing exactly that: ftp, http, and s3 (which requires https, which requires that samtools be compiled with libcurl support) Thanks! That example was detracting, so I removed it.</p>\n\n<p><code>samtools view</code> does fetch a subportion of a file (potentially from a remote location), but it does a few extra things, like decompressing the chunks retrieved. With this question, I'm interested in manipulating the gz chunks directly.</p>\n","3235","","3235","2020-01-28T11:29:16.240","2020-02-29T03:47:50.877","How to obtain file offsets corresponding to range query in block-gzipped file","<bam><samtools><indexing><tabix><bgzip>","1","5","",""
"11257","2","","11255","2020-01-28T07:14:56.147","1","","<p>It seems <a href=""https://github.com/a-slide/NanoCount"" rel=""nofollow noreferrer"">NanoCount</a> can do what you are looking for.:</p>\n\n<blockquote>\n  <p>EM based transcript abundance from nanopore reads mapped to a\n  transcriptome with minimap2 Python package adapted from\n  <a href=""https://github.com/jts/nanopore-rna-analysis"" rel=""nofollow noreferrer"">https://github.com/jts/nanopore-rna-analysis</a> by Jared Simpson</p>\n  \n  <p>NanoCount estimates transcript abundance from ONT direct-RNA\n  Sequencing reads mapped to a transcriptome. It uses an\n  expectation-maximization approach like RSEM, Kallisto, salmon, etc to\n  handle multi-mapping reads. The reads must be mapped to the transcript\n  set using minimap2.</p>\n</blockquote>\n","681","","","","2020-01-28T07:14:56.147","","","","3","",""
"11258","2","","11251","2020-01-28T10:57:49.717","4","","<p>The  evolutionary related group (clade) of betacoronaviruses you have identified share an amino acid homology of 85% and include SARS. I know this from the underlying tree published on BioRxiv of a broader group of betacoronaviruses, i.e. your data is a defined subset of the betacoronaviruses which all share a unique, single common ancester.</p>\n\n<p>Lets call this group the SARS clade.</p>\n\n<p>You have performed a nucleotide blast and asked NCBI to produce a NJ tree using 2019-nCov as the reference. I can tell this by the colour coding and the scale bar at the bottom right hand corner shows the genetic distance involved is reasonably larger than 15% divergence by amino acid data. The scale bar represents the number of mutations per nucleotide. </p>\n\n<p>In summary, your tree is essentially a subset of the broader genetic diversity of the one published, but there is there is a rooting issue. </p>\n\n<p>In your tree, the majority of sequences are from the 2002 SARS epidemic and the virtually zero genetic distance between them is simply because it is a rapidly transmitted outbreak. <em>I didn't realize that SARS has two independent origins both initially from bats. This is quite scary.</em> </p>\n\n<ul>\n<li>2019-nCov is an outgroup within the 'SARS clade' hence it appears at the other side of the tree, i.e. they share a more distant common ancestor</li>\n<li>However 2019-nCov is not the most distant common ancester within this subset of the betacoronaviruses, i.e. SARS clade, this belongs to the two bat viruses ZC45 and ZX21. The program has made a likely rooting error (below).</li>\n<li>Again the blast omitted the majority of the betacoronaviruses, for example MERS</li>\n</ul>\n\n<p><strong>Rooting issue</strong> the reason I suggest there is a rooting error is because the BioRxiv tree using a broad sample of the betacoronaviruses placed the bat strains ZC45 and ZX21 as outgroups to the SARS clade and 2019-nCOV was immediately within that, so in this definition 2019-nCOV is an 'ingroup' within the SARS clade, whereas your tree it is an 'outgroup'. It not a huge issue, but the location of the ""root"" is determined by the common ancester above it (sequences with &lt;85% homology) and in your tree those have been omitted. </p>\n\n<p>Generally, I like the analysis otherwise and gives an insight into SARS that I hadn't previosly been aware of.</p>\n","3848","","3848","2020-01-28T11:12:01.483","2020-01-28T11:12:01.483","","","","0","",""
"11259","2","","11256","2020-01-28T11:03:40.973","1","","<p>So if the purpose of obtaining file offsets is to access subportions of remote files via protocols that samtools supports, as mentioned in <a href=""https://bioinformatics.stackexchange.com/questions/11256/how-to-obtain-file-offsets-corresponding-to-range-query-in-block-gzipped-file#comment15279_11256"">this comment</a>, then you can make your life easier and use samtools/tabix directly on the cli, prefixing your datafile with the corresponding scheme. </p>\n\n<p>One side effect of using the CLI however, is that the blocks retrieved will be decompressed (e.g. samtools view). You'd have to re-compress them if what you're trying to do is create a smaller ""bam"" file.</p>\n\n<p>Keep in mind that if you retrieve all zipped chunks which cover your range of interest, then you might be extracting more data than you asked for. The first and last chunk may contain records outside your region, just because their neighboring records happened to be compressed in the same chunk. You'd have to be careful to filter those downstream.</p>\n\n<p><strong>Specification for the tabix format</strong></p>\n\n<p>If you absolutely need a custom solution, the tabix data structure format, authored by Heng Li, is on the samtools website: </p>\n\n<p><a href=""https://samtools.github.io/hts-specs/tabix.pdf"" rel=""nofollow noreferrer"">https://samtools.github.io/hts-specs/tabix.pdf</a></p>\n\n<p>A more in depth discussion of how the data structure works can be found here:</p>\n\n<p><a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3042176/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3042176/</a></p>\n\n<p>The tree-shaped binning index is a bit hard to understand from the text alone, but the tabix paper mentions that their approach is almost identical to the binning algorithm in this paper: <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC186604/pdf/X6.pdf"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC186604/pdf/X6.pdf</a> (page 1003). That paper has a nice illustration which makes things clearer.</p>\n\n<p><strong>Operation</strong></p>\n\n<p>Note: This is from my understanding of how the indices interoperate.</p>\n\n<p>You don't need the data file to retrieve chunk offsets, you can do the work with just the tabix file. But it's complicated, and probably error prone if you're implementing this yourself.</p>\n\n<p>Let's assume the query interval is <code>Chr01:100-Chr02:2000</code>.</p>\n\n<p>Each sequence (Chr01, Chr02, etc) has its own pair of indices (binning index + linear index) in the tabix file. So you would break the query into two queries: <code>Chr01:100-999999999999</code> and <code>Chr02:0-2000</code>, and concatenate the two results.</p>\n\n<p>Within one sequence, to find all the blocks for open interval <code>[X,Y)</code>, you would use the binning index to retrieve all the bins overlapping this interval (There is C code to do this in the data structure pdf). Then, you need to go through all the possible bins selected, and filter chunks. Not all chunks in a given bin will be overlapping with <code>X, Y</code>.</p>\n\n<p>To filter chunks within a bin, you need the help of the linear index. There is one entry in the linear index per 16kbp of sequence positions. Each entry of the linear index contains the virtual offset of the leftmost record overlapping that 16kb window. I don't know what the entry contains if there is no record overlapping that 16kb window, presumably 0.</p>\n\n<p>You would find the linear entry in which position X falls (X/16Kb), take that offset, call it O_x. You find the linear entry corresponding to position Y, and take that offset, call it O_y. When you go through the selected bins, filter out any chunk whose offset is &lt; O_x, or whose offset >= O_y.</p>\n\n<p>Assuming your datafile is sorted, the chunk range you're interested in is contained between the minimum offset and the maximum offset selected.</p>\n\n<p>The virtual offsets are 64bit unsigned integers. The higher 48 bits of the virtual chunk offset corresponds to the file offset of the datafile (the other 16 you don't need. They are the offset of the record within that decompressed chunk).</p>\n\n<p><strong>Software</strong></p>\n\n<p>The software I have found typically mimics closely the hts_lib API. And hts_lib doesn't export a function which loads an index without also requiring a datafile. As far as I know.</p>\n","3235","","3235","2020-02-29T03:47:50.877","2020-02-29T03:47:50.877","","","","2","",""
"11260","2","","11196","2020-01-28T15:13:07.623","0","","<p>Here's my attempt.</p>\n\n<pre><code>library(tidyverse)\nlibrary(S4Vectors)\n\n# This would probably be read in using `read_tsv()` or something\nrawData &lt;- structure(\n    list(\n        Readlength = c(25, 26, 27, 28, 29, 30, 31, 32, \n                       33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, \n                       49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, \n                       65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, \n                       81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, \n                       97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, \n                       110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, \n                       123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, \n                       136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, \n                       149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, \n                       162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, \n                       175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, \n                       188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, \n                       201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, \n                       214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, \n                       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, \n                       240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, \n                       253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, \n                       266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, \n                       279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289), \n        Sample1 = c(457, 603, 599, 686, 796, 1009, 1225, 1579, 1817, 2322, 2861, 3460, \n                    4161, 4930, 5910, 7007, 8339, 10105, 11720, 13312, 15430, 17515, \n                    20022, 22825, 26150, 28998, 33236, 37302, 41212, 45683, 49844, \n                    53833, 58486, 64650, 69514, 76202, 83273, 90783, 98805, 107247, \n                    115200, 123359, 134258, 142179, 155549, 165425, 178681, 194293, \n                    211417, 225639, 242618, 259391, 274041, 295365, 315802, 337826, \n                    363906, 388803, 416728, 446797, 474343, 496615, 524080, 549720, \n                    573864, 604559, 632324, 657924, 686875, 712778, 732786, 755081, \n                    772734, 787861, 807004, 823211, 836362, 853124, 865378, 874578, \n                    887322, 888977, 889700, 893138, 887592, 888684, 887037, 887121, \n                    883085, 886536, 875703, 870749, 863969, 849188, 840690, 834472, \n                    822558, 811864, 805300, 794602, 784650, 775107, 763282, 748652, \n                    736764, 723134, 710686, 701834, 689531, 678070, 668757, 654506, \n                    644720, 630305, 615571, 603896, 597816, 584121, 571689, 564142, \n                    552281, 543184, 532683, 520019, 510484, 507832, 495343, 484169, \n                    471422, 460944, 448227, 437738, 428359, 417445, 408065, 395244, \n                    386510, 377028, 366563, 357659, 348242, 337553, 327824, 317584, \n                    306454, 295255, 284770, 274444, 264469, 253923, 243462, 233021, \n                    223360, 213341, 202191, 193325, 182022, 173011, 164207, 154985, \n                    147070, 138927, 130203, 122935, 115565, 107969, 100995, 94510, \n                    88365, 82247, 77315, 71657, 66920, 61298, 57765, 53811, 48906, \n                    44847, 41546, 38125, 35039, 31902, 29076, 26271, 24037, 21487, \n                    19187, 17317, 15741, 14000, 12494, 10963, 9928, 8652, 7604, 6515, \n                    5735, 4783, 4108, 3611, 3224, 2676, 2261, 1936, 1664, 1529, 1180, \n                    975, 897, 812, 712, 587, 555, 524, 443, 411, 382, 344, 340, 362, \n                    325, 290, 319, 293, 304, 354, 296, 315, 315, 331, 321, 326, 349, \n                    370, 318, 344, 346, 341, 345, 348, 371, 405, 380, 365, 366, 351, \n                    431, 340, 405, 370, 374, 385, 361, 410, 410, 368, 397, 379, 384, \n                    389, 419, 403, 423, 480, 478), \n        Sample2 = c(580, 648, 715, 914, \n                    1018, 1366, 1653, 2135, 2678, 3424, 4167, 5220, 6088, 7609, 9308, \n                    11062, 13341, 16288, 19023, 22152, 25360, 28789, 32985, 38019, \n                    42972, 49293, 56239, 62791, 69865, 76922, 82906, 90092, 98194, \n                    105602, 114319, 123807, 133289, 146387, 159302, 168856, 180408, \n                    191083, 200708, 212830, 226788, 239063, 255338, 272725, 288512, \n                    307752, 324919, 338429, 354866, 373456, 389945, 411033, 437069, \n                    460419, 487959, 518295, 538191, 558646, 583043, 601107, 623715, \n                    648209, 668883, 698291, 727313, 745380, 766930, 784337, 793358, \n                    804737, 820443, 826114, 839164, 853144, 861670, 875069, 883285, \n                    875065, 877648, 875875, 865221, 863268, 863513, 855519, 854312, \n                    855445, 844795, 837992, 830157, 809476, 798965, 791200, 776191, \n                    765913, 761200, 747483, 739639, 730152, 713285, 699446, 686165, \n                    671281, 658314, 650564, 635331, 628475, 617210, 601336, 592700, \n                    579707, 562022, 550812, 540016, 526066, 516774, 509033, 496389, \n                    487283, 477035, 461698, 451095, 446390, 431015, 417551, 404674, \n                    390280, 380134, 369703, 354902, 343343, 331193, 315775, 305048, \n                    292336, 277876, 267642, 255871, 242636, 231054, 219066, 204829, \n                    192423, 180440, 166529, 155605, 145166, 134932, 124474, 114975, \n                    105267, 95632, 87027, 78802, 71116, 64758, 57082, 51728, 46295, \n                    40908, 36275, 31825, 28028, 24447, 21125, 18489, 15878, 13881, \n                    12183, 10305, 9096, 7759, 6645, 5532, 4642, 3997, 3360, 2858, \n                    2447, 2185, 1811, 1494, 1313, 1028, 954, 819, 693, 614, 556, \n                    549, 421, 404, 394, 325, 301, 308, 290, 307, 304, 273, 303, 302, \n                    291, 313, 288, 274, 281, 318, 288, 299, 314, 336, 354, 339, 342, \n                    366, 353, 373, 340, 345, 341, 365, 381, 366, 415, 399, 377, 410, \n                    421, 433, 429, 439, 433, 400, 419, 499, 418, 454, 441, 476, 464, \n                    461, 483, 481, 503, 493, 529, 469, 503, 496, 491, 521, 514, 501, \n                    546, 502, 513, 571, 519, 536, 653, 564)),\n    class = c(""spec_tbl_df"", ""tbl_df"", ""tbl"", ""data.frame""),\n    row.names = c(NA, -265L), \n    spec = structure(\n        list(\n            cols = list(\n                Readlength = structure(\n                    list(), class = c(""collector_double"", ""collector"")\n                    ), \n                Sample1 = structure(\n                    list(), class = c(""collector_double"", ""collector"")\n                ), \n                Sample2 = structure(\n                    list(), class = c(""collector_double"", ""collector""))\n                ), \n            default = structure(\n                list(), class = c(""collector_guess"", ""collector"")\n            ), \n            skip = 1), \n        class = ""col_spec"")\n    )\n\nrlData &lt;- rawData %&gt;%\n    pivot_longer( # Put all the counts in a single column\n        cols = contains(""Sample""),\n        names_to = ""Sample"",\n        values_to = ""Count""\n        ) %&gt;%\n    split(f = .<span class=""math-container"">$Sample) %&gt;% # Separate into each sample\nlapply(function(x){\n    # Now create a run-length encoded vector\n    # This encodes millions of repetitive values into 2 numbers\n    # 1) the value &amp; 2) how many\n    Rle(values = x$Readlength, lengths = x$</span>Count)\n        # They can be a bit difficult to plot though\n    }) \n\n# I would create the quantiles for the boxplot separately now\nq &lt;- c(min = 0, q25 = 0.25, median = 0.5, q75 = 0.75, max = 1)\n# Collect the stats\nbxp &lt;- rlData %&gt;%\n    lapply(quantile, probs = q) %&gt;% # for every Rle, get the stats \n    as_tibble() %&gt;%\n    mutate(stat = names(q)) %&gt;% # This just collates the output\n    pivot_longer(\n        # Make it long\n        cols = contains(""Sample""),\n        names_to = ""Sample"",\n        values_to = ""value""\n    ) %&gt;%\n    pivot_wider(\n        # Now spread it out for each stat/column\n        names_from = stat,\n        values_from = value\n    ) %&gt;%\n    mutate(\n        # set a few values for easier plotting\n        IQR = q75 - q25,\n        Sample = as.factor(Sample),\n        x = as.integer(Sample)\n    )\n\nw &lt;- 0.4 # Box width\nbxp %&gt;%\n    mutate(\n        # Set the width of the boxes\n        xmin = x - w,\n        xmax = x + w,\n        # Set the values for the whiskers. \n        # These usually extend 1.5*IQR. \n        # Any points beyond are considered outliers\n        boxMax = case_when(\n            max &gt; q75 + 1.5*IQR ~ q75 + 1.5*IQR,\n            max &gt; q75 + 1.5*IQR ~ max\n        ),\n        boxMin = case_when(\n            min &lt; q25 - 1.5*IQR ~ q25 - 1.5*IQR,\n            min &gt; q25 - 1.5*IQR ~ min\n        )\n    ) %&gt;%\n    ggplot(aes(x = x, y = median)) +\n    geom_rect(\n        # Draw the box\n        aes(xmin = xmin, xmax = xmax, ymin = q25, ymax = q75),\n        fill = ""grey80"",\n        colour = ""black""\n    ) +\n    geom_segment(\n        # Add the median\n        aes(x = xmin, xend = xmax, yend = median),\n        colour = ""black""\n    ) +\n    geom_segment(\n        # The top whiskers\n        aes(x = x, xend = x, y = q75, yend = boxMax)\n    ) +\n    geom_segment(\n        # The bottom whiskers\n        aes(x = x, xend = x, y = q25, yend = boxMin)\n    ) +\n    geom_point(aes(y = max)) + # Add some symbolic outliers if you want\n    geom_point(aes(y = min)) +\n    scale_x_continuous(\n        # Tidy up the x-axis ticks\n        breaks = bxp<span class=""math-container"">$x,\n    labels = bxp$</span>Sample\n    ) +\n    labs(\n        # Tidy up the labels\n        x = ""Library"",\n        y = ""Read Length""\n    ) +\n    theme_bw() # Get rid of the stupid grey background\n</code></pre>\n\n<p>That gives me this figure</p>\n\n<p><a href=""https://i.stack.imgur.com/r3Ice.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r3Ice.png"" alt=""enter image description here""></a></p>\n\n<p>Hope that solves it for you. I probably went the long way around though :)</p>\n","7020","","","","2020-01-28T15:13:07.623","","","","1","",""
"11261","2","","11254","2020-01-29T07:21:52.870","1","","<p>Pathways are generally directed graphs, which can be implemented in python using the <code>networkx</code> package. Note that there will be cycles in graphs.</p>\n\n<p>Please note that there are large numbers of pathways analysis packages that already exist (both python and R packages but also just webservers), so I strongly recommend searching to see if something already exists before wasting time and implementing it yourself.</p>\n","77","","","","2020-01-29T07:21:52.870","","","","3","",""
"11262","1","11263","","2020-01-29T10:38:07.387","-3","79","<p>I have a data frame</p>\n\n<pre><code>&gt; head(custom.cn.data)\n      Gene       Sample_name         CN\n1: DDX11L1          F02         0.6788844\n2: DDX11L1          C02        -0.9924524\n3: DDX11L1          A07        -2.3833559\n&gt; \n</code></pre>\n\n<p>I want to convert values in CN column &lt; 0 to Del and >0 to Amp</p>\n\n<p>But I am getting error</p>\n\n<pre><code>&gt; a %&gt;% mutate(q = replace(CN, which(CN&lt;0), Del))\nError in replace(CN, which(CN &lt; 0), Del) : object 'Del' not found\n\n&gt; a<span class=""math-container"">$CN &lt;- replace(a$</span>CN, which(a<span class=""math-container"">$CN &lt; 0), Del)\nError in replace(a$</span>CN, which(a$CN &lt; 0), Del) : object 'Del' not found\n</code></pre>\n\n<p>Any help?</p>\n","4595","","","","2020-01-30T11:15:32.617","Converting values to string","<r>","3","2","",""
"11263","2","","11262","2020-01-29T10:42:12.237","6","","<pre><code>custom.cn.data<span class=""math-container"">$new_column &lt;- ifelse(custom.cn.data$</span>CN &lt; 0, ""Del"", ""Amp"")\n</code></pre>\n\n<p>In this scenario <code>CN == 0</code> would be <code>Amp</code></p>\n","3541","","","","2020-01-29T10:42:12.237","","","","0","",""
"11264","2","","11262","2020-01-29T10:48:09.770","4","","<p>The following code would create a new column with the desired values, 0 values will have <code>NA</code> in this new column.</p>\n\n<pre><code>custom.cn.data[custom.cn.data$CN &lt; 0, ""CN_string""] &lt;- ""Del"" \ncustom.cn.data[custom.cn.data$CN &gt; 0, ""CN_string""] &lt;- ""Amp""\n</code></pre>\n","6955","","","","2020-01-29T10:48:09.770","","","","0","",""
"11265","1","","","2020-01-29T13:37:56.677","0","32","<p>I have a metagenomic dataset across multiple samples that has been grouped and binned (we did this due to poor sequence quallity). I have annotated the bins and now want to calculate the abundance of each bin in a sample but looking at the number of reads from a sample that map to each gene in a bin. </p>\n\n<p>Is this a viable method? If so how would I do it and if not what else could I do?</p>\n","7031","","","","2020-01-29T13:37:56.677","How to calculate coverage depth of genes in an annotated sequence?","<genome><metagenome>","0","3","",""
"11267","2","","11253","2020-01-30T09:21:41.813","0","","<p>You can make use of <code>table()</code> and <code>prop.table()</code> to calculate the number and proportions of your cell types with respect to tissue of origin.</p>\n\n<p><code>prop.table()</code> requires a table as input so you can start with something like <code>prop.table(table(object@meta.data$cell_type, object@meta.data$tissue)</code>. This will create an objet of class <code>table</code>.</p>\n\n<p>Next step is to create a data frame with <code>as.data.frame(your_table_object)</code> and then to convert it to ""long format"", for example with <code>tidyr::pivot_longer()</code>. You can feed these data in long format to <code>ggplot()</code>.</p>\n","3541","","","","2020-01-30T09:21:41.813","","","","0","",""
"11268","2","","11250","2020-01-30T10:55:19.290","5","","<p><strong>Overview</strong>\nThe central focus of the tree is to highlight the key biological concern of the new coronavirus, 2019-nCov. The key concern is the genetic similarities to SARS epidemic, and relates to the SARS receptor. </p>\n\n<p><strong>SARS background</strong> SARS is endemic in bats (your <a href=""https://www.ecohealthalliance.org/2020/01/phylogenetic-analysis-shows-novel-wuhan-coronavirus-clusters-with-sars"" rel=""noreferrer"">BioRxiv tree</a> partly shows that and <a href=""https://bioinformatics.stackexchange.com/questions/11251/how-to-interpret-this-blast-tree"">this tree</a> definitely shows it) and in the 2002 epidemic infected civet cats which then infected humans. More importantly it is then transmitted from one human to another. This is a consequence of the SARS receptor been able to exploit the cellular receptors in the respiratory tract of bats, civet cats and humans and use this tissue as a site of replication. The concern is exacerbated because the tree here shows SARS independently infected humans on two separate occasions, suggesting the cross-over has an underlying genetic basis, which is part of the ""SARS lineage"".</p>\n\n<p><strong><a href=""https://www.ecohealthalliance.org/2020/01/phylogenetic-analysis-shows-novel-wuhan-coronavirus-clusters-with-sars"" rel=""noreferrer"">BioRxiv 2019-nCov tree</a></strong> Your tree shows the 2019-nCov has a recent common ancestor with SARS in comparison to the rest of the betacoronaviruses. This therefore provides some circumstantial evidence the receptor mechanism and ability to frequently crossover from bats and ultimately transmit human to human could be shared with SARS. At an evolutionary level (which is what ANY tree is supposed to reconstruct) it raises the question which common ancestor of SARS did the ""SARS receptor"" originate. I'd need to draw a diagram to better demonstrate this point, but I hope you get the idea.</p>\n\n<p><strong>The ""influenza receptor""</strong> The analogy is with influenza virus and the entire epidemiology of influenza hinges around is sialic acid receptor of influenza and its ability to bind on to the cellular receptors in the upper/lower respiratory tract of birds, pigs and humans. The hypothesised mechanism is called the <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2702078/"" rel=""noreferrer"">mixing vessel theory</a> and is the classic epidemiological understanding of how new pathogenic influenza pandemics occur. If you replace ""birds and pigs"" with e.g. ""bats, civet cats and humans"" you get the idea why this could be scary. We don't the intemediate host of 2019-nCov ... but I speculate there must be one, unless eating Cov-infected bats is common in China.</p>\n\n<p><strong>Technical details of the BioRxiv tree</strong> The tree is a nicely diverse selection of the beta-coronaviruses. The authors have rooted the tree using the outgroups of delacoronaviruses and gammacoronaviruses, so it is a good robust selection of outgroups which can be used to correctly identify the direct of evolution of betacoronavirus divergence. In tree theory (its formal name is phylogenetics theory), extensive rooting is good and minimises artefacts.</p>\n\n<p>One of the earliest members of the betacoronaviruses to diverge is MERS (Middle Eastern respiratory syndrome), which form a single ""clade"" (all viruses share a unique common ancestor) and these represent around 50% amino acid divergence from SARS/2019-nCov. The selection of MERS in the tree from both camels and humans looks good. The selection of other betacoronaviruses looks great, I wasn't aware of the ""ruminant clade""  at all involving buffalo, cow etc .. infections and there was an associated human infection. There are loads of bat isolates of the betacoronaviruses throughout the tree, but we eventually arrive at the ""SARS clade"". The authors show that 2019-nCov is an outgroup to the SARS clade and shows a close relationship to one (BioRxiv tree) or two (this tree) bat isolates. Looking at the precise SARS clade (better shown in this tree) we can see loads of bat virus associating with SARS lineages. We therefore assume the reservoir to SARS and likely 2019-nCov is bat, moreover that the single ancestor to both viruses was a bat (it is called a parsimonious hypothesis). </p>\n\n<p>The one thing the BioRXiv tree omits, for example with regards <a href=""https://bioinformatics.stackexchange.com/questions/11251/interpretion-of-my-coronavirus-2019-ncov-wuhan-china-blast-tree"">this tree</a>, is the diversity of SARS and in particularly the two independent origins of SARS, which is a weakness of their analysis, particular if this fed into downstream analysis. It is not to say the authors were wrong, but it was uncool. </p>\n\n<p><strong>Word of caution</strong> my understanding is the divergence between SARS and 2010-nCov is around 15% and this is a quite a large amount of genetic divergence, even if both viruses share a most recent common-ancestor, form part of the same clade and receptor peptide motifs. Nevertheless there is sufficient amino acid divergence to generate notable differences in epidemiology, clinical symptom and transmission. </p>\n\n<p><strong>MERS, SARS and 2019-nCov mortality rates</strong> It is worth noting that within the betacoronaviruses the mortality rates between difference clinically important viruses is very different. MERS has a mortality rate of 40-60%, SARS is around 10% but 2019-nCov is &lt;2.5%. The 2019-nCov mortality rate is still important given it has infected more and could potentially infect alot more people than SARS.</p>\n","3848","","3848","2020-01-30T13:01:51.960","2020-01-30T13:01:51.960","","","","4","",""
"11269","2","","11262","2020-01-30T11:15:32.617","2","","<p>Try to stay within one package (paradigm) for one problem. This will make the code more readable, easier to debug.</p>\n\n<p>At the moment you have data as a <em>data.table</em> object from <strong>data.table</strong> package, then trying to add a new column using <em>mutate</em> from <strong>dplyr</strong> package, and within <em>mutate</em> using <strong>base</strong> <em>replace</em> function.</p>\n\n<p>Solutions:</p>\n\n<p><strong>data.table</strong></p>\n\n<pre><code># update by row index\ncustom.cn.data[ CN &lt; 0, NewColumn := ""Del"" ]\ncustom.cn.data[ CN &gt;= 0, NewColumn := ""Amp"" ]\n</code></pre>\n\n<p>or use <em>ifelse</em></p>\n\n<pre><code>custom.cn.data[, NewColumn := ifelse(CN &lt; 0, ""Del"", ""Amp"") ]\n</code></pre>\n\n<p><strong>base</strong> </p>\n\n<p>see <a href=""https://bioinformatics.stackexchange.com/a/11264/131"">@DavyCat's</a> and <a href=""https://bioinformatics.stackexchange.com/a/11263/131"">@haci's</a> answers.</p>\n\n<p><strong>dplyr</strong></p>\n\n<p>Note, this will change the <em>data.table</em> into <em>data.frame</em>.</p>\n\n<pre><code># ifelse\ncustom.cn.data &lt;- custom.cn.data %&gt;%\n  mutate(NewColumn = ifelse(CN &lt; 0, ""Del"", ""Amp""))\n</code></pre>\n\n<p>or use <em>case_when</em></p>\n\n<pre><code>custom.cn.data &lt;- custom.cn.data %&gt;%\n  mutate(NewColumn = case_when(\n    CN &lt; 0 ~ ""Del"",\n    CN &gt;= 0 ~ ""Amp""))\n</code></pre>\n","131","","","","2020-01-30T11:15:32.617","","","","0","",""
"11270","1","","","2020-01-30T13:39:25.420","0","15","<p>I am variantcalling my 96 multi sampled bam file with samtools/bcftools however I obtain some strange results.</p>\n\n<p>When I use the following command:</p>\n\n<pre><code>bcftools mpileup -O v -f ref.fa -a DP,AD,ADF,ADR,SP --threads 12 multi_sample_bam_file.bam | bcftools call -c -A -v -V indels --threads 12 -O z -o vcf_out.vcf.gz\n</code></pre>\n\n<p>I get for example, the following line:</p>\n\n<pre><code>Contig0 231901  .       C       T       999     .       DP=1725;VDB=0;SGB=556.637;RPB=0.87714;MQB=0.990852;BQB=0.0121288;MQ0F=0;AF1=0.540145;G3=0.451283,7.53222e-07,0.548716;HWE=4.30362e-25;AC1=104;DP4=829,0,764,0;MQ=60;FQ=999;PV4=1,1,0.0506106,1  GT:PL:DP:SP:ADF:ADR:AD  0/1:0,0,0:0:0:0,0:0,0:0,0    0/0:0,36,193:12:0:12,0:0,0:12,0 0/0:0,21,155:7:0:7,0:0,0:7,0    1/1:167,21,0:7:0:0,7:0,0:0,7\n</code></pre>\n\n<p>What I find kinda strange is that it shows this FORMAT: <code>1/0:0,0,0:0:0:0,0:0,0:0,0</code> Meaning it assigned a heterozygous genotype to my sample. However this sample has not a single read aligned. So shouldn't this be something like\n <code>./.:0,0,0:0:0:0,0:0,0:0,0</code>  (No genotype called)</p>\n\n<p>So my question is:</p>\n\n<ol>\n<li>Why does this happen?</li>\n<li>And is there and easy fix to set those genotypes to the correct <code>./.</code> no call format?</li>\n</ol>\n\n<p>Asked on Biostars as well: <a href=""https://www.biostars.org/p/418753/"" rel=""nofollow noreferrer"">https://www.biostars.org/p/418753/</a></p>\n","2655","","","","2020-01-30T13:39:25.420","Bcftools call always assigns a genotype even if there are no reads?","<vcf><samtools><snp><mpileup><bcftools>","0","0","",""
"11271","1","11272","","2020-01-30T13:50:18.507","1","33","<p>I have been working with several tools on bam files recently and I am not sure how I should interpret some of the outputs.</p>\n\n<pre><code>samtools idxstats mapped.sorted.bam\n</code></pre>\n\n<pre><code>Chr1    15096   0   0\nChr2    33397   0   0\nChr3    43888   41  0\nChr4    20819   1   0\n*   0   0   34\n</code></pre>\n\n<p>Documentation says third and fourth column give ""mapped reads"" and ""unmapped reads"". So I interpreted the last line (<code>*</code>) as the number of unmapped reads as this is the only line with a non 0 value at this field.</p>\n\n<p>Then I used </p>\n\n<pre><code>bam splitChromosome --in mapped.sorted.bam --out test/aln.\n</code></pre>\n\n<pre><code>Reference Name: chr6 has 6 records\nReference Name: Chr7 has 1 records\nReference Name: Chr3 has 41 records\nReference Name: Chr4 has 1 records\nReference Name: * has 34 records\n</code></pre>\n\n<p>And looking at the bams outputted (one per reference sequence) in <code>test/</code> I find an <code>aln.UnknownChrom.bam</code> that I am not sure where it comes from. I actually found 97 unique reads ID in it, where I was waiting for 34 or less. </p>\n\n<p>So my <strong>first question</strong> is : Does ""*"" actually means ""all reads that didn't mapp"" and this information is stored in way I didn't know about in BAM files ?</p>\n\n<p>And my <strong>second question</strong> would be : Anybody knows where this <code>aln.UnknownChrom.bam</code> comes from when using <code>splitChromosome</code> ? It is a source of error in a pipeline I am building and I'd like to get rid of it, if these are not actual mapping informations.</p>\n","6273","","","","2020-01-30T14:58:36.630","samtools / bamUtil | Meaning of <*> as Reference Name","<bam><samtools>","1","0","",""
"11272","2","","11271","2020-01-30T14:07:22.740","2","","<p>As per the SAM/BAM format specifications:</p>\n\n<blockquote>\n  <p>""RNAME: Reference sequence NAME of the alignment. If @SQheader lines are present, RNAME(if not ‘*’) must be present in one of the SQ-SN tag. <strong>An unmapped segment without coordinate has a ‘*’ at this field.</strong>  However, an unmapped segment may also have an ordinary coordinate such that it can be placed at a desired position after sorting.  If RNAME is ‘*’, no assumptions can be made about POS and CIGAR."" -- <a href=""http://samtools.github.io/hts-specs/SAMv1.pdf"" rel=""nofollow noreferrer"">http://samtools.github.io/hts-specs/SAMv1.pdf</a>, emphasis mine</p>\n</blockquote>\n\n<p>So, yes, these are the unmapped reads. </p>\n\n<p>I haven't used this <code>splitChromosome</code> utility before myself, but as it splits the BAM file into one BAM file per chromosome, I suspect it simply also creates a BAM file for the unknown (<code>*</code>) chromosome. Why the numbers don't match I can't tell.</p>\n\n<p>How to deal with that file in your pipeline will depend on what the pipeline is meant to do. But if you aren't interested in these reads, I suppose you could simply remove it before continuing to the next step.</p>\n","6955","","6955","2020-01-30T14:58:36.630","2020-01-30T14:58:36.630","","","","1","",""
"11273","1","","","2020-01-30T17:00:43.330","1","10","<p>I have cfDNA samples which were sequenced (at about 300-1000X) on a 30-gene panel via a capture-seq protocol for the purposes of calling variants on circulating tumour DNA.</p>\n\n<p>However, the resulting variant calls (from both varscan and Mutect2) are dominated by transversions from C>A and G>T at low allele frequencies, which are characteristic of DNA oxidation of guanine to 8-oxoguanine, and which I suspect are artefacts.</p>\n\n<p>I have matched baseline tumour samples from the same patients, sequenced in the same way, (and there are a few C>A/G>T transversions at low AFs in these samples that I can filter out by a direct allele fraction filter).</p>\n\n<p>My question is this, what is the best way of filtering out the false positive mutations derived from the 8-oxoG formation and retaining the true positives (which will be at low allelic fraction)?</p>\n\n<p>Some background: I do have serial (cfDNA) time points for some patients, so I am loath to just look at mutations called in the baseline tumour samples, as any (true) novel variants appearing later in the time points would be missed.</p>\n","2244","","","","2020-01-30T17:00:43.330","Filtering out 8-oxoG-derived artefacts in variant calls from cfDNA","<variants><cfdna><capture-seq>","0","0","","1"
"11274","1","","","2020-01-31T02:51:48.647","0","46","<p>GATK4's <code>BaseRecalibrator</code> uses a list of known variants to adjust the base quality scores in a BAM file. I would like to visualize the pre and post recalibration scores with <code>MultiQC</code>. My workflow is as follows:</p>\n\n<ol>\n<li>Call <code>BaseRecalibrator</code> on the pre-calibration BAM and save base quality scores to <code>pre_recal_data.table</code>. This records pre-recalibration base scores.</li>\n<li>Call <code>ApplyBQSR</code> on pre-calibration BAM using info from <code>pre_recal_data.table</code>. This creates <code>recalibrated.bam</code>.</li>\n<li>Call <code>BaseRecalibrator</code> on <code>recalibrated.bam</code> and write adjusted scores to <code>post_recal_data.table</code>. </li>\n</ol>\n\n<p>In the <code>.table</code> files, there is an argument field <code>recalibration_report</code> that can specify the name of a pre-recalibration report (i.e. <code>pre_recal_data.table</code>). If the field is <code>null</code>, this means the report is for pre-recalibration data. </p>\n\n<p><code>MultiQC</code> uses this field to classify the quality scores as pre or post recalibration. This field doesn't seem to be populated when I run <code>BaseRecalibrator</code>, and I can't find a command line argument to make sure its set to <code>pre_recal_data.table</code>. Here's my code:</p>\n\n<pre><code>gatk BaseRecalibrator -I {input.bam} -R {input.ref} -O pre_recal_data.table \\n            {params.ks}\ngatk ApplyBQSR -I {input.bam} -R {input.ref} -O recalibrated.bam\n            -bqsr pre_recal_data.table \\n            --static-quantized-quals 10 --static-quantized-quals 20 \\n            --static-quantized-quals 30 --add-output-sam-program-record \\n            --create-output-bam-md5 --use-original-qualities\ngatk BaseRecalibrator -I recalibrated.bam -R {input.ref} -O post_recal_data.table \\n            {params.ks}\n</code></pre>\n\n<p>Does anyone know how to force <code>BaseRecalibrator</code> to set the <code>recalibration_report</code> field?</p>\n","6674","","","","2020-01-31T02:51:48.647","Using BaseRecalibrator in GATK4","<bam><fasta><sequencing><variant-calling><gatk>","0","0","",""
"11276","1","","","2020-01-31T12:12:49.253","2","74","<p>I've aligned reads onto an haploid reference genome. I'd like to have a consensus sequence of all my aligned reads that doesn't take into account the reference genome I used (i.e. I just want the nucleotides from my reads at each base (taking into account frequency and quality) but if there is no read a one position I rather have N/? than the reference nucleotide. \nIs there a way to do this ? Thanks</p>\n","6459","","","","2020-02-13T12:50:28.870","Make a fasta out of mapped reads without taking into account the reference","<alignment><bam>","3","0","",""
"11277","1","","","2020-01-31T14:11:23.757","-3","131","<p>I have a big clinical data file with 96 columns like age, gender, BMI, etc</p>\n\n<p>I want to see which of these clinical characteristics relate to response to chemotherapy. Response to chemotherapy divides patients to two groups as <code>yes</code> and <code>no</code>. Among these clinical characteristics, some are <code>numerical</code> and some are <code>categorical</code>. So is it possible to write a function to loop over these characteristics and test for relationship between response to chemotherapy? Something like wilcox test for numerical values and chi square for categorical characteristics. By hand takes to much to do for all one by one and less accurate of course</p>\n\n<p>This is my data</p>\n\n<pre><code>ID  response_to_chemo   BMI Chemo   Predictor   DJANGO  gender\nAH/155  no  50  1   1   0   M\nRS/022  no  67  1   1   0   M\nRS/027  no  80  1   1   1   F\nST/023  no  65  1   1   0   M\nSH/051  yes 47  1   1   0   M\nAH/075  yes 90  0   1   0   M\nRS/047  no  67  0   1   0   F\nST/029  no  61  1   1   0   F\n</code></pre>\n\n<p>For instance </p>\n\n<pre><code>myTable &lt;- table(clin<span class=""math-container"">$gender, clin$</span>response_to_chemo)\n\nchisq.test(myTable)\n</code></pre>\n\n<p>Gives p-value for testing the relationship of gender (CATEGORICAL) in response to chemotherapy</p>\n\n<p>And</p>\n\n<pre><code>t.test(clin<span class=""math-container"">$BMI ~ clin$</span>response_to_chemo)\n</code></pre>\n\n<p>is for BMI</p>\n\n<p>I meant a function calculating these p-values for clinical characteristics one by one </p>\n","4595","","4595","2020-01-31T16:21:47.620","2020-02-10T17:58:50.190","Is it possible to do this in R","<r><statistics><loop>","2","5","",""
"11278","1","","","2020-01-31T16:30:12.643","1","30","<p>How can I obtain a BED file that describes the splicing sites of all genes for GRCh38 human genome ?</p>\n\n<p>Thank you</p>\n","4413","","","","2020-01-31T16:30:12.643","How to get the splicing regions (BED) of all genes for Human GRCh38?","<bed><human>","0","3","",""
"11280","2","","11276","2020-01-31T18:44:38.540","0","","<p>Have you googled <em>de novo assembly</em>?</p>\n","2388","","","","2020-01-31T18:44:38.540","","","","3","",""
"11281","1","","","2020-01-31T19:13:22.650","0","40","<p>I am trying to make a paired matrix of gene-gene correlation. Considering that I have a huge matrix (13000 genes and 900 samples) and for some reasons I don't want to decrease the number of my genes, my gene-correlation matrix would be 13000*13000 and my paired matrix will become 169 million *4 (Column 1: Gene 1; Column 2 : Gene 2; Column 3: Correlations; Column 4: P-values) . In this case, I have to exclude unnecessary calculations as much as I can. I have excluded the situation that Gene 1 = Gene 2. But I couldn't find a way to exclude the condition that ""Column 1: Gene 1 ; Column 2: Gene 2 = Column 1: Gene 2; Column 2: Gene 1 "". To make a long story short, correlation between G1 and G2 is equal to G2 and G1. It is like calculating just lower section of diag in a symmetric matrix. I would be grateful if anybody help me in this case. I have enclosed my python codes here:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport scipy\nimport math\nimport openpyxl\nfrom openpyxl import Workbook\nfrom scipy.stats import spearmanr\n\n\ndin=pd.read_csv('m_test.csv', index_col=0)\n\nout=pd.DataFrame()\noutdf=pd.DataFrame()\n\nfor g1 in din.index:\n    for g2 in din.index:\n        temp=din.loc[[g1, g2]]\n        if g1==g2:\n            next\n        else: \n            spR, spP=spearmanr(temp.loc[g1], temp.loc[g2])\n            frame={'g1':[g1], 'g2':[g2], 'spR':[spR], 'spP':[spP]}\n\n            out=pd.DataFrame(frame)\n\n            outdf=pd.concat([outdf, out])\n</code></pre>\n","6670","","","","2020-02-13T09:47:33.470","How to exclude the repetition of gene-gene correlation calculation in python?","<python><statistics><gene-expression><correlation>","2","0","",""
"11282","2","","11277","2020-01-31T19:35:12.220","2","","<p>The general approach would be to loop through every column starting at column 2. You can use numeric indexes to do that.</p>\n\n<p>For each column, check its type. If it is a factor, use your <code>chisq.test</code> method. If it is of numeric type, use your <code>t.test</code> approach. Write the results to a list so you can parse them later.</p>\n\n<p>This will be a good exercise for you in trying to automate analyses. Please do not expect people to do your job for you - that is why your posts receive downvotes.</p>\n","650","","650","2020-01-31T20:52:03.883","2020-01-31T20:52:03.883","","","","0","",""
"11283","1","11285","","2020-01-31T20:37:11.530","7","807","<p>Quote:</p>\n\n<blockquote>\n  <p>We found 4 insertions in the spike glycoprotein (S) which are unique to the 2019-nCoV and are not present in other coronaviruses. Importantly, amino acid residues in all the 4 inserts have identity or similarity to those in the HIV-1 gp120 or HIV-1 Gag. Interestingly, despite the inserts being discontinuous on the primary amino acid sequence, 3D-modelling of the 2019-nCoV suggests that they converge to constitute the receptor binding site. The finding of <strong>4 unique inserts</strong> <strong>in the 2019-nCoV, all of which have identity/similarity to amino acid residues in key structural proteins of HIV-1 is unlikely to be fortuitous in nature</strong>.</p>\n</blockquote>\n\n<p>Source: <a href=""https://www.biorxiv.org/content/10.1101/2020.01.30.927871v1"" rel=""noreferrer"">https://www.biorxiv.org/content/10.1101/2020.01.30.927871v1</a></p>\n\n<p><a href=""https://i.stack.imgur.com/CzZTD.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CzZTD.jpg"" alt=""enter image description here""></a></p>\n\n<p>I know this is a preprint paper and it's not yet peer reviewed, but can someone tell me what the implications are if this is true? Does this mean that the virus is artificially constructed?</p>\n","6920","","3848","2020-02-27T17:48:07.683","2020-02-27T17:48:07.683","A new paper suggests the Corona Virus has ""Uncanny similarity of unique inserts in the 2019-nCoV spike protein to HIV-1"" - What does this mean?","<sequencing><phylogenetics><phylogeny>","2","0","","1"
"11284","1","","","2020-01-31T17:30:30.037","0","25","<p>I have multiple ligands (in pdb format) which I want to try and dock to various proteins.</p>\n\n<p>But for each ligand I have only a single conformation (by which I meant the spatial relationship between it's atoms, not its position relative to the target protein).</p>\n\n<p>How do I generate other possible conformations?\nIs there a generally used tool for that? </p>\n","7057","user2182857","","","2020-02-03T09:40:40.880","How to generate conformations for a ligand for in silico screening?","<pdb>","2","6","",""
"11285","2","","11283","2020-01-31T23:10:57.760","10","","<hr>\n\n<p><strong>UPDATE:</strong> The article has now been withdrawn with the following note:</p>\n\n<blockquote>\n  <p>This paper has been withdrawn by its authors. They intend to revise it\n  in response to comments received from the research community on their\n  technical approach and their interpretation of the results. If you\n  have any questions, please contact the corresponding author.</p>\n</blockquote>\n\n<hr>\n\n<p>This is very odd, and will require a rigorous investigation, but my initial reaction is one of scepticism. Considering just the 1st insert, the insert sequence is <code>GTNGTKR</code>, short at just 7 amino acids. A simple BLASTP vs NR did not find perfect matches to HIV sequences, but did reveal 100% identity across the full sequence length to >50 other short protein sequences, which could be spurious 'chance' hits of course. Many of the top 100 hits are against eukaryotic sequences, for example one is against <em>Pristionchus pacificus</em>, a type of nematode worm (see alignment below). Someone needs to do a proper peer review of this preprint before any conclusions are drawn from it.</p>\n\n<pre><code>&gt;tank-1 [Pristionchus pacificus]\nSequence ID: PDM74036.1 Length: 2481 \nRange 1: 1474 to 1480\n\nScore:24.0 bits(49), Expect:2477, \nMethod:, \nIdentities:7/7(100%), Positives:7/7(100%), Gaps:0/7(0%)\n\nQuery  1     GTNGTKR  7\n             GTNGTKR\nSbjct  1474  GTNGTKR  1480\n</code></pre>\n\n<hr>\n\n<p><strong>UPDATE 1:</strong> Things are moving fast. There are now 10 comments under the preprint, all of them critical of the idea that these inserts are meaningfully similar to HIV-1 sequences for the reason I outlined above (i.e. short sequences with many hits against organisms from across the tree of life) and additional points, such as small insertions being quite normal evolution for RNA viruses. There are also similar <a href=""https://twitter.com/trvrb/status/1223337991168380928?s=20"" rel=""nofollow noreferrer"">critiques on twitter</a> and biorxiv has added the following disclaimer header to their website:</p>\n\n<blockquote>\n  <p>bioRxiv is receiving many new papers on coronavirus 2019-nCoV.   A\n  reminder: these are preliminary reports that have not been\n  peer-reviewed. They should not be regarded as conclusive, guide\n  clinical practice/health-related behavior, or be reported in news\n  media as established information.</p>\n</blockquote>\n\n<p>I have never seen such rapid (and unanimous) post-publication peer review before. It is therefore already (just 1 day after publication) fairly clear that at least some claims in this study are completely false and there is no good evidence to support the claim that the 2019-nCoV have acquired sequences from HIV or evidence to suggest the virus is engineered.</p>\n","104","","77","2020-02-05T07:49:50.970","2020-02-05T07:49:50.970","","","","3","",""
"11286","2","","7415","2020-02-01T00:23:46.150","2","","<p>The UCSC Genome Browser downloads page has old human genome assembly versions way back to the first human genome ever published (by UCSC in 2000).\n<a href=""http://hgdownload.soe.ucsc.edu/downloads.html#human"" rel=""nofollow noreferrer"">http://hgdownload.soe.ucsc.edu/downloads.html#human</a></p>\n\n<p>We also have a variety of archived dbSNP data up to dbSNP141 on hg38, dbSNP 131 on hg37, dbSNP126 on hg18:\n<a href=""http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/snp141.txt.gz"" rel=""nofollow noreferrer"">http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/snp141.txt.gz</a>\n<a href=""http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/snp138.txt.gz"" rel=""nofollow noreferrer"">http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/snp138.txt.gz</a>\n<a href=""http://hgdownload.soe.ucsc.edu/goldenPath/hg18/database/snp126.txt.gz"" rel=""nofollow noreferrer"">http://hgdownload.soe.ucsc.edu/goldenPath/hg18/database/snp126.txt.gz</a></p>\n\n<p>They're in a custom table format, which can be turned into VCF manually.</p>\n\n<p>You can also get all of this data from Table Browser and add filters, etc.:\n<a href=""http://genome.ucsc.edu/cgi-bin/hgTables"" rel=""nofollow noreferrer"">http://genome.ucsc.edu/cgi-bin/hgTables</a></p>\n","7067","","","","2020-02-01T00:23:46.150","","","","0","",""
"11287","1","","","2020-02-01T00:35:14.790","2","46","<p>I have a BAM file produced using BWA-MEM and I need to calculate the coverage of each exon in this sequencing. I tried using this command:</p>\n\n<p><code>bedtools coverage -a exome_sorted.bed -b exome_alignment.bam -sorted</code></p>\n\n<p>And the result was the following:</p>\n\n<pre><code>chr1    12595   12802   ref|DDX11L1,ref|NR_046018,ens|ENST00000518655,ens|ENST00000450305,ens|ENST00000456328,ens|ENST00000515242  .       ref|DDX11L1     0       0       207     0.0000000\nchr1    13163   13658   ref|DDX11L1,ref|NR_046018,ens|ENST00000518655,ens|ENST00000450305,ens|ENST00000456328,ens|ENST00000515242  .       ref|DDX11L1     0       0       495     0.0000000\nchr1    14620   15015   ref|WASH7P,ref|NR_024540,ens|ENST00000538476,ens|ENST00000438504,ens|ENST00000488147,ens|ENST00000541675,ens|ENST00000423562       .       ref|WASH7P      0       0       395     0.0000000\n...\n</code></pre>\n\n<p>That is, zero coverage for all exons. But there are reads mapping to the exons, as I can see by opening my BAM file on IGV and looking at some exonic regions:</p>\n\n<p><a href=""https://i.stack.imgur.com/TdnLY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TdnLY.png"" alt=""IGV view of the exonic region chr1:12,595-12,802 , clearly showing many reads mapping.""></a></p>\n\n<p>I have already tried sorting the BED file using <code>sort -k1,1 -k2,2n</code> (<a href=""https://bedtools.readthedocs.io/en/latest/content/tools/coverage.html"" rel=""nofollow noreferrer"">as recommended on the Bedtools documentation</a>) and using <code>bedtools sort</code> with the <code>-faidx</code> option, but neither option solved my issue.</p>\n\n<p>I have also checked if the chromosome names are the same in the BAM file and in the BED file, using <code>samtools view -H</code>. Both use <code>chr1,chr2,...,chr22,chrX,chrY</code>, although BAM has <code>chrM</code> and stuff like <code>chrUn_gl000214</code> which are not on the BED.</p>\n\n<p>Thanks!</p>\n\n<p>EDIT:</p>\n\n<p>@ATpoint pointed out that all visible reads in this region have MAPQ0. I looked at another exon that did not have this issue, but Bedtools still outputs zero coverage. IGV picture below:</p>\n\n<p><a href=""https://i.stack.imgur.com/qJvWz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qJvWz.png"" alt=""IGV view of the exonic region chr1:12,595-12,802 , clearly showing many reads mapping.""></a></p>\n","3389","","3389","2020-02-03T18:58:39.353","2020-02-04T16:25:06.760","Bedtools wrongly indicates zero coverage","<bedtools><coverage>","1","4","",""
"11289","2","","11283","2020-02-01T15:50:02.980","6","","<p>Normally ""inserts"" used in the manuscript are ""indels"" in protein alignments, short for insertions and deletions.</p>\n\n<p>What I think has happened is a group investigating indels in HIV env noticed indels in 2019-nCov. Essentially I think the correlation is spurious - but I haven't test it, but the area of research in understanding indels is certainly valid and important.</p>\n\n<p>What is certain is that indels induce a large structural change to a protein structure and any Gibbs free-energy style calculation will identify this.</p>\n\n<p><strong>Vaccine</strong>\nThe spike protein will be the primary candidate to make a 2019-nCov vaccine and that is a very important reason why the sequence was rapidly released. So it is an important protein and the structural changes indels induce mean that a SARS vaccine will probably not provide much protection against 2019-Cov, even apart from the amino acid divergence (below). </p>\n\n<p><strong>Differences 2019-nCov vs HIV</strong>\nIn summary, alot. HIV env and particularly HIV gag are very different from coronaviruses, both in the mechanism of genome replication, coronavirus never leaves the cytoplasm, clinical outcomes, tissue tropism and duration of infection.</p>\n\n<p><strong>Similarities</strong>\nHIV env and the glycoprotein spike of coronaviruses are the receptor binding protein to gain entry into a cell. They are called <strong>structural</strong> proteins. Entry to a cell can be blocked by antibodies and these antibodies are called ""neutralizing antibodies"". Neutralizing antibodies are catastrophic for a virus. Other antibody responses can be effective, such as IgM, but to clear an infection just using antibodies, you need neutralising antibodies. Both HIV env and the coronavirus spike protein are subject to neutralising antibodies. HIV gag has nothing to do with HIV env, in terms of function or antibody exposure. This is why the spike protein will be the primary vaccine candidate for a subunit vaccine.</p>\n\n<p><strong>Coincidence, law of chance</strong>\nThere is large variation of indels in HIV env within HIV and what the authors are inferring is there is a resemblence to that between SARS and 2019-nCov. In my opinion this is a coincidence, because they are comparing a large repertoire of HIV varients against a single indel pattern in the coronaviruses.</p>\n\n<p><strong>Why coronavirus indels?</strong></p>\n\n<p>That is a very good question. Generically indels in viral surface antigen genes are common, much more common in other proteins - such as those involved in virus replication (non-structural proteins). The amino acid identity between SARS and 2019-nCov is 80%, and in any virus, such as flaviviruses 80% identity means indels will be present in surface antigens between the viruses. The answer is it is not unusual in any RNA virus to see indels at a comparatively large amino acid divergence. </p>\n\n<p><strong>What function could they serve</strong></p>\n\n<p>I've briefly looked at indel bioinformatics between flaviviruses (Zika virus, yellow fever virus etc..) notably using envelope (E) protein sequences, and they also occur between African Zika viruses in the E-protein. E-protein being the equivalent of coronavirus spike protein, the receptor-binding protein. No-one has ascribed a function to them and that is the problem with this manuscript.</p>\n\n<p>Hypthoses</p>\n\n<ul>\n<li>One theory is that a structural change in the protein will occur to stop antibody binding.</li>\n<li>Another theory is they have functional differences, such as cell tropism</li>\n</ul>\n\n<p>Bioinformatically separating the two theories is extremely hard without wetlab experimentation.</p>\n","3848","","3848","2020-02-01T18:31:01.017","2020-02-01T18:31:01.017","","","","1","",""
"11290","2","","11284","2020-02-01T16:34:43.037","1","","<p>One option I found was Frog2</p>\n\n<p><a href=""https://mobyle.rpbs.univ-paris-diderot.fr/cgi-bin/portal.py#forms::Frog2"" rel=""nofollow noreferrer"">https://mobyle.rpbs.univ-paris-diderot.fr/cgi-bin/portal.py#forms::Frog2</a></p>\n\n<p><a href=""https://github.com/tuffery/Frog2"" rel=""nofollow noreferrer"">https://github.com/tuffery/Frog2</a></p>\n\n<p>From an input of a <code>mol2</code> or <code>sdf</code> file it creates a plethora of conformers, which can be outputted as a <code>pdb</code> file</p>\n","7057","","","","2020-02-01T16:34:43.037","","","","0","",""
"11291","1","","","2020-02-01T21:36:07.770","1","31","<p>I am looking for a way to download all pdb files that detail a protein-drug complex.</p>\n\n<p>While I could download the entirety of the pdb and search manually (or maybe write a script to discard all proteins that do not have an HETATM entry that details a non-water, non-salt molecule), that sounds effort intensive and error prone.</p>\n\n<p>It seems reasonable that this subset already exist, but I was not able to find it.</p>\n\n<p>Does such a list/database exists?  </p>\n","7057","","","","2020-02-08T15:26:30.383","Looking for a way to download all pdb files that detail a protein-drug complex","<pdb><interactions>","1","3","",""
"11292","1","11293","","2020-02-03T05:36:49.583","1","174","<p>This is my data \n dput(METADATA)</p>\n\n<pre><code>structure(list(Run = c(""SRR2920466"", ""SRR2920477"", ""SRR2920478"", \n""SRR2920506"", ""SRR2920507"", ""SRR2920532"", ""SRR2920531"", ""SRR2920469"", \n""SRR2920470"", ""SRR2920481"", ""SRR2920482"", ""SRR2920500"", ""SRR2920501"", \n""SRR2920536"", ""SRR2920537"", ""SRR2920471"", ""SRR2920472"", ""SRR2920483"", \n""SRR2920484"", ""SRR2920505"", ""SRR2920538"", ""SRR2920539"", ""SRR2920475"", \n""SRR2920476"", ""SRR2920487"", ""SRR2920488"", ""SRR2920542"", ""SRR2920543"", \n""SRR2920555"", ""SRR2920559"", ""SRR2920570"", ""SRR2920573"", ""SRR2920578"", \n""SRR2920583"", ""SRR2920586"", ""SRR2920594"", ""SRR2920546"", ""SRR2920547"", \n""SRR2920548"", ""SRR2920549"", ""SRR2920550"", ""SRR2920551"", ""SRR2920553"", \n""SRR2920558"", ""SRR2920563"", ""SRR2920565"", ""SRR2920567"", ""SRR2920569"", \n""SRR2920572"", ""SRR2920575"", ""SRR2920577"", ""SRR2920580"", ""SRR2920582"", \n""SRR2920585"", ""SRR2920589"", ""SRR2920590"", ""SRR2920591"", ""SRR2920593"", \n""SRR2920554"", ""SRR2920552"", ""SRR2920556"", ""SRR2920557"", ""SRR2920560"", \n""SRR2920561"", ""SRR2920562"", ""SRR2920564"", ""SRR2920566"", ""SRR2920568"", \n""SRR2920571"", ""SRR2920574"", ""SRR2920576"", ""SRR2920581"", ""SRR2920584"", \n""SRR2920587"", ""SRR2920588"", ""SRR2920592"", ""SRR2920595"", ""SRR2920579""\n), source_name = c(""HSC"", ""HSC"", ""HSC"", ""HSC"", ""HSC"", ""HSC"", \n""HSC"", ""CMP"", ""CMP"", ""CMP"", ""CMP"", ""CMP"", ""CMP"", ""CMP"", ""CMP"", \n""GMP"", ""GMP"", ""GMP"", ""GMP"", ""GMP"", ""GMP"", ""GMP"", ""Mono"", ""Mono"", \n""Mono"", ""Mono"", ""Mono"", ""Mono"", ""LSC"", ""LSC"", ""LSC"", ""LSC"", ""LSC"", \n""LSC"", ""LSC"", ""LSC"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", \n""Blast"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", \n""Blast"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", ""Blast"", \n""Blast"", ""Blast"", ""Blast"", ""Blast"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", \n""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", \n""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"", ""pHSC"")), class = c(""spec_tbl_df"", \n""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -78L), spec = structure(list(\n    cols = list(Run = structure(list(), class = c(""collector_character"", \n    ""collector"")), `Assay Type` = structure(list(), class = c(""collector_character"", \n    ""collector"")), AvgSpotLen = structure(list(), class = c(""collector_double"", \n    ""collector"")), BioProject = structure(list(), class = c(""collector_character"", \n    ""collector"")), BioSample = structure(list(), class = c(""collector_character"", \n    ""collector"")), Cell_type = structure(list(), class = c(""collector_character"", \n    ""collector"")), `Center Name` = structure(list(), class = c(""collector_character"", \n    ""collector"")), Consent = structure(list(), class = c(""collector_character"", \n    ""collector"")), `DATASTORE filetype` = structure(list(), class = c(""collector_character"", \n    ""collector"")), `DATASTORE provider` = structure(list(), class = c(""collector_character"", \n    ""collector"")), `DATASTORE region` = structure(list(), class = c(""collector_character"", \n    ""collector"")), donorid = structure(list(), class = c(""collector_character"", \n    ""collector"")), Experiment = structure(list(), class = c(""collector_character"", \n    ""collector"")), GEO_Accession = structure(list(), class = c(""collector_character"", \n    ""collector"")), Instrument = structure(list(), class = c(""collector_character"", \n    ""collector"")), LibraryLayout = structure(list(), class = c(""collector_character"", \n    ""collector"")), LibrarySelection = structure(list(), class = c(""collector_character"", \n    ""collector"")), LibrarySource = structure(list(), class = c(""collector_character"", \n    ""collector"")), MBases = structure(list(), class = c(""collector_double"", \n    ""collector"")), MBytes = structure(list(), class = c(""collector_double"", \n    ""collector"")), Organism = structure(list(), class = c(""collector_character"", \n    ""collector"")), Platform = structure(list(), class = c(""collector_character"", \n    ""collector"")), ReleaseDate = structure(list(format = """"), class = c(""collector_datetime"", \n    ""collector"")), sample_acc = structure(list(), class = c(""collector_character"", \n    ""collector"")), `Sample Name` = structure(list(), class = c(""collector_character"", \n    ""collector"")), sample_type = structure(list(), class = c(""collector_character"", \n    ""collector"")), source_name = structure(list(), class = c(""collector_character"", \n    ""collector"")), `SRA Study` = structure(list(), class = c(""collector_character"", \n    ""collector""))), default = structure(list(), class = c(""collector_guess"", \n    ""collector"")), skip = 1), class = ""col_spec""))\n</code></pre>\n\n<p>I want to have the below output to be in my final dataframe which i want to use for renaming the original file.</p>\n\n<pre><code> Run        source_name\n   &lt;chr&gt;      &lt;chr&gt;      \n 1 SRR2920466 HSC1        \n 2 SRR2920477 HSC2       \n 3 SRR2920478 HSC3        \n 4 SRR2920506 HSC4        \n 5 SRR2920507 HSC5        \n 6 SRR2920532 HSC6        \n 7 SRR2920531 HSC7        \n 8 SRR2920469 CMP1        \n 9 SRR2920470 CMP2        \n10 SRR2920481 CMP3    \n</code></pre>\n\n<p>Found a solution but I have to apply it to each string in the last column</p>\n\n<pre><code>i1 &lt;- !sapply(df1, is.numeric)\ndf1[i1] &lt;- lapply(df1[i1], function(x) factor(replace(as.character(x),\n                                                      x == ""HSC"", paste0(x[x==""HSC""], seq_len(sum(x == ""HSC""))))))\n</code></pre>\n\n<p>Right now I am able to match repeated string in the column and add increment.How to apply it for all the other string .</p>\n","285","","285","2020-02-03T06:44:51.120","2020-02-08T12:34:45.967","How to add an increment to a repeated strings","<r>","2","0","",""
"11293","2","","11292","2020-02-03T07:07:09.150","1","","<p>EDIT: Wheres the OP accepted this answer, @zx8754's answer below, as well as the ones in the link they gave, are much more elegant and straightforward.</p>\n\n<hr>\n\n<p>Here is a work-around with <code>table()</code>, <code>lapply()</code> and <code>seq_len()</code> (I have basically used your <code>lapply()</code> call on the output of a <code>table()</code> call):    </p>\n\n<pre><code>my_counts &lt;- table(df1$source_name)\n\n&gt; my_counts\n\nBlast   CMP   GMP   HSC   LSC  Mono  pHSC \n   23     8     7     7     8     6    19 \n\ndf1 &lt;- df1[order(df1$source_name),]\n\n&gt; head(df1)\n          Run source_name\n37 SRR2920546       Blast\n38 SRR2920547       Blast\n39 SRR2920548       Blast\n40 SRR2920549       Blast\n41 SRR2920550       Blast\n42 SRR2920551       Blast\n\n&gt; names(unlist(lapply(my_counts, seq_len)))\n [1] ""Blast1""  ""Blast2""  ""Blast3""  ""Blast4""  ""Blast5""  ""Blast6""  ""Blast7""  ""Blast8""  ""Blast9""  ""Blast10"" ""Blast11"" ""Blast12"" ""Blast13""\n[14] ""Blast14"" ""Blast15"" ""Blast16"" ""Blast17"" ""Blast18"" ""Blast19"" ""Blast20"" ""Blast21"" ""Blast22"" ""Blast23"" ""CMP1""    ""CMP2""    ""CMP3""   \n[27] ""CMP4""    ""CMP5""    ""CMP6""    ""CMP7""    ""CMP8""    ""GMP1""    ""GMP2""    ""GMP3""    ""GMP4""    ""GMP5""    ""GMP6""    ""GMP7""    ""HSC1""   \n[40] ""HSC2""    ""HSC3""    ""HSC4""    ""HSC5""    ""HSC6""    ""HSC7""    ""LSC1""    ""LSC2""    ""LSC3""    ""LSC4""    ""LSC5""    ""LSC6""    ""LSC7""   \n[53] ""LSC8""    ""Mono1""   ""Mono2""   ""Mono3""   ""Mono4""   ""Mono5""   ""Mono6""   ""pHSC1""   ""pHSC2""   ""pHSC3""   ""pHSC4""   ""pHSC5""   ""pHSC6""  \n[66] ""pHSC7""   ""pHSC8""   ""pHSC9""   ""pHSC10""  ""pHSC11""  ""pHSC12""  ""pHSC13""  ""pHSC14""  ""pHSC15""  ""pHSC16""  ""pHSC17""  ""pHSC18""  ""pHSC19""\n\ndf1$new_col &lt;- names(unlist(lapply(my_counts, seq_len)))\n\n&gt; head(df1)\n          Run source_name new_col\n37 SRR2920546       Blast  Blast1\n38 SRR2920547       Blast  Blast2\n39 SRR2920548       Blast  Blast3\n40 SRR2920549       Blast  Blast4\n41 SRR2920550       Blast  Blast5\n42 SRR2920551       Blast  Blast6\n</code></pre>\n\n<p>You can store the initial row order somewhere to have resulting data frame in the same order the initial one.</p>\n","3541","","3541","2020-02-08T12:34:45.967","2020-02-08T12:34:45.967","","","","3","",""
"11294","2","","11292","2020-02-03T07:41:22.420","3","","<p>We need to group by <em>source_name</em> and add row id, here is using <em>data.table</em> package:</p>\n\n<pre><code>library(data.table)\n\n# convert to data.table\nsetDT(df1)\n\n# group by source_name, and add rowid\ndf1[, myNewColumn := paste0(source_name, rowid(source_name))]\n\ndf1\n#            Run source_name myNewColumn\n#  1: SRR2920466         HSC        HSC1\n#  2: SRR2920477         HSC        HSC2\n#  3: SRR2920478         HSC        HSC3\n#  4: SRR2920506         HSC        HSC4\n#  5: SRR2920507         HSC        HSC5\n#  6: SRR2920532         HSC        HSC6\n#  7: SRR2920531         HSC        HSC7\n#  8: SRR2920469         CMP        CMP1\n#  9: SRR2920470         CMP        CMP2\n# 10: SRR2920481         CMP        CMP3\n# ...\n</code></pre>\n\n<p>See <a href=""https://stackoverflow.com/q/12925063/680068"">here</a> for more solutions.</p>\n","131","","","","2020-02-03T07:41:22.420","","","","5","",""
"11295","2","","11284","2020-02-03T09:40:40.880","1","","<h2>Open babel</h2>\n\n<p>The most commonly used tool is <code>open-babel</code> (obabel). It seems annoying that you have to specify the number of conformers, but this is because if you get something like retinol, the number of conformers is huge. The command from the <a href=""https://open-babel.readthedocs.io/en/latest/3DStructureGen/multipleconformers.html"" rel=""nofollow noreferrer"">documentation</a> is:</p>\n\n<pre><code>obabel startingConformer.mol -O ga_conformers.sdf --conformer --nconf 30\n   --score rmsd --writeconformers\n</code></pre>\n\n<p>An alternative argument is <code>--confab</code> which will pick each representative of a RMSD cutoff cluster.</p>\n\n<p>Do note that 2D representations and un-protonated molecules will need extra command line arguments.</p>\n\n<h2>RDKit</h2>\n\n<p>You can also generate conformers in RDkit, but I think this is beyond your requirements, but I thought it may be good to mention the essential compchem python library.</p>\n","6322","","","","2020-02-03T09:40:40.880","","","","0","",""
"11296","2","","11253","2020-02-03T11:11:19.227","0","","<p>If I understand your question correctly and you have <code>dplyr</code> and <code>ggplot</code> installed, you could try something like (if a is your data.frame)</p>\n\n<pre><code>a %&gt;% \ndplyr::group_by(orig.ident, old.ident) %&gt;% \ndplyr::summarise(count  = n()) %&gt;% \nggplot(aes(x=orig.ident, y=count)) + geom_bar(stat='identity', aes(group=old.ident), position='dodge')\n</code></pre>\n","3694","","","","2020-02-03T11:11:19.227","","","","0","",""
"11297","1","11298","","2020-02-03T16:00:28.557","4","63","<p>In the fastq format every 4k+4-th line contains the positionwise qualityscore (ascii encoded):</p>\n\n<pre><code>@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65 \n...\n</code></pre>\n\n<p><strong>which program uses this information besides trimming tools (cutadapt) or quality control (fastqc)?</strong></p>\n\n<p>I heard that segemehl (mapping) abandons this quality string in favor of running time (maybe other mapping tools too?)\nBut what about abyss, trinity, ...? </p>\n\n<p>UPDATE:</p>\n\n<p>So i atrifically generated a fastq file containing 2 identical reads that perfectly match a sequence in some genome (C.fna). \nThe two reads have the most extreme phred scores (only '!!'=highest sanger phread and only 'II'=lowest sanger phred)</p>\n\n<p>Now using segemehl the mapping quality is identical (60)</p>\n\n<pre><code>@HD VN:1.0\n@SQ SN:C_fna    LN:15440\n@RG ID:A1   SM:sample1  LB:library1 PU:unit1    PL:illumina\n@PG ID:segemehl VN:0.3.4    CL:segemehl -i C2.fna.idx -d C2.fna -q test.fq\nA   0   C_fna   261 60  220=    *   0   0   ACAGGAAACACAGAAAAAAGCCCGCACCTGACAGTGCGGGCTTTTTTTTTCGACCAAAGGTAACGAGGTAACAACCATGCGAGTGTTGAAGTTCGGCGGTACATCAGTGGCAAATGCAGAACGTTTTCTGCGTGTTGCCGATATTCTGGAAAGCAATGCCAGGCAGGGGCAGGTGGCCACCGTCCTCTCTGCCCCCGCCAAAATCACCAACCACCTGGTG    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!    HI:i:0  NH:i:1  NM:i:0  MD:Z:220    RG:Z:A1 YZ:Z:0\nB   0   C_fna   261 60  220=    *   0   0   ACAGGAAACACAGAAAAAAGCCCGCACCTGACAGTGCGGGCTTTTTTTTTCGACCAAAGGTAACGAGGTAACAACCATGCGAGTGTTGAAGTTCGGCGGTACATCAGTGGCAAATGCAGAACGTTTTCTGCGTGTTGCCGATATTCTGGAAAGCAATGCCAGGCAGGGGCAGGTGGCCACCGTCCTCTCTGCCCCCGCCAAAATCACCAACCACCTGGTG    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII    HI:i:0  NH:i:1  NM:i:0  MD:Z:220    RG:Z:A1 YZ:Z:0\n[SEGEMEHL] Thu Feb  6 08:35:15 2020: matching w/ suffixarray has taken 0.000000 seconds.\n</code></pre>\n\n<p>So segemehl does NOT use the read quality phred (basically only using the read sequence for mapping)</p>\n","2766","","2766","2020-02-06T07:48:17.467","2020-02-10T16:04:46.210","Is the quality score of fastq used somewhere besides trimming/fastqc?","<rna-seq><fastq>","2","1","",""
"11298","2","","11297","2020-02-03T16:20:38.773","4","","<p>I think almost all alignment programs would use base quality as a factor to calculate mapping quality (See <a href=""http://maq.sourceforge.net/qual.shtml"" rel=""nofollow noreferrer"">here</a>). These would also be used in Variant Calling to pick reads where high quality bases confidently align as either the reference base or one of the alternate bases.</p>\n","650","","","","2020-02-03T16:20:38.773","","","","3","",""
"11299","1","","","2020-02-03T17:50:13.917","2","62","<p>I am analysing a human single cell RNA seq experiment, where we have 4 groups, four samples each. Data has been analysed using Seurat, with the canonical workflow. I have tried DE using various methods (Wilcox, MAST, LR), and for each of them the top genes (biggest fold change, lowest adjusted p) are genes over expressed by specific samples, e.g.:\n<a href=""https://i.stack.imgur.com/PCyTH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PCyTH.png"" alt=""enter image description here""></a></p>\n\n<p>Looking at the average expression for these genes per each subject, and plotting on a Boxplot, it does not seem to be a genuine difference (e.g. for the gene above):\n<a href=""https://i.stack.imgur.com/l0041.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l0041.png"" alt=""enter image description here""></a></p>\n\n<p>Besides the subject, there is no other parameter (e.g. age, sex) that is biased across groups and that shows up as differentially represented in the PCA plot.</p>\n\n<p>Is there a DE method that takes into account this behaviour and corrects for it?\nHow can these results be presented? \nBesides increasing number of subjects (not really feasible), is there a solution to analyse these data?</p>\n\n<p>Thanks a lot!</p>\n","7113","","7113","2020-02-04T10:41:55.230","2020-02-06T06:59:33.553","Differential gene expression bias due to effect of an individual sample","<rna-seq><scrnaseq><seurat><differential-expression>","1","5","",""
"11300","2","","11281","2020-02-03T21:44:12.630","0","","<p>You don't need <code>g2</code> to go from <code>0</code> to <code>din.index</code>, just from <code>0</code> to <code>g1 - 1</code>. This way, you'll end up calculating just for the ""lower triangle"":</p>\n\n<pre><code>g1-&gt; 0   1   2   3\ng2\n0                    #(0,0 to 0,1-1 = nothing)\n1    X               #(1,0 to 1,1-1 = 1,0)\n2    X   X           #(2,0 to 2,2-1 = 2,0; 2,1)\n3    X   X   X       #(3,0 to 3,3-1 = 3,0; 3,1 and 3,2)\n4    X   X   X   X   #(4,0 to 4,4-1 = 4,0; 4,1; 4,2 and 4,3)\n</code></pre>\n","650","","","","2020-02-03T21:44:12.630","","","","1","",""
"11301","1","11306","","2020-02-04T01:54:06.940","3","324","<p>I have heard several conspiracy theories regarding the origin of the new coronavirus, 2019-nCov. For example that the virus and/or SARS were produced in a laboratory or were some variant of Middle Eastern respiratory syndrome (MERS), shipped via laboratory workers. </p>\n\n<p>I am well aware bioinformatics has debunked many conspiracy theories involving infectious diseases, an important one being polio vaccination programs in Africa were the origin of HIV, for example <a href=""https://www.historyofvaccines.org/content/articles/debunked-polio-vaccine-and-hiv-link"" rel=""nofollow noreferrer"">here</a>. Alternatively, numerous conspiracy theories on the man-made origins of HIV were similarly overturned using bioinformatic studies.</p>\n\n<p>Do you have bioinformatics evidence that would debunk the current conspiracists about coronaviruses?</p>\n","7122","","3541","2020-02-05T21:58:54.773","2020-02-05T21:58:54.773","Is it possible for coronavirus or SARS to be synthetic?","<phylogenetics><protein-structure><phylogeny>","1","1","","1"
"11302","2","","11277","2020-02-04T05:46:36.093","5","","<p>R supports logistic regression, which would seem to be the most efficient method for tackling this question. Assuming the ""Chemo"" variable is the type of chemo the  code would be something like:</p>\n\n<pre><code>glm( (response_to_chemo == ""yes"") ~  BMI + Chemo  + Predictor +  DJANGO + gender, \n        family=""binomial"")\n</code></pre>\n\n<p>EDIT: Corrected a typo (added a missing double quote)</p>\n","6668","","650","2020-02-10T17:58:50.190","2020-02-10T17:58:50.190","","","","0","",""
"11303","1","","","2020-02-04T14:37:00.590","0","34","<p>In order to obtain orthologs for human genes, I am working with <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2808972/"" rel=""nofollow noreferrer"">InParanoid version 7.0</a>. This version of the database uses Ensembl Protein Identifiers (<strong>ENSP</strong>) from ENSEMBL version v54, which is based on <strong>NCBI 36</strong> assembly of the human genome.</p>\n\n<p>For my project, I need to have the Ensembl Gene Identifiers (<strong>ENSG</strong>) of these proteins, but in the last version of ENSEMBL, which is based on <strong>GRCh38</strong> assembly. I need to use this version of InParanoid for consistency with previous analyzes.</p>\n\n<p>Thus, my question is, <strong>could I just use biomaRt to map ENSP (from NCBI36) to ENSG (actual version)</strong> or this approach is inherently wrong?</p>\n\n<p>Is it possible to ""convert"" ENSP identifiers from NCBI36 to GRCh38 and then do the map to ENSG?. I am aware of <a href=""https://genome.ucsc.edu/cgi-bin/hgLiftOver"" rel=""nofollow noreferrer"">liftOver</a> but I am not sure it would work here.</p>\n","678","","","","2020-02-04T15:27:04.863","Ensembl protein identifiers from different assemblies","<annotation><ensembl><liftover>","1","0","",""
"11304","2","","11303","2020-02-04T14:43:08.397","4","","<p>Those IDs are elderly! Ensembl 54 was 2009!</p>\n\n<p>I would recommend using BioMart combined with the <a href=""http://www.ensembl.org/Homo_sapiens/Tools/IDMapper"" rel=""nofollow noreferrer"">ID history converter</a>. The ID history converter will convert old IDs to new, and BioMart will convert ENSPs to ENSGs. You can either use the ID history converter with the ENSPs first, then the <a href=""http://www.ensembl.org/biomart/martview"" rel=""nofollow noreferrer"">current BioMart</a> to get the ENSGs. Or you can use <a href=""http://may2009.archive.ensembl.org/biomart/martview"" rel=""nofollow noreferrer"">Ensembl 54 BioMart</a> to convert the old ENSPs to old ENSGs, then update the ENSGs with the ID history converter. I would probably do both and compare results.</p>\n","1332","","1332","2020-02-04T15:27:04.863","2020-02-04T15:27:04.863","","","","5","",""
"11305","2","","11287","2020-02-04T16:25:06.760","0","","<p>If this is your output :</p>\n\n<pre><code>chr1    12595   12802   ref|DDX11L1,ref|NR_046018,ens|ENST00000518655,ens|ENST00000450305,ens|ENST00000456328,ens|ENST00000515242  .       ref|DDX11L1     0       0       207     0.0000000\n</code></pre>\n\n<p>Then I guess your bedfile looks like this (the first 6 fields) :</p>\n\n<ul>\n<li>chrom : chr1</li>\n<li>FeatureStart : 12595</li>\n<li>FeatureEnd : 12802</li>\n<li>name : ref|DDX11L1,ref|NR_046018,ens|ENST00000518655,ens|ENST00000450305,ens|ENST00000456328,ens|ENST00000515242</li>\n<li>score : .</li>\n<li>strand : ref|DDX11L1</li>\n</ul>\n\n<p>The issue I see here is that you are giving the wrong information to the strand field of your bedfile. It should be either <code>+</code>, either <code>-</code> or <code>.</code> if you don't want to consider the strand information.</p>\n\n<p>Hope it helps !</p>\n","6273","","","","2020-02-04T16:25:06.760","","","","0","",""
"11306","2","","11301","2020-02-04T16:30:11.957","13","","<p>The scenarios are impossible and would be laughable if they were not so serious. The evidence is in the phylogenetic trees. Its a bit like a crime scene when the forensics team investigate. We've done enough crime-scenes often going to the site, collecting the pathogen, sequencing and then analysis - (usually neglected diseases) without any associated conspiracy theories. </p>\n\n<p>The key technical issue is coronaviruses are <strong>zoonoses</strong> , pathogens spread to humans from animal reservoirs and phylogenetic tree really helps understand the  how the virus is transmitted.</p>\n\n<p><em>Trees</em> </p>\n\n<ol>\n<li>The key thing about all the trees are <strong>bats</strong>. Bats lineages are present at every single point of the betacoronavirus phylogeny (tree), both as paraphyletic and monophyletic lineages, one example is this tree of betacoronaviruses <a href=""https://www.ecohealthalliance.org/2020/01/phylogenetic-analysis-shows-novel-wuhan-coronavirus-clusters-with-sars"" rel=""noreferrer"">here</a>. Meaning the nodes connecting the branches of the tree to the ""master-branch"", represent common ancestors and these were almost certainly bat-borne cornaviruses. This is especially true for SARS and - <a href=""https://bioinformatics.stackexchange.com/questions/11251/interpretion-of-my-coronavirus-2019-ncov-wuhan-china-blast-tree"">here</a> bat-viruses are EVERYWHERE. </li>\n<li>The tree <a href=""https://bioinformatics.stackexchange.com/questions/11251/interpretion-of-my-coronavirus-2019-ncov-wuhan-china-blast-tree"">here</a> also shows that SARS arose on independently on two occassions, again surrounded by bat lineages and 2019-nCov has emerged separately at least once, again associated with bats. </li>\n<li>Finally the tree below is a figure from BioRxiv Zhou et al (2020) ""Discovery of a novel coronavirus associated with the recent pneumonia outbreak in humans and its potential bat origin"" shows the 2019-nCov lineage is a direct descendent of a very closely virus isolated from a bat (RaTG13*). This is a really conclusive finding BTW.\n<a href=""https://i.stack.imgur.com/55HtA.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/55HtA.jpg"" alt=""enter image description here""></a></li>\n</ol>\n\n<p><strong>Note</strong>, I don't normally present inline images, but it is such a nice finding (hint to reviewers) and BioRxiv is open access.</p>\n\n<p><strong>Conspiracy theory 1</strong>: <em>laboratory made virus</em></p>\n\n<p>Literally it would require someone passaging a new virus, with unknown human pathogenicity, and independently introducing all the earlier passages enmass across bat populations of China. They would then hope each lineage becomes an indepedent virus population before, then introducing the virus to humans. Thus when field teams of scientists go around using mist nets to trap the bats, buy them from markets, isolate the virus and sequence it they would find a beautiful, array of natural variation in the bat populations leading up to the human epidemics, that perfectly matches vast numbers of other viral zoonoses. Moreover, this would have to have happen substancially prior to SARS and 2019-nCov, because the bat betacoronaviruses have been known about prior both epidemics, viz. its simply not feasible.</p>\n\n<p><strong>Biological explanation</strong>\n<em>General</em> Bats are a reservoir host to vast numbers of pathogens, particularly viruses, including many alphaviruses, flaviviruses, rabies virus and beleived to be important in ebolavirus (I don't know about this) and even important to several eukaryotic parasites. It makes sense, they are mammals, so evolutionary much closer to us than birds for example, with large dispersal potential and roost in 'overcrowded' areas enable rapid transmission between bats.</p>\n\n<p><em>Technical</em>\nThe trees show bats are the common ancestor of betacoronaviruses in particular for the lineage leading into the emergence of 2019-nCov and SARS, this is seen in this <a href=""https://www.ecohealthalliance.org/2020/01/phylogenetic-analysis-shows-novel-wuhan-coronavirus-clusters-with-sars"" rel=""noreferrer"">tree</a>, this <a href=""https://bioinformatics.stackexchange.com/questions/11251/interpretion-of-my-coronavirus-2019-ncov-wuhan-china-blast-tree"">one</a> and the tree above. The obvious explanation is the virus circulates endemically in bats and has jumped into humans. For SARS the intermediate host, or possible ""vector"" was civet cats. </p>\n\n<p>The theory and the observations fit into a seamless biological answer.</p>\n\n<p><strong>Conspiracy theory 2</strong>: <em>Middle Eastern connection</em></p>\n\n<p>I heard a very weird conspiracy theory attempting to connect MERS with 2019-nCov. The theory was elaborate and I don't think it is productive to describe here.</p>\n\n<p><strong>Biological explanation</strong>\nAll the trees of betacoronaviruses show MERS was one of the earliest viruses to diverge and is very distant from 2019-nCov, to the extent the theory is completely implausible. The homology between these viruses is 50%, so its either MERS or 2019-nCov. Its more extreme than mixing up yellow fever virus (mortality 40-80%) with West Nile virus (mortality &lt;&lt;0.1%), the two viruses are completely different at every level.</p>\n\n<p><strong>What about errors?</strong> Phylogeneticists can spot it a mile off. There are tell-tale phylogenetic signatures we pick up, but also we do this to assess 'rare' genetic phenomina. There is nothing 'rare' about the coronaviruses. The only anomaly is variation in the poly-A tail and that is the natural variation from in vitro time-series experiments. Basically we've looked at enough virses/parasites through trees, that have no conspiracy theories at all (often neglected diseases), and understand how natural variation operates - so a phylogenecist can shift the wheat from the chaff without really thinking about it.</p>\n\n<p><strong>Opinion</strong>\nThe conspiracy theories are deeply misplaced, and the only connection I can imagine is its China. However, the Chinese have loads of viruses, influenza in particular which causes major pandemics, but that is a consequence of their natural ecology (small-holder farming) allowing the virus to move between reservoir hosts. I've not visited small-hold farms in China, but I have in other parts of the world and when you see them, you get it. The pigs, chickens (ducks .. China), dogs, horses and humans all living within 10 meters of each other. </p>\n\n<p><strong>Conclusion</strong>\nShipping large numbers of bats to market, bat soup, raw meat from arboreal (tree-living) mammals such as civets that are sympatric to bats. Then consider the classical epidemiology in light of the phylogenetic data, which is very consistent, a single picture emerges that coronavirus is one of many zoonoses which has managed to transmit between patients. </p>\n\n<p><strong>Summary</strong> The fundamental point is the bioinformatics fit into the classical epidemiology of a zoonose.</p>\n\n<p>*, <strong>Note</strong> The bat coronavirus RaTG13 predates the 2019-nCov outbreak by 7 years. It is not even clear whether the virus has been isolated, i.e. could just be a RNA sequence.</p>\n\n<p>""They have found some 500 novel coronaviruses, about 50 of which fall relatively close to the SARS virus on the family tree, including RaTG13—it was fished out of a bat fecal sample they collected in 2013 from a cave in Moglang in Yunnan province.""</p>\n\n<p>Cohen, ""Mining coronavirus genomes for clues to the outbreak’s origins"" Feb, 2020 Science Magazine,</p>\n","3848","","3848","2020-02-04T21:02:48.577","2020-02-04T21:02:48.577","","","","0","",""
"11307","1","","","2020-02-04T16:34:08.517","0","37","<p>I'm trying to show cell cycle regression and input sample metadata to my umap. </p>\n\n<p>I've been following Satija Lab's tutorials and have generated my umap but now want to specify which sample corresponds to each cluster, if one sample is driving a certain cluster, etc. I also, would like yo show cell cycle regression in each cluster. I following the cell cycle regression tutorial but that only tells me the board overview of the cells in each cycle. I would want to highlight it in the umap. </p>\n\n<p>How would I go about each of these tasks?</p>\n","6827","","","","2020-02-04T19:30:58.680","Seurat clusters","<scrnaseq><seurat>","1","0","",""
"11308","1","11311","","2020-02-04T16:40:40.257","2","104","<p>I wrote some throwaway code for ingesting paired-end reads from a Fastq file into memory.</p>\n\n<pre class=""lang-py prettyprint-override""><code>def ingest(stream1, stream2):\n    while True:\n        record1, record2 = list(), list()\n        for _ in range(4):\n            record1.append(next(stream1))\n            record2.append(next(stream2))\n        yield record1, record2\n</code></pre>\n\n<p>I had initially put the <code>for</code> loop in a <code>try/Except</code> block, anticipating I'd need to explicitly break out of the endless <code>while</code> loop. However, the code seems to work correctly even after I removed it. I even ""corrupted"" the final read in one of the paired files, and the <code>ingest</code> function performs reasonably (the last read pair is never <code>yield</code>ed).</p>\n\n<p>My question is...<strong>why does this code work</strong>, since I never explicitly break out of the loop?</p>\n\n<p>Here's an example of how the code is invoked.</p>\n\n<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; with open('seqR1.fastq', 'r') as fh1, open('seqR2.fastq', 'r') as fh2:\n...     counter = 0\n...     for r1, r2 in interleave(fh1, fh2):\n...         counter += 1\n&gt;&gt;&gt; counter\n5000\n&gt;&gt;&gt; r1\n['@read5000/1\n', 'AGGTCTGGGGACAGGTGTTTTACAGGCTCCTTTAGGGGTGGCAAGTCTCCCGGAAACATGATTAGTGAAGACACGTTAACAGCGAAACTGACAGAGAACCTCAACATTCTCTGGTAGGAAAAGCCAGGATCCTGGCAGAGGAAGCAACGATTCGAGCAGGGAACTGTGCAGGACAGAGAACAATGCGGCCCCAATTTTGATCCAGGGGTAGTTAGCTTAGCTAAGGAGTTCCCCAGTCCCTAAGTGGAGAGGGGATCCAGAACCCTACAATTTATTATTATATCTTAGCGGATATATCTAT\n', '+\n', 'CCCCCGGGEGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG@GGGGGGGFAGGGCGGGGGFGGGGGGGGGCGGGGGDGGGGFGGGGGGGGGCGGGGGGGGGGGGGGGCGGGGGGGGGGGGGGGGG@G&lt;GGGGGGGGGGGGFGGFGFF:DGGFFCGGGGCGGFEGGGGGGEGGGFB:GGGDG=FGGGG,GAGGGDGGFGGGCD3GFG+G?G*FGGG&lt;GCGGGGC@CD&lt;FFGF*:+G=FE&lt;;GG,8C7GF=;9D=G&lt;C5C66G6C&gt;7)FD*6G7D)9;*C:)4D5+9&gt;@G*=0):)*;1*504\n']\n&gt;&gt;&gt; r2\n['@read5000/2\n', 'TGTGGGGTTCTGGATCCCCTCTCCACTTAGGGACTGGGGAACTCCTTAGCTAAGCTAATTACCCCTGGATCAAAATTGGGGCCGCATTGTTCTCTGTCCTGCACAGTTCCCTGCTCGAATCGTTGCTTCCTCTGCCAGGATCCTGGCTTTTCCTACCAGAGAATGTTGAGGTTCTCTGTCATTTTCGCTGCTAATGTGTCTTCACTAATCGTGTTTCCGGGAGACTTGCCACCCCTAAAGGAGCTTGTTGAACACTTGTCACCAGACCTAAGTCCTTAGGGATGAACCTGTGTCTCGGCAC\n', '+\n', 'CCCCBGC9FGCD-GGGFGGGEFEFF@CCGFGGGGFF+DG6,F69+FGG+FGF&lt;FCC;GGEBFGGEDECFGGG6GDG7BFG9GCG9G9GDGGGG&lt;GDFFFGEGC,&lt;GC=GFG:,:A:EE+GD&lt;:E,FGF=FCAGEFF++G,G,&lt;EGGFGFFA&gt;*FG9EGFCFCGC,B&gt;43GFF@E@2&gt;9ED@,568D=A;C+=5@E/F53D&gt;,6F*9@+,E,C=*0+*@56&lt;*9@:/F,1,:*)*14*0CC=*1)*0*,*)0;+)*))/**)**;0;//9)1+)10.()5/)070*)7*C**3/)(2*****\n']\n</code></pre>\n","96","","","","2020-02-05T15:46:30.730","Why does this naive Python paired Fastq reader work?","<python><fastq>","2","0","",""
"11309","2","","11307","2020-02-04T19:30:58.680","0","","<p>You can use the <code>group.by</code> argument of the <code>DimPlot()</code> function. <code>group.by</code> accepts column names present in the <code>meta.data</code> column of the Seurat objects.</p>\n","3541","","","","2020-02-04T19:30:58.680","","","","2","",""
"11310","2","","11308","2020-02-04T21:42:24.807","2","","<p>EDIT: <strong><a href=""https://bioinformatics.stackexchange.com/a/11311/4980"">Chris_Rands's answer</a> is more authoritative and clarifies that this behavior is essentially a bug.</strong>  (I had only tested my example on Python 3.6.8 and hadn't seen PEP 479.)</p>\n\n<p>When <code>next</code> runs out of lines, it raises <code>StopIteration</code>, and apparently that stops the while loop similarly to calling <code>break</code> without you needing to catch the exception.  I couldn't see a definitive source on that but you can see it in action if you catch and re-raise the exception:</p>\n\n<pre><code>def ingest2(stream1, stream2):\n    while True:\n        try:\n            record1, record2 = list(), list()\n            for _ in range(4):\n                record1.append(next(stream1))\n                record2.append(next(stream2))\n            yield record1, record2\n        except StopIteration:\n            print(""StopIteration raised"")\n            raise\n</code></pre>\n\n<p>You can also try to catch it outside the loop, but it doesn't get re-raised at that point.  (If you try raising one yourself in a while loop, though, it <em>doesn't</em> catch it in the same way, so there must be some extra Python magic going on.  I can't see any difference between the two exception objects so I'd be curious what's going on there.)  Neat example.  You could also make a pair of FASTQ parsers using Biopython's <a href=""https://biopython.org/wiki/SeqIO#sequence-input"" rel=""nofollow noreferrer"">SeqIO.parse</a> for more complex/permanent situations if needed.</p>\n","4980","","4980","2020-02-05T15:46:30.730","2020-02-05T15:46:30.730","","","","0","",""
"11311","2","","11308","2020-02-04T21:54:48.543","4","","<p>Your code only runs prior to Python 3.7, now it raises <code>RuntimeError: generator raised StopIteration</code>. This is essentially a bug fix described in <a href=""https://www.python.org/dev/peps/pep-0479/"" rel=""nofollow noreferrer"">PEP 479 -- Change StopIteration handling inside generators</a>. You can enable the new behaviour from Python 3.5 via <code>from __future__ import generator_stop</code>.</p>\n\n<p>Note, it's not related to file-handles per se, you can reproduce it with a smaller example with a simple iterator:</p>\n\n<pre><code>it = iter([1,2])\n\ndef f(it):\n    while True:\n        yield next(it)\n\nprint(list(f(it)))\n</code></pre>\n\n<p>You should not write code relying on the bug</p>\n","104","","","","2020-02-04T21:54:48.543","","","","0","",""
"11313","1","11314","","2020-02-05T01:48:39.167","11","2727","<p>My colleague and I have developed a software tool intend to release it open-source. This tool is specifically for tasks in bioinformatics but we think it would be helpful for the wider community. Our institution will permit us to release it provided we get appropriate credit.</p>\n\n<p>Thus we wish to publish it in peer-review. Is peer-review publication of bioinformatics software available? If so what is required to publish it?</p>\n","1855","","","","2020-02-05T12:25:23.877","Can open-source software be peer-reviewed and published?","<software-development><publishing>","2","4","","2"
"11314","2","","11313","2020-02-05T07:36:47.217","12","","<p>There are whole journals based primarily around publishing open source tools. The primary example of that is ""Bioinformatics"", where a lot of the open-source tools are published. We've also had luck publishing in the Nucleic Acids Research yearly special webservers issue, since we make Galaxy wrappers around our tools.</p>\n\n<p>You can also publish on tools in venues like Nature Methods, but it's vastly more difficult to get in there if your paper is purely on a tool and it's not something game-changing like <code>salmon</code> or <code>kallisto</code>.</p>\n\n<p>Regarding what's actually needed to publish a tool, you will typically need the following:</p>\n\n<ul>\n<li>Decent documentation. I ask for this as a reviewer and am asked about it when I submit things.</li>\n<li>Easy installation. No one wants to spend two hours trying to compile your tool, so make sure there's a conda package or at least a docker container for it.</li>\n<li>Test data. Without this your reviewers and users will likely not be happy since they won't be able to try it out on something small.</li>\n</ul>\n\n<p>For the actual submission you'll tend to need comparisons to other tools. I'm generally not a big fan of this, but you will find that some reviewers will demand it (assuming there are other tools that do something similarish to yours).</p>\n","77","","","","2020-02-05T07:36:47.217","","","","0","",""
"11315","2","","11313","2020-02-05T12:25:23.877","10","","<p>Yes, there is even a journal dedicated to this: <a href=""https://joss.theoj.org"" rel=""noreferrer"">The Journal of Open Source Software</a></p>\n\n<p>Here the software itself is peer reviewed and a decent documentation and tests are required. Since the submission and review process is based on GitHub, publishing there is straightforward if your code is on GitHub already.</p>\n\n<p>Note: I am not affiliated with this journal, I just published two software packages there a while ago.</p>\n","668","","","","2020-02-05T12:25:23.877","","","","0","",""
"11316","2","","11276","2020-02-05T19:58:07.297","1","","<p>This is similar to <a href=""https://www.biostars.org/p/303439/"" rel=""nofollow noreferrer"">this question</a>, which links to <a href=""https://samtools.github.io/bcftools/howtos/consensus-sequence.html"" rel=""nofollow noreferrer"">this solution</a>.</p>\n\n<p>The difference here is that you want to go through and replace low coverage (maybe zero-coverage?) positions with Ns rather than using the reference. You can likely accomplish this <a href=""https://www.htslib.org/doc/samtools-depth.html"" rel=""nofollow noreferrer"">using</a> <code>samtools depth</code> to measure coverage and then writing a script that will go through each position in your reference, check its coverage, and string-replace with an N. Probably a bash one-liner could do this if someone very devoted to bash one-liners came up with one (I don't care for them, but other people are more creative than I). Very quick and dirty pseudocode might look like this:</p>\n\n<pre><code>### Someone could come up with something better if they spent &gt;3min on it.\n### should be easy to get it working if you use biopython\n# read_and_preprocess() stands for whatever you need to do to get data in the right shape\ncoverage_data = read_and_preprocess('samtools_coverage.txt')\n# reference_with_variants.fasta comes from the solution linked above\nreference_fasta = read_and_preprocess('reference_with_variants.fasta')\n\nthreshold = 0\nfasta_string = ''\nfor position in reference_fasta:\n  if coverage_data[position] &lt;= threshold:\n    fasta_string += ""N""\n  else:\n    fasta_string += reference_fasta[position]\n\n# nonexistent fn that writes a fasta. \nwrite_as_fasta(fasta_string)\n</code></pre>\n\n<p>I am a little skeptical of this approach because it seems like <strong>if you do not trust the reference sequence to stand in for zero-coverage areas, then you probably shouldn't trust it to align to and call variants against</strong>. Read mapping can be a rather noisy process, especially when you are using a more distantly related reference. I think this is related to the @swbarnes2 comment.</p>\n\n<p>I would suggest instead indicating the regions of low/zero coverage by putting them in lowercase <code>acgt</code> with your covered regions uppercase <code>ACGT</code>, which preserves the coverage information that you care about but still allows you access to the reference information. It's not a perfect solution, but I think it makes the most out of the marginal data you're working with, and it is trivial to change the above example code to do it.</p>\n\n<p>Hope that helps-</p>\n","2108","","2108","2020-02-05T21:09:02.633","2020-02-05T21:09:02.633","","","","0","",""
"11317","1","","","2020-02-05T20:26:44.400","0","16","<p>I have alignment data in SAM files from ChIP-seq analysis for H3K27ac mark. I need to call peaks using MACS2. But I'm having issues with selecting parameters and values of the parameters. I will be really grateful if someone can help me figuring out this issue.\nThank you.\nMudith</p>\n","856","","","","2020-02-05T20:26:44.400","How to decide the parameters and parameter values when running ChIP-seq Peak Calling for H3K27ac data in MACS?","<genomics><chip-seq><peak-calling><macs2>","0","1","",""
"11318","2","","11225","2020-02-05T20:35:36.013","0","","<p>If your goal is only to account for degree distribution (i.e. Matteo Ferla's comment), a simulation approach you could use is:</p>\n\n<ol>\n<li>measure the degree (number of links) of each protein in your PPI</li>\n<li>find a large set of proteins that are equal or similar in degree to each of your proteins of interest (you may have to use a range of degrees to get a large enough set for comparison, ideally >=1000). We'll call these ""comparable proteins"" for each of your proteins of interest.</li>\n<li>For each protein of interest, make all of your measurements also for its set of comparable proteins</li>\n<li>Compare your measurement (shortest distance?) for the protein of interest to that for its comparable proteins. You could compute Z-scores from the parameters of the comparable protein distribution, or just empirical Monte Carlo p-values from where it falls in the distribution.</li>\n</ol>\n\n<p>This will give you at least a heuristic null distribution for each protein. It should be relatively easy to code up in whatever your favorite language is.</p>\n\n<p>Hope that helps-</p>\n","2108","","","","2020-02-05T20:35:36.013","","","","0","",""
"11319","1","","","2020-02-05T22:01:04.287","1","41","<p>I am very new to using python3 and am hoping to use it to reduce the time I need to spend running Infernal to look for a variety of RNAs in a large collection of bacterial genomes. I am hoping to write a short module in python that will let me sequentially run infernal using a single covariance model on a list of genome files. </p>\n\n<p>At the moment I am trying to use subprocess.run() or subprocess.Popen() to do so. </p>\n\n<pre class=""lang-py prettyprint-override""><code>\nimport subprocess\nimport os \nimport glob\n\ngenomes = list(glob.glob(os.path.join(""/path/to/genomes/"", ""*.fa"")))\n\nfor x in files:\n    subprocess.Popen(args=[""cmsearch"", ""/path/to/covariancemodel/"", ""genomes""], shell=False)\n</code></pre>\n\n<p>my issue is I do not know a way to have ""genomes"" passed as the components of the list within the command. </p>\n\n<p>Is this a way off approach to trying to automate this? If so is there a better way to do it or is there a way to simply pass each file in the list to the cmsearch command? </p>\n","7137","","","","2020-02-06T01:39:30.860","Writing a python module to run Infernal on a list of fasta files","<python><rna>","1","3","",""
"11320","2","","411","2020-02-06T00:21:28.930","0","","<p>NCBI is moving slowly to cloud (AWS/Google) for hosting. As such the tradiational FTP urls do not always work. My solution involves using <a href=""https://github.com/saketkc/pysradb"" rel=""nofollow noreferrer"">pysradb</a>.</p>\n\n<p>An entire project can be downloaded by:</p>\n\n<pre><code>pysradb download -p &lt;SRP_ID&gt;\n</code></pre>\n\n<p>A Google Colab notebook with metadata retrieval commands is <a href=""https://colab.research.google.com/drive/1C60V-jkcNZiaCra_V5iEyFs318jgVoUR"" rel=""nofollow noreferrer"">here</a>.</p>\n","161","","","","2020-02-06T00:21:28.930","","","","0","",""
"11321","2","","11319","2020-02-06T01:39:30.860","1","","<p>I think what you want to do is:</p>\n\n<pre><code>import subprocess\nimport os \nimport glob\n\ngenomes = glob.glob(os.path.join(""/path/to/genomes/"", ""*.fa""))\n\nfor gfile in genomes:\n    subprocess.Popen(args=[""cmsearch"", ""/path/to/covariancemodel/"", gfile], shell=False)\n</code></pre>\n\n<p>each pass through the for loop will take a one of the genome files and pass it into the cmsearch command. </p>\n\n<p>Note: </p>\n\n<p>1) glob.glob returns a list so no need to cast it again - <a href=""https://docs.python.org/2/library/glob.html"" rel=""nofollow noreferrer"">https://docs.python.org/2/library/glob.html</a></p>\n","964","","","","2020-02-06T01:39:30.860","","","","2","",""
"11322","2","","11206","2020-02-06T02:08:25.840","0","","<p>Removing the old id from 'Parent`:</p>\n\n<pre><code>for subfeature in feature.sub_features:\n    subfeature.qualifiers[""Parent""].pop()\n</code></pre>\n\n<p>Chromosome name:</p>\n\n<pre><code> for rec in GFF.parse(gff3):\n            chrID = rec.id \n</code></pre>\n","2477","","","","2020-02-06T02:08:25.840","","","","0","",""
"11323","2","","11299","2020-02-06T06:59:33.553","0","","<p>Leave-one-out cross validation is an iterative method that leaves out one sample until each sample has been left out once. You can then identify the intersection of the differentially expressed genes from the sample space of repeated analyses to produce a set of genes that are consistently differentially expressed independent of individual sample biases.\nmore info here:\n<a href=""http://efavdb.com/leave-one-out-cross-validation/"" rel=""nofollow noreferrer"">http://efavdb.com/leave-one-out-cross-validation/</a></p>\n","4507","","","","2020-02-06T06:59:33.553","","","","1","",""
"11326","1","","","2020-02-06T16:50:53.130","1","20","<p>I would like to address a question Regarding Transposable Elements, (mobile elements), your comment highly appreciated.</p>\n\n<p>I have been working to identify the transposable elements in an animal genome, ( same animal but 4 different species )</p>\n\n<p>my confusion is based on related articles the number of Non-LTR in eukaryotic Genome is very high compared to my results.</p>\n\n<p>identifications of LTR retrotransposons done first then, I used these results ( coordinates) to mask the genome to avoid conflicts or duplicate hits with (RT). then Extracted all the ORF from the masked genome, next I utilised the maskingORF genome via MGEScan Non-LTR, to identify NonLTR.</p>\n\n<p>MY approach is correct? </p>\n\n<p>Kindly check, the table below shows the results </p>\n\n<p><a href=""https://i.stack.imgur.com/kMCt3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kMCt3.png"" alt=""enter image description here""></a></p>\n\n<p>is this the final number to report?</p>\n","3332","","3332","2020-02-12T11:25:22.220","2020-02-12T11:25:22.220","Identification of SINEs, LINEs ( NonLTR) in eukaryotic genome","<repeatmasker><transposable-elements><repeat>","0","0","",""
"11327","1","","","2020-02-06T18:53:29.067","0","14","<p>I need to plot the 3-mer and 4-mer using the Chaos game representation for which I have referred the tutorial available at <a href=""https://towardsdatascience.com/chaos-game-representation-of-a-genetic-sequence-4681f1a67e14"" rel=""nofollow noreferrer"">https://towardsdatascience.com/chaos-game-representation-of-a-genetic-sequence-4681f1a67e14</a>. The code in this tutorial works on Python2.X and I am using Python3.X (and I do not wish to install Python2.X). I have made the necessary changes for it to work on Python3.X. But, It doesn't plot the required output as shown in the tutorial. Here is my code</p>\n\n<pre><code>import collections\nfrom collections import OrderedDict\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\nimport pylab\nimport math\nfrom past.builtins import xrange\n\nf = open(""D:/NC_012920.fasta"")\ns1 = f.read()\ndata = """".join(s1.split(""\n"")[1:])\n\ndef count_kmers(sequence, k):\n    d = collections.defaultdict(int)\n    for i in xrange(len(data)-(k-1)):\n        d[sequence[i:i+k]] +=1\n    for key in list(d):\n        if ""N"" in key:\n            del d[key]\n    return d\n\ndef probabilities(kmer_count, k):\n    probabilities = collections.defaultdict(float)\n    N = len(data)\n    for key, value in kmer_count.items():\n        probabilities[key] = float(value) // (N - k + 1)\n    return probabilities\n\ndef chaos_game_representation(probabilities, k):\n    array_size = int(math.sqrt(4**k))\n    chaos = []\n    for i in range(array_size):\n        chaos.append([0]*array_size)\n\n    maxx = array_size\n    maxy = array_size\n    posx = 1\n    posy = 1\n    for key, value in probabilities.items():\n        for char in key:\n            if char == ""T"":\n                posx += maxx // 2\n            elif char == ""C"":\n                posy += maxy // 2\n            elif char == ""G"":\n                posx += maxx // 2\n                posy += maxy // 2\n            maxx = maxx // 2\n            maxy //= 2\n        chaos[posy-1][posx-1] = value\n        maxx = array_size\n        maxy = array_size\n        posx = 1\n        posy = 1\n\n    return chaos\n\nf3 = count_kmers(data, 3)\nf4 = count_kmers(data, 4)\n\nf3_prob = probabilities(f3, 3)\nf4_prob = probabilities(f4, 4)\n\nchaos_k3 = chaos_game_representation(f3_prob, 3)\npylab.title('Chaos game representation for 3-mers')\npylab.imshow(chaos_k3, interpolation='nearest', cmap=cm.gray_r)\npylab.show()\n\nchaos_k4 = chaos_game_representation(f4_prob, 4)\npylab.title('Chaos game representation for 4-mers')\npylab.imshow(chaos_k4, interpolation='nearest', cmap=cm.gray_r)\npylab.show()\n</code></pre>\n\n<p>The output that I got looks like below:</p>\n\n<p><a href=""https://i.stack.imgur.com/iNbvl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iNbvl.png"" alt=""enter image description here""></a></p>\n\n<p>Can someone explain to me to why the output is blank and help me to get the output as shown in the tutorial? And also help me plot the output as below?</p>\n\n<p><a href=""https://i.stack.imgur.com/eQ8Pa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eQ8Pa.png"" alt=""enter image description here""></a></p>\n\n<p>Thanks.</p>\n","6794","","","","2020-02-06T18:53:29.067","Chaos game representation to calculate 3 and 4mer result in a blank plot","<python><k-mer>","0","0","",""
"11328","1","","","2020-02-06T20:59:47.763","0","26","<p>The question says it all. I want to calculate lipophilicity of proteins to screen the compounds out for antigen-antibody interactions.</p>\n","7150","","","","2020-02-12T00:35:39.963","How to calculate lipophilicity of proteins?","<protein-protein-interaction>","1","6","",""
"11329","1","11330","","2020-02-06T21:01:10.527","0","13","<p>I'm trying to run the following Rosetta command.</p>\n\n<pre><code>rosetta_scripts.linuxgccrelease -s data/binding/dataset/test/protein.pdb \ \n    -parser:protocol data/binding/test/scripts/pack.xml \\n    -out:path:all data/binding/test/output/ \\n    -resfile data/binding/test/output/resfile.txt \\n    -out:prefix RosettaOutput_ \\n    -nstruct 1 -ignore_zero_occupancy false -overwrite\n</code></pre>\n\n<p>However, I keep getting the error:</p>\n\n<pre><code>core.pack.task.operation.TaskOperations: {0} [ ERROR ] File:resfile.txt not found!\n\nERROR: Cannot open file resfile.txt\nERROR:: Exit from: /home/sean/src/rosetta_src_2019.35.60890_bundle/main/source/src/core/pack/task/operation/TaskOperations.cc line: 1645\nError: {0} [ ERROR ] ERROR: Exception caught by JobDistributor while trying to get pose from job 'RosettaOutput_2MWY_0001'\nError: {0} [ ERROR ] Treating failure as bad input; canceling similar jobs\n\n[ ERROR ]: Caught exception:\n\n\nFile: /home/sean/src/rosetta_src_2019.35.60890_bundle/main/source/src/core/pack/task/operation/TaskOperations.cc:1645\n[ ERROR ] UtilityExitException\nERROR: Cannot open file resfile.txt\n\n\n\n\nAN INTERNAL ERROR HAS OCCURED. PLEASE SEE THE CONTENTS OF ROSETTA_CRASH.log FOR DETAILS.\n\n\nprotocols.jd2.FileSystemJobDistributor: {0} job failed, reporting bad input; other jobs of same input will be canceled: RosettaOutput_2MWY_0001\nprotocols.jd2.JobDistributor: {0} no more batches to process... \nprotocols.jd2.JobDistributor: {0} 1 jobs considered, 1 jobs attempted in 13 seconds\n\n[ ERROR ]: Caught exception:\n\n\nFile: /home/sean/src/rosetta_src_2019.35.60890_bundle/main/source/src/protocols/jd2/JobDistributor.cc:327\n1 jobs failed; check output for error messages\n\n\nAN INTERNAL ERROR HAS OCCURED. PLEASE SEE THE CONTENTS OF ROSETTA_CRASH.log FOR DETAILS.\n</code></pre>\n\n<p>The error resolves itself if I move <code>resfile.txt</code> to the current working directory. Why can't <code>resfile.txt</code> be found?</p>\n","6966","","","","2020-02-06T21:01:10.527","How to specify input resfile for Rosetta?","<protein-structure>","1","1","",""
"11330","2","","11329","2020-02-06T21:01:10.527","2","","<p>The problem is my <code>pack.xml</code> specified a <code>resfile.txt</code> location which over-wrote my command-line option.</p>\n\n<pre><code>    &lt;TASKOPERATIONS&gt;\n       &lt;ReadResfile name=""Design_Resfile"" filename=""resfile.txt""/&gt;\n    &lt;/TASKOPERATIONS&gt;\n</code></pre>\n\n<p>If you remove the <code>filename</code> option in the XML file, you are then able to specify the path.</p>\n\n<pre><code>    &lt;TASKOPERATIONS&gt;\n       &lt;ReadResfile name=""Design_Resfile""/&gt;\n    &lt;/TASKOPERATIONS&gt;\n</code></pre>\n","6966","","","","2020-02-06T21:01:10.527","","","","0","",""
"11331","1","","","2020-02-06T21:22:43.683","1","21","<p>There is a place in my pipeline where I call PDB's API with a ribosomal id to get all the subchains of that molecule programmatically. </p>\n\n<p>I'd like to separate subchains that are proteins from nucleic acids in the response, or at least know how to get the chain-ids of the nucleic acids so that i can exclude them from all manually.  </p>\n\n<p><a href=""https://i.stack.imgur.com/NDLUe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NDLUe.png"" alt=""enter image description here""></a> </p>\n\n<p>Wondering if there is some query parameter like <em>is_nucleic_acid</em> in the PDB's ADL, but that's just a hunch. I'd be happy to learn if there is a more idiomatic way to do it( from Uniprot mappings maybe? ).</p>\n\n<p>My query as of now looks like when i want to get, say <em>uniprot acccessions</em> for subchains(in <strong>fl</strong>):</p>\n\n<pre><code>const PDBSearchSelect = ""https://ww.ebi.ac.uk/pdbe/search/select?"""";\nconst params = { \n// this is of the form ""3j9m"", ""5afi"", etc...   \nq: `${input.toLowerCase()}`,        \n//wondering if there is an ""fl"" parameter to get all nucleic acids\nfl: ""molecule_name, uniprot_id, uniprot_accession, struct_asym_id,...other"", \n//\nrows: ""200""\n};\n\naxios.get(PDBSearchSelect, {params}).... =&gt; ..\n</code></pre>\n\n<p>I would actually be super grateful if somebody could elucidate what's the difference between \n<code>https://www.ebi.ac.uk/pdbe/api/...</code> and </p>\n\n<p><code>https://www.ebi.ac.uk/pdbe/search/pdb/select?</code>. I've been looking at sides of the PDB API for a few weeks now and it still seems like complete babylon in terms of naming. I realize that it grew organically over years. </p>\n\n<p>Thanks a ton. </p>\n","6511","","","","2020-02-06T21:22:43.683","Get nucleic acids' chain names from PDB API","<pdb><uniprot><api>","0","6","",""
"11332","1","","","2020-02-06T22:17:36.747","0","9","<p>I have a set of several protein-ligand complexes in pdb format which I generated using Schrodinger software.</p>\n\n<p>I would like to rescore the whole set to obtain scoring results once again, given that I have deleted all the previous docking results and only have the pdb files.</p>\n\n<p>Is there any script in Schrodinger (or any other software) to input only pdb files and rescore them?</p>\n","7150","","","","2020-02-06T22:17:36.747","How to rescore docked protein-ligand poses?","<pdb><docking>","0","1","",""
"11334","1","","","2020-02-07T05:08:41.877","1","104","<p>When calculating how lethal a virus outbreak is, I've noticed that most sources use </p>\n\n<pre><code>CONFIRMED_DEATHS/CONFIRMED_INFECTIONS\n</code></pre>\n\n<p>However, given that there are only 2 possible outcomes; death or recovery, wouldn't it be more logical to use:</p>\n\n<pre><code>CONFIRMED_DEATHS/(CONFIRMED_DEATHS + CONFIRMED_RECOVERIES)\n</code></pre>\n\n<p>Which formula is applicable to which situations?</p>\n","7156","","3848","2020-02-08T06:58:47.830","2020-02-12T18:25:34.603","Calculating the mortality rate of a pandemic, e.g. coronavirus 2019-nCov?","<statistics><epidemiology>","2","0","",""
"11335","1","11340","","2020-02-07T11:10:43.583","0","38","<p>I have 25 tumor samples with counts data. Initially, I filtered out low expressed genes and then converted counts to <code>varianceStabilizingTransformation</code> using <code>deseq2</code> package. With this data I started using <code>WGCNA</code> for co-expression network analysis. </p>\n\n<p>For selecting the soft threshold I see very strange plot. R2 cutoff is 0.8 and I see that none of the scale free topology model fit is above that. </p>\n\n<p>Here is the code I used:</p>\n\n<p><code>df</code> is a dataframe with genes as rows and 25 samples as columns with counts data.</p>\n\n<pre><code>library(""DESeq2"")\nfiltered.counts &lt;- df[rowSums(df==0)&lt;5, ]\n\nU3 &lt;- as.matrix(filtered.counts)\nvsd &lt;- vst(U3, blind=FALSE)\n\noed &lt;- vsd\n\ngene.names=rownames(oed)\ntrans.oed=t(oed)\ndim(trans.oed)\n\nn=16462;\ndatExpr=trans.oed[,1:n]\ndim(datExpr)\n\nSubGeneNames=gene.names[1:n]\n\nlibrary(WGCNA)\noptions(stringsAsFactors = FALSE);\nallowWGCNAThreads()\n\npowers = c(c(1:10), seq(from = 12, to=20, by=2));\nsft=pickSoftThreshold(datExpr,dataIsExpr = TRUE,\n                      powerVector = powers,corFnc = cor,\n                      corOptions = list(use = 'p'),networkType = ""unsigned"")\n\n   Power SFT.R.sq  slope truncated.R.sq mean.k. median.k. max.k.\n1      1 0.000273  0.041          0.786 4140.00  4000.000 6730.0\n2      2 0.428000 -1.130          0.852 1570.00  1400.000 3770.0\n3      3 0.673000 -1.540          0.882  729.00   579.000 2410.0\n4      4 0.737000 -1.720          0.891  383.00   268.000 1670.0\n5      5 0.745000 -1.830          0.886  220.00   134.000 1210.0\n6      6 0.704000 -1.990          0.860  134.00    71.700  909.0\n7      7 0.737000 -1.980          0.890   85.90    40.300  701.0\n8      8 0.742000 -2.020          0.903   57.30    23.500  551.0\n9      9 0.733000 -2.090          0.917   39.40    14.200  441.0\n10    10 0.750000 -2.080          0.934   27.80     8.810  357.0\n11    12 0.770000 -2.080          0.952   14.80     3.670  243.0\n12    14 0.775000 -2.090          0.951    8.43     1.660  171.0\n13    16 0.397000 -2.820          0.459    5.06     0.801  124.0\n14    18 0.409000 -2.770          0.474    3.18     0.406   91.4\n15    20 0.421000 -2.720          0.490    2.06     0.215   68.8\n\n\n# Plot the results\nsizeGrWindow(9, 5)\npar(mfrow = c(1,2));\ncex1 = 0.9;\n\n# Scale-free topology fit index as a function of the soft-thresholding power\nplot(sft<span class=""math-container"">$fitIndices[,1], -sign(sft$</span>fitIndices[,3])*sft<span class=""math-container"">$fitIndices[,2],xlab=""Soft Threshold (power)"",ylab=""Scale Free Topology Model Fit, signed R^2"",type=""n"", main = paste(""Scale independence""));\ntext(sft$</span>fitIndices[,1], -sign(sft<span class=""math-container"">$fitIndices[,3])*sft$</span>fitIndices[,2],labels=powers,cex=cex1,col=""red"");\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/MJBpD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MJBpD.png"" alt=""enter image description here""></a></p>\n\n<p>Is this the right way? Which soft threshold power should I select? </p>\n\n<p>Any help is appreciated. thanq.</p>\n","1369","","","","2020-02-07T18:02:07.940","WGCNA: Problem with selecting soft threshold","<r><rna-seq><networks><rna><wgcna>","1","0","",""
"11336","1","","","2020-02-07T11:44:52.787","1","23","<p>My impression is that small InDel (a couple of bp) is identified through cigar string in BAM and typical CNV (at least thousands of bp) is detected through read depth.</p>\n\n<p>What about InDel or CNV with size these them, say hundreds of bp? These CNV is too long to be covered by read and too short to be detected by statistically comparing the difference of read depth.</p>\n\n<p>Do we have consensus that this kind of CNV is hard to detect? Or is there a common strategy of detection for them? Do we generally think them having impact on protein function (especially in oncology background) ?</p>\n","4905","","96","2020-02-07T16:07:40.437","2020-02-07T16:07:40.437","Detection of CNV(InDel) of intermediate size","<variant-calling><human-genome><cnv><indel>","1","1","",""
"11337","2","","11334","2020-02-07T15:45:34.467","3","","<p>This may not strike most as a bioinformatics, but getting the key clinical outcome is essential in understanding the molecular basis of pathogenicity.</p>\n\n<p>I think the mortality rate is over-reported. This is not to say the situation of 2019-nCov is not serious - it is very serious.</p>\n\n<p>The two essential factors missing in your equation for 2019-nCov are:</p>\n\n<ul>\n<li>Age. Mortality is likely skewed towards the old and very young</li>\n<li>Seroprevalence. To identify the number of asymptomatic patients.</li>\n</ul>\n\n<p>Old people have a weakened immune system (e.g. reduced T-cell response). The very young have an underdeveloped immune system. The problem is hospital cases are skewed towards those with severe infection, which is likely the old and very young. In 2019-nCov the first infecteds were middle aged men, but likely to be restricted to those exposed to the ""Wuhan meat market"".</p>\n\n<p><strong>Problem</strong> \nAsymptomatic ""spreaders"" are known in influenza and are suspected in 2019-nCov. Asymptomatics will inflate the mortality rate (because we're not detecting them).</p>\n\n<p><strong>Solution(s)</strong></p>\n\n<ol>\n<li>One solution is to conduct seroprevalence surveys. Serological surveys screening for antibodies (IgG) against 2019-nCov will identify asymptomatic patients, using the first formula you stated. However, would you seriously go around the general population in a 2019-nCov epidemic taking blood samples? Errhhh no.</li>\n<li>The <strong><em>bio-statistics</em></strong> approach is to use a <strong>catalytic model</strong> and critical to this is the age of the patient. Catalytic models are a priori models that incorporates the age-prevelance profile of infection. By introducing the incidence and age of hospitalisation of e.g. the old and the young, the rest of the distribution, i.e. asymptomatics can be calculated by regression. This estimates the total population of infecteds and again you use equation 1 to calculate the mortality rate recorded by the hospital.</li>\n<li>Mathematical models based on transmission dynamics. The problem here is the rate of transmission is unknown.</li>\n</ol>\n\n<p>I would assume the hospital is the place virus-induced mortality is reasonably accurate.</p>\n\n<p>The second formula you described works very well for Ebolavirus, no-one has ever heard of asymptomatic Ebolavirus. It would also work well for another betacoronavirus Middle Eastern respiratory syndrome. The key problem with 2019-nCov is we don't know the number of asymptomatic patients or patients with such mild infections they never report illness. </p>\n\n<p><strong>Summary</strong>\nIn summary, I think the mortality is being over-reported if it is based on your equation 2. Equation 1, however, will also over-report the mortality rate.</p>\n","3848","","3848","2020-02-07T16:28:58.377","2020-02-07T16:28:58.377","","","","0","",""
"11338","2","","11336","2020-02-07T16:07:19.757","1","","<blockquote>\n  <p><em>My impression is that small InDel (a couple of bp) is identified through cigar string in BAM...</em></p>\n</blockquote>\n\n<p>Well, yes, by gaps in the alignment, easily detected by examining the CIGAR string.</p>\n\n<blockquote>\n  <p><em>...and typical CNV (at least thousands of bp) is detected through read depth.</em></p>\n</blockquote>\n\n<p>Also by deviations from expected read pair distance or orientation.</p>\n\n<blockquote>\n  <p><em>What about InDel or CNV with size these them, say hundreds of bp? These CNV is too long to be covered by read and too short to be detected by statistically comparing the difference of read depth. Do we have consensus that this kind of CNV is hard to detect?</em></p>\n</blockquote>\n\n<p>Yes, what you describe has certainly been recognized and acknowledged in the literature. It was one of the primary motivations for <a href=""https://doi.org/10.1016/j.isci.2019.07.032"" rel=""nofollow noreferrer"">a recently published paper I contributed to</a> on <em>de novo</em> (germline) variant discovery. Here's an excerpt from that paper:</p>\n\n<blockquote>\n  <p><em>In a reference-mapping context, calling indels with confidence requires accurate mapping of each read spanning the indel, with all gaps arranged consistently. This is possible only for short indels and tends to be prone to error and misalignment. Thus prediction of indels with length 10 bp has proved to be very challenging and accompanied by high false-positive and false-negative rates. Furthermore, the prediction of SVs via read mapping is only possible through indirect signatures such as alterations in read depth or read-pair signatures. These signatures can be quite noisy and result in high rate of false-negative and false-positive prediction.</em></p>\n</blockquote>\n\n<p>As for your question:</p>\n\n<blockquote>\n  <p><em>is there a common strategy of detection for them?</em></p>\n</blockquote>\n\n<p>Numerous mapping-free k-mer based methods for variant detection are coming up in the literature, and all seem to perform pretty well. The trick is how to define which k-mers are interesting—then you grab those k-mers (or the reads containing them) and assemble them into variant-spanning contigs. A few examples:</p>\n\n<ul>\n<li>In the <a href=""https://doi.org/10.1016/j.isci.2019.07.032"" rel=""nofollow noreferrer"">Kevlar paper</a>, we defined  ""interesting"" k-mers as those with high abundance in an individual of interest but zero (or very low) abundance in the individual's parents.</li>\n<li>The <a href=""https://doi.org/10.7554/eLife.32920"" rel=""nofollow noreferrer"">HAWK paper</a>, which describes a k-mer based GWAS-like method, defines ""significant"" k-mers as those significantly over- or under-represented in one class of samples versus a second class. </li>\n<li>The <a href=""https://doi.org/10.1038/nmeth.4084"" rel=""nofollow noreferrer"">NovoBreak</a> paper compares paired tumor and normal WGS data to find k-mers that span mutatino breakpoints in cancer genomes.</li>\n</ul>\n\n<blockquote>\n  <p><em>Do we generally think them having impact on protein function (especially in oncology background)?</em></p>\n</blockquote>\n\n<p>There is definitely <em>potential</em> that indels can have a substantial impact on protein function. But since their detection has been so difficult for so long, a lot remains unknown about their <em>actual</em> collective impact on function.</p>\n","96","","","","2020-02-07T16:07:19.757","","","","0","",""
"11340","2","","11335","2020-02-07T18:02:07.940","1","","<p>Don't worry about it too much. I would go with power 8 based on my general experience (also reflected in WGCNA FAQ) and on the mean connectivity around 50 and median around 20, which seems reasonable to me.</p>\n","851","","","","2020-02-07T18:02:07.940","","","","1","",""
"11341","1","","","2020-02-07T18:45:13.550","0","12","<p><strong>Is there a way to programmatically access the <code>fastq-load.py</code> arguments from the SRR metadata?</strong> </p>\n\n<p>Specifically I would like to capture the exact command line arguments used by the uploader, the point being to recover information about the runs that is not systematically recorded during ingest (original file names, sample numbers, lane numbers, etc.).</p>\n\n<h2>Example</h2>\n\n<p>Here is a snapshot of SRA's Metadata tab for SRR10145265:</p>\n\n<p><a href=""https://i.stack.imgur.com/fje9a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fje9a.png"" alt=""enter image description here""></a></p>\n\n<p>Given this SRR ID, I would want to capture:</p>\n\n<pre><code>SRR          read1PairFiles             read2PairFiles             read3PairFiles     \nSRR10145265  10X_2_1_4_I1_001.fastq.gz  10X_2_1_4_R1_001.fastq.gz  10X_2_1_4_R2_001.fastq.gz\n</code></pre>\n\n<p>I'd accept solutions using any means, though I would prefer to avoid a scraping-based solution.</p>\n","900","","","","2020-02-07T18:45:13.550","Programmatic Access to `fastq-load.py` Arguments in SRR Metadata","<rna-seq><fastq><ncbi>","0","0","",""
"11342","2","","11291","2020-02-08T15:26:30.383","1","","<p>Proving a negative is hard, but to my knowledge I do not know a database that has <em>all</em> the structure of <em>drug</em> bound protein. I can give some close matches.</p>\n\n<p>First a few terms to be on the same page. A <strong>ligand</strong> is something that binds, which can be physiological, such as a <strong>cofactor</strong> or <strong>substrate</strong>, or not (especially man-made), such as a <strong>drug</strong>, i.e. a compound that has a physiological effect, or a <strong>hit</strong>, a small fragment from a fragment screen that will not bind with nanomolar affinity. Generally, ligands are small molecules, but may include oligopeptides and oligonucleotides. In the case of crystal structures, you also have <strong>crystallisation additives</strong>, which are not ligands, but are hetatoms in PDBs. Also, the definition of drug may vary. <a href=""https://www.drugbank.ca/"" rel=""nofollow noreferrer"">drugbank.ca</a> has the following categories of drugs ""approved"", ""experimental"", ""withdrawn"" for pharmaceuticals, ""illicit"" for narcotics and ""investigational"" for probes and counts macromolecules as drugs.</p>\n\n<ul>\n<li><p><a href=""https://www.bindingdb.org/bind/surflex_entry.jsp"" rel=""nofollow noreferrer"">bindingdb.org</a> contains many table, including a '3D' one. This table contains all the PDBs with ligands and their affinities. This contains crystallisation additives and natural ligands. Peter Curran's repo for the HotSpots algorithm (first written by Chris Radoux) has a <a href=""https://github.com/prcurran/hotspots/blob/master/hotspots/excluded_het_id.txt"" rel=""nofollow noreferrer"">blacklist</a>, but you are bound to get stuff missed.</p></li>\n<li><p><a href=""http://bindingmoad.org/"" rel=""nofollow noreferrer"">bindingmoad.org</a> is a database with all the protein-ligand structures from PDB up until 2018, but it includes natural ligands, like cofactors and substrates as above.</p></li>\n<li><p><a href=""https://klifs.vu-compmedchem.nl/index.php"" rel=""nofollow noreferrer"">https://klifs.vu-compmedchem.nl</a> a well curated database of kinases, which has a lot of members and many ligands, but is not at all diverse. As a result it is a clean dataset often used as a testbed for novel methodologies.</p></li>\n<li><p>I mentioned in the comments the <a href=""https://www.thesgc.org/tep"" rel=""nofollow noreferrer"">SGC Target Enabling Packages</a> as a small subset of proteins with many hits each. But I realise that there is not a handy PDB list and the targets are not drugs, but probes.</p></li>\n</ul>\n","6322","","","","2020-02-08T15:26:30.383","","","","0","",""
"11343","1","11344","","2020-02-08T15:44:32.220","0","30","<p>I am new to bioinformatics. While reading Momand's '<em>Concepts in Bioinformatics and Genomics</em>' , I understood clearly that each codon translates to a specific amino acid.</p>\n\n<p>But then, he talks about the <em>TP53</em> gene, and then without explanation talks about P53... Is that a naming convention? It's not explicitly explained... Does that applies for any case? ie <em>TP90</em> gene encodes protein P90?</p>\n\n<p>I don't understand the naming: correct me if wrong, but they named the gene after the protein's weight, which is 53 kilodaltons? \n(It confuses me with the location of the gene).</p>\n\n<p>Lastly I do not understand: if there is a mutation in the DNA, the mutation shall appear in only one cell I guess, how then the others cells' DNA start to mutate? or is it that all DNA in all cells are mutated instantly?</p>\n","6829","","","","2020-02-08T17:35:46.460","tp53 example in Momand's book","<gene>","1","0","",""
"11344","2","","11343","2020-02-08T17:35:46.460","3","","<p>This is possibly more of a molecular genetics question appropriate to biology.stackexchange.com, also note that it's usually better on stackexchange sites to <a href=""https://meta.stackexchange.com/help/how-to-ask"">restrict your post to a single question</a> and do some preliminary research via google or your search engine of choice. But I don't see any harm in answering it here nonetheless. </p>\n\n<p>The real answer is that naming conventions for genes/proteins are incredibly inconsistent and counterintuitive. You can't really draw any conclusions about a gene/protein based on its name alone. For example, some proteins such as p53 are named for biochemical features, others are named for phenotypes observed when their gene is mutated, such as for instance the gene <em>decapentaplegic</em> in fruit flies. Often they involve little jokes, such as the gene called <em>mothers against decapentaplegic</em> which represses the <em>decapentaplegic</em> mutant phenotype. So it's all a bit of a mess and people kind of assume that you'll follow along, which can be confusing.</p>\n\n<p>It's a little rarer that gene names differ substantially from the names of the proteins they encode, but not unheard-of. This is indeed the case with your <em>TP53</em>/p53 example. See a short explainer <a href=""https://ghr.nlm.nih.gov/gene/TP53"" rel=""nofollow noreferrer"">here</a> or a longer one <a href=""https://en.wikipedia.org/wiki/P53"" rel=""nofollow noreferrer"">here</a>. What you'll notice is that there are in fact multiple different names for the same gene/protein. What you'll often see is that the shorter name (p53) is the one that people tend to use colloquially.</p>\n\n<p>For your second question regarding how mutations are propagated, the short answer is that mutations are inherited from mother to daughter cells. Cells don't generally ""infect"" other cells with mutations (though in biology there are always exceptions). I think that what you are interested in is <a href=""https://www.google.com/search?q=somatic%20and%20germline%20mutations&amp;rlz=1C5CHFA_enUS695US695&amp;oq=somatic%20and%20germ&amp;aqs=chrome.0.0j69i57j0l6.3541j0j4&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow noreferrer"">the difference between ""somatic"" and ""germ-line"" mutation</a>.</p>\n\n<p>Hope that helps.</p>\n","2108","","","","2020-02-08T17:35:46.460","","","","2","",""
"11345","1","","","2020-02-08T20:33:52.533","2","66","<p>I want to convert a list of fasta ( protein sequences) in a .text file into corresponding nucleotide sequences. A Google search gives me result of DNA to protein conversion but not vice versa. Also, I came across How do I find the nucleotide sequence of a protein using Biopython?, but this is what I am not looking for. Is there any possible way to do it using python.Moreover, I would like to solve it using python programming. I am sure there must be some way to do it rather than writing a code from scratch. Thanks!</p>\n","6794","","","","2020-02-09T16:57:30.287","How to translate amino acid sequences to Nucleotide sequences","<python><proteins><biopython><dna><sequence>","2","1","",""
"11346","1","","","2020-02-09T04:31:03.800","0","12","<p>I am working with the <code>createMAE()</code> function of the ELMER Bioconductor package. While executing the <code>createMAE()</code> function using TCGA LIHC DNA Methylation 450K and RNASeq HTSeq - Counts data, I am getting this error:</p>\n\n<pre><code>Error in.rowNamesDF &lt;-(x, value = value) :\n\nduplicate 'row.names' are not allowed\n\nIn addition: Warning message:\n\nnon-unique values when setting 'row.names': 'TCGA-DD-AACA-02A', 'TCGA-DD-AACA-02B'\n</code></pre>\n\n<p>Can anyone tell me how to get rid of such error?</p>\n","4498","","3541","2020-02-09T06:35:52.713","2020-02-09T06:39:31.330","Error in createMAE function: non-unique values when setting 'row.names' in TCGA LIHC Data","<r><rna-seq><bioconductor><methylation>","1","0","",""
"11347","2","","11346","2020-02-09T06:39:31.330","0","","<p>The error message is pretty clear: R requires unique row names on data frames and some of the ""values"" that you are trying to set as row names are non-unique.</p>\n\n<p>You will need to provide a vector i) of length the same as your row number, ii) that is composed of non-unique values.</p>\n","3541","","","","2020-02-09T06:39:31.330","","","","0","",""
"11350","2","","11345","2020-02-09T09:57:14.263","1","","<p>You can't do this because there is redundancy in the genetic code and the the <em>same</em> protein sequences can be encoded by <em>different</em> nucleotide sequences. There are 64 codons and ~20 amino acids, e.g. <code>GCT</code>, <code>GCC</code>, <code>GCA</code> and <code>GCG</code> all encode Alanine.</p>\n","104","","","","2020-02-09T09:57:14.263","","","","2","",""
"11351","1","11375","","2020-02-09T10:05:54.883","1","37","<p>I am trying to obtain the sequence from PDB file, but no output is given when I run the following code :  </p>\n\n<pre><code>from Bio import SeqIO\n\nwith open(""1a9l.pdb"", ""r"") as handle:\n    for record in SeqIO.parse(handle, ""pdb-atom""):\n        print(record.seq)\n</code></pre>\n","6445","","4622","2020-02-10T16:20:42.637","2020-02-11T13:12:11.250","No output when trying to obtain protein sequence from PDB file","<python><biopython>","2","2","",""
"11352","2","","11345","2020-02-09T16:57:30.287","3","","<p>Although there is not a unique nucleotide sequence that translates to a given protein, one can list all the possible DNA sequences that do translate to that protein.</p>\n\n<p>An online tool that does just that is <a href=""https://www.ebi.ac.uk/Tools/st/emboss_backtranambig/"" rel=""nofollow noreferrer"">Backtranambig</a>, from EMBOSS. It produces a DNA sequence representing all the nucleotide sequences matching the input protein, using <a href=""https://droog.gs.washington.edu/parc/images/iupac.html"" rel=""nofollow noreferrer"">IUPAC ambiguity codes</a>.</p>\n","3389","","","","2020-02-09T16:57:30.287","","","","0","",""
"11353","1","11358","","2020-02-09T17:13:16.303","21","6486","<p>I'm looking at <a href=""https://www.ncbi.nlm.nih.gov/nuccore/MN988713.1?report=fasta"" rel=""noreferrer"">a genome sequence for 2019-nCoV on NCBI</a>. The FASTA sequence looks like this:</p>\n\n<pre><code>&gt;MN988713.1 Wuhan seafood market pneumonia virus isolate 2019-nCoV/USA-IL1/2020, complete genome\nATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGATCTGTTCTCTAAA\nCGAACTTTAAAATCTGTGTGGCTGTCACTCGGCTGCATGCTTAGTGCACTCACGCAGTATAATTAATAAC\nTAATTACTGTCGTTGACAGGACACGAGTAACTCGTCTATCTTCTGCAGGCTGCTTACGGTTTCGTCCGTG\n...  \n...\nTTAATCAGTGTGTAACATTAGGGAGGACTTGAAAGAGCCACCACATTTTCACCGAGGCCACGCGGAGTAC\nGATCGAGTGTACAGTGAACAATGCTAGGGAGAGCTGCCTATATGGAAGAGCCCTAATGTGTAAAATTAAT\nTTTAGTAGTGCTATCCCCATGTGATTTTAATAGCTTCTTAGGAGAATGACAAAAAAAAAAAA\n</code></pre>\n\n<p>Coronavirus is an RNA virus, so I was expecting the sequence to consist of <code>AUGC</code> characters. But the letters here are <code>ATGC</code>, which looks like DNA!</p>\n\n<p>I found a possible answer, that this is the sequence of a <a href=""https://en.wikipedia.org/wiki/Complementary_DNA"" rel=""noreferrer"">""complementary DNA""</a>. I read that</p>\n\n<blockquote>\n  <p>The term <em>cDNA</em> is also used, typically in a bioinformatics context, to refer to an mRNA transcript's sequence, expressed as DNA bases (GCAT) rather than RNA bases (GCAU).</p>\n</blockquote>\n\n<p>However, I don't believe this theory that I'm looking at a cDNA. If this were true, the end of the true mRNA sequence would be <code>...UCUUACUGUUUUUUUUUUUU</code>, or a ""poly(U)"" tail. But I believe the coronavirus has a <a href=""https://en.wikipedia.org/wiki/Polyadenylation"" rel=""noreferrer"">poly(A) tail</a>.</p>\n\n<p>I also found that the start of all highlighted genes begin with the sequence <code>ATG</code>. This is the DNA equivalent of <a href=""https://en.wikipedia.org/wiki/Start_codon"" rel=""noreferrer"">the RNA start codon <code>AUG</code></a>.</p>\n\n<p>So, I believe what I'm looking at is the true mRNA, in 5'→3' direction, but with all <code>U</code> converted to <code>T</code>.</p>\n\n<p><strong>So, is this really what I'm looking at? Is this some formatting/representation issue? Or does 2019-nCoV really contain DNA, rather than RNA?</strong></p>\n","6910","","6910","2020-02-09T18:32:04.697","2020-02-10T23:06:46.277","Why does the FASTA sequence for coronavirus look like DNA, not RNA?","<rna-seq><fasta><rna><dna>","4","2","","3"
"11354","1","","","2020-02-09T17:14:05.957","0","34","<p>I've got some doubts on the hisat2 --rna-strandness option and its output for downstream analysis. Please see below.</p>\n\n<p>I understand that the --rna-strandness option produces an XS tag to indicate where a transcript is from (on the + or - strand) for downstream transcriptome assembly analysis. I have a paired-end stranded sequencing library that was aligned to the genome using hisat2 without specifying the --rna-strandness (in other words, the default unstranded was the usage). Following this, the reads were assigned to genes using htseq-count and this time ""-s reverse"" was specified given the strand-specific sequencing assay type. </p>\n\n<p>Would the above handling affect the alignment and counting results given the default usage of --rna-strandness in hisat2 followed by htseq-count -s reverse on a strand-specific assay? Since --rna-strandness is for transcriptome assembly using the XS tags generated and htseq does not use XS tags for counting, I presume there should be no practical impact from the above. Could you also shed light on this? in case I may have been overlooking other facts of the usages of the tools.</p>\n\n<p>To help verify the above, I re-aligned and counted the reads from 2 samples by switching on --rna-strandness RF in hisat2. I attach the alignment and count features info. below for assessment.</p>\n\n<p>Overall alignment rate of Sample 1: 94.52% (--rna-strandness RF) vs.94.12% (--rna-strandness unstranded)\nOverall alignment rate of Sample 2: 94.57% (--rna-strandness RF) vs.94.15% (--rna-strandness unstranded)</p>\n\n<p>Feature counts of Sample 1 (following --rna-strandness RF + -s reverse):\n__no_feature        6327294\n__ambiguous     2954776\n__too_low_aQual     3784481\n__not_aligned       688856\n__alignment_not_unique      4858182</p>\n\n<p>Feature counts of Sample 1 (following --rna-strandness unstranded + -s reverse):\n__no_feature        6291151\n__ambiguous     2911298\n__too_low_aQual     4075017\n__not_aligned       754400\n__alignment_not_unique      16136045</p>\n\n<p>Feature counts of Sample 2 (following --rna-strandness RF + -s reverse):\n__no_feature        5417882\n__ambiguous     1708510\n__too_low_aQual     3532352\n__not_aligned       564596\n__alignment_not_unique      2859501</p>\n\n<p>Feature counts of Sample 1 (following --rna-strandness unstranded + -s reverse):\n__no_feature        5359434\n__ambiguous     1676091\n__too_low_aQual     3813344\n__not_aligned       623122\n__alignment_not_unique      2891792</p>\n\n<p>These results look comparable to me across pipelines.</p>\n\n<p>Thanks\nGuan</p>\n","7167","","","","2020-02-11T21:53:08.760","hisat2 --rna-strandness option and downstream htseq-count analysis","<rna-seq>","1","2","",""
"11356","1","11359","","2020-02-09T17:39:14.907","0","65","<p>I have a long format data frame like this</p>\n\n<pre><code>&gt; head(df)\n     gene \n\n1:  CCL28   2.030674       2.023333 1.921508    1.952007\n2:  CXCL2   2.204847       2.275296 2.175790    2.241554\n3:  CXCR4   2.121186       2.147181 1.950065    1.982639\n4: IFNGR1   2.311390       2.324111 2.220727    2.255556\n</code></pre>\n\n<p>And by this code </p>\n\n<pre><code>ggplot(df, aes(variable,value)) +\n  stat_boxplot(geom=""errorbar"", width=.5)+\n  geom_boxplot(aes(fill=variable))+\n  theme_bw()+\n  theme(axis.title.x=element_blank(), axis.title.y=element_blank())+\n  stat_summary(fun.y=median, colour=""red"", geom=""line"", aes(group = 1))+\n  geom_jitter(position = position_jitter(0.2))\n</code></pre>\n\n<p>I have this box</p>\n\n<p>I have added this line to my code where I got </p>\n\n<pre><code>geom_text(data = df_melt, aes(x = variable, y = value, label = gene), size = 5) \n</code></pre>\n\n<p>[![enter image description here][2]][2]</p>\n\n<p>I want to show the name of genes in box plot but I don't how, cam you help?</p>\n\n<p>Thanks</p>\n","4595","","4595","2020-02-10T10:47:13.820","2020-02-10T10:47:13.820","Showing name of genes in boxplot","<r><ggplot2>","1","2","","1"
"11357","2","","11353","2020-02-09T19:03:37.820","12","","<p>Most sequencing experiments, be it Illumina-based next-generation-sequencing or Sanger sequencing uses DNA as template, not RNA. Even if this virus is RNA-based it would be reverse-transcribed prior to any sequencing experiment. Therefore the output is DNA and this is what NCBI provides here.</p>\n","3051","","3051","2020-02-09T19:10:59.477","2020-02-09T19:10:59.477","","","","0","",""
"11358","2","","11353","2020-02-09T19:10:08.697","35","","<p>That is the correct sequence for 2019-nCov. Coronavirus is of course an RNA virus and in fact, to my knowledge, every RNA virus in Genbank is present as cDNA (AGCT, i.e. thydmine) and not RNA (AGCU, i.e. uracil).</p>\n\n<p>The reason is simple, we never sequence directly from RNA because RNA is too unstable and easily degraded by RNase. Instead the genome is reverse transcribed, either by targeted reverse transcription or random amplification and thus converted to cDNA. cDNA is stable and is essentially reverse transcribed RNA.</p>\n\n<p>The cDNA is either sequenced directly or further amplified by PCR and then sequenced. Hence the sequence we observe is the cDNA rather than RNA, thus we observe thymine rather than uracil and that is how it is reported. </p>\n","3848","","3848","2020-02-09T19:15:37.033","2020-02-09T19:15:37.033","","","","2","",""
"11359","2","","11356","2020-02-09T19:13:38.297","2","","<p>In addition, to have several errors in your code for plotting (for example, you are using <code>df</code> for plotting and then <code>df_melt</code> for <code>geom_text</code> but you didnot mention what is it). Based on your graph, it seems that you have issues with the melting part. </p>\n\n<p>However, these are quite secondary issues that are probably be fixed with the code below. with your example, there is one trick because you are using <code>jitter</code> to plot your points (that is not addressed by the link that @ATpoint provided in comments). This will randomly position the point at the right y value but with different x position in order to not overlap points. </p>\n\n<p>So, when you will add your text label, the labeling won't fit to the position of points. But you can make the jitter reproducible by adding a <code>seed</code> (see this github discussion: <a href=""https://github.com/tidyverse/ggplot2/pull/1996"" rel=""nofollow noreferrer"">https://github.com/tidyverse/ggplot2/pull/1996</a>). However, <code>geom_jitter</code> function does not have the argument <code>seed</code>, so you can use <code>geom_point</code> function and the argument <code>position_jitter</code> instead.</p>\n\n<p>So, using the few genes that you provided as an example. I have to reshape it (a step you probably did but you did not provide in your question), here I'm using <code>pivot_longer</code> function from <code>tidyr</code> package:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(dplyr)\nlibrary(tidyr)\nDF &lt;- df %&gt;% pivot_longer(., -gene, names_to = ""variable"", values_to = ""value"")\n\n# A tibble: 16 x 3\n   gene   variable       value\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n 1 CCL28  Res_immune      2.03\n 2 CCL28  Non_res_immune  2.02\n 3 CCL28  Res_OBP         1.92\n 4 CCL28  Non_res_OBP     1.95\n 5 CXCL2  Res_immune      2.20\n 6 CXCL2  Non_res_immune  2.28\n 7 CXCL2  Res_OBP         2.18\n 8 CXCL2  Non_res_OBP     2.24\n 9 CXCR4  Res_immune      2.12\n10 CXCR4  Non_res_immune  2.15\n11 CXCR4  Res_OBP         1.95\n12 CXCR4  Non_res_OBP     1.98\n13 IFNGR1 Res_immune      2.31\n14 IFNGR1 Non_res_immune  2.32\n15 IFNGR1 Res_OBP         2.22\n16 IFNGR1 Non_res_OBP     2.26\n</code></pre>\n\n<p>And then, for the plotting part, you can use <code>geom_text_repel</code> function of the <code>ggrepel</code> package to add labeling without overlapping your points:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(ggplot2)\nlibrary(ggrepel)\nggplot(DF, aes(x = variable, y = value, fill = variable, label = gene))+\n  geom_boxplot(alpha = 0.2)+\n  geom_point(position = position_jitter(width = 0.2, seed = 2), show.legend = FALSE)+\n  geom_text_repel(position = position_jitter(0.2, seed = 2), show.legend = FALSE)\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/KW09u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KW09u.png"" alt=""enter image description here""></a></p>\n\n<p><strong>Warning</strong></p>\n\n<p>If you have a long list of genes, it will be pointless to show all of them on the boxplot. It will makes things really illisible. If you want to show some particular genes of interest, make a subset of the original dataframe. A lot of posts on SO and here have addressed this subsetting.</p>\n\n<p><strong>Reproducible data example</strong></p>\n\n<pre class=""lang-r prettyprint-override""><code>structure(list(gene = c(""CCL28"", ""CXCL2"", ""CXCR4"", ""IFNGR1""), \n    Res_immune = c(2.030674, 2.204847, 2.121186, 2.31139), Non_res_immune = c(2.023333, \n    2.275296, 2.147181, 2.324111), Res_OBP = c(1.921508, 2.17579, \n    1.950065, 2.220727), Non_res_OBP = c(1.952007, 2.241554, \n    1.982639, 2.255556)), row.names = c(NA, -4L), class = c(""data.table"", \n""data.frame""), .internal.selfref = &lt;pointer: 0x563d91b65350&gt;)\n</code></pre>\n","6437","","6437","2020-02-09T19:37:56.163","2020-02-09T19:37:56.163","","","","3","",""
"11360","1","","","2020-02-09T22:54:19.853","-2","38","<p>I have two groups, for them I have adjusted p values for some KEGG pathways like this</p>\n\n<pre><code>Term    Adjusted P-value group 1    Adjusted P-value group 2\nPI3K-Akt signaling pathway  7.80E-08    1.78E-07\nPathways in cancer  1.26E-06    5.54E-06\nRap1 signaling pathway  6.43E-06    2.14E-04\nProteoglycans in cancer 6.81E-06    3.94E-04\nRegulation of actin cytoskeleton    7.84E-05    4.17E-04\nRas signaling pathway   9.54E-05    3.67E-04\nMAPK signaling pathway  9.89E-05    4.94E-04\nCytokine-cytokine receptor interaction  1.12E-04    4.63E-04\n</code></pre>\n\n<p>I want a stack plot showing log10 of adjusted pvalue for each pathway in each group something like this</p>\n\n<p><a href=""https://i.stack.imgur.com/26iWh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/26iWh.png"" alt=""enter image description here""></a></p>\n\n<p>Can you help?</p>\n\n<p>Thanks</p>\n","4595","","","","2020-02-09T22:54:19.853","Plotting such stack bar plot in r","<r>","0","3","",""
"11361","1","","","2020-02-10T12:18:38.520","0","9","<p>I am trying to do a de novo assembly using Trinity. Using Transdecoder and blastp/hmmscan search to do annotation (using Uniprot/pfam databases). <a href=""https://github.com/TransDecoder/TransDecoder/wiki"" rel=""nofollow noreferrer"">https://github.com/TransDecoder/TransDecoder/wiki</a> . But i would like to do a blastp/hmmscan search only for a specific taxonomy, viridiplantae to be more precise (taxid: 33090). I managed to find a Uniprot database for plants only, but my problem is how to find a pfam database specific for plants. Here : <a href=""http://pfam.xfam.org/search#tabview=tab1"" rel=""nofollow noreferrer"">http://pfam.xfam.org/search#tabview=tab1</a> there is the taxonomy tab, but when checking the box ""find unique.."" it returns an error. Thanks in advance</p>\n","7172","","","","2020-02-10T12:18:38.520","Pfam search using hmmer but only for a specific taxonomy","<blast>","0","0","",""
"11362","1","11368","","2020-02-10T13:46:52.977","4","53","<p>I have written a bioinformatics package in R that I want to publish in a bioinformatics Journal. Presently, I am maintaining a local repo of that package and I want to put in the Bioconductor repository (after publication or at least after submission to the journal), is it possible? or I must submit it to Bioconductor first?\nThanks</p>\n","7177","","","","2020-02-10T18:14:54.630","Can I submit a R package to Bioconductor or CRAN if I have already published it a journal?","<bioconductor>","1","0","",""
"11363","2","","11351","2020-02-10T14:03:09.313","3","","<p>That <a href=""http://www.rcsb.org/structure/1A9L"" rel=""nofollow noreferrer"">PDB entry</a> is an RNA molecule, so there is no protein sequence to extract.</p>\n\n<p>If it extracted the RNA sequence by default here you can see there could be an ambiguity as to whether it was an RNA sequence or a protein sequence, as the alphabets share characters.</p>\n","4622","","","","2020-02-10T14:03:09.313","","","","2","",""
"11364","2","","11297","2020-02-10T16:04:46.210","1","","<p><a href=""https://csb5.github.io/lofreq/"" rel=""nofollow noreferrer"">LoFreq</a> is a variant calling tool that specifically leverages per-base quality scores. </p>\n\n<p>For long read technologies (PacBio SMRT, ONT), I think quality score usage is mostly limited to qc steps as well, such as filtering out poor reads either by mean quality score of the whole read (as in ONT's basecalling softwares and <a href=""https://github.com/rrwick/Filtlong"" rel=""nofollow noreferrer"">Filtlong</a>) or by the lowest quality score within a window in a read (<a href=""https://github.com/rrwick/Filtlong"" rel=""nofollow noreferrer"">Filtlong</a>).  I think most long read mappers ignore these values (for example minimap2 and ngmlr do and can take fasta for read inputs).</p>\n","3908","","","","2020-02-10T16:04:46.210","","","","0","",""
"11365","2","","11353","2020-02-10T16:22:43.560","3","","<blockquote>\n  <p>If this were [cDNA], the end of the true mRNA sequence would be ...UCUUACUGUUUUUUUUUUUU, or a ""poly(U)"" tail.</p>\n</blockquote>\n\n<p>A cDNA sequence, maybe confusingly, refers to the <em><a href=""https://en.wikipedia.org/wiki/Coding_strand"" rel=""nofollow noreferrer"">coding strand</a></em> of the cDNA (despite being called “complementary”). So while cDNA is the result of reverse transcribing RNA into DNA, by convention it has the same strandedness as the original RNA. That’s why what you’re seeing is read in 5′→3′ direction and contains a visible poly(A) tail. Having a single conventional reading direction for all archived sequences vastly simplifies data handling, and reduces errors.</p>\n\n<p>In fact, since cDNA is double-stranded, there is no a priori reason why a computer-stored cDNA sequence should refer to the template strand (i.e. the opposite strand, which is synthesised from the RNA during reverse transcription).</p>\n\n<p><a href=""https://en.wikipedia.org/wiki/Complementary_DNA#Synthesis"" rel=""nofollow noreferrer"">The whole (simplified) synthesis process of cDNA is as follows</a>:</p>\n\n<ol>\n<li>A primer hybridises to the template RNA molecule.</li>\n<li>The RNA template is reverse transcribed into DNA using reverse transcriptase.</li>\n<li>The RNA template is removed.</li>\n<li>A complementary strand is transcribed along the (currently) single-stranded cDNA, resulting in a double-stranded cDNA product.</li>\n</ol>\n","29","","","","2020-02-10T16:22:43.560","","","","0","",""
"11366","1","11367","","2020-02-10T16:59:40.240","0","12","<p>FlyBase uses systematic naming of genes, transcripts and proteins in a format <code>FBtypeNUMBER</code>. However, the numbers are not the same for genes and their products. For example gene <code>FBgn0259101</code> transcribes to <code>FBtr0299513</code> and translates to <code>FBpp0288787</code>.</p>\n\n<p>On this <a href=""https://flybase.org/convert/id"" rel=""nofollow noreferrer"">webpage</a> users can conver individual IDs, but what If I need to convert practically whole transcriptome? Where can I get the full translation table?</p>\n","57","","","","2020-02-10T16:59:40.240","how to get FlyBase ID conversion table","<identifiers><conversion>","1","1","",""
"11367","2","","11366","2020-02-10T16:59:40.240","0","","<p>Indeed the web interface does not seem to link the full translation table, but it is available on their <a href=""ftp://ftp.flybase.net"" rel=""nofollow noreferrer"">ftp server</a> at <code>releases -&gt; release_ID -&gt; precomputed_files -&gt; genes -&gt; fbgn_fbtr_fbpp_fb_2019_06.tsv.gz</code></p>\n\n<p>For example, <a href=""ftp://ftp.flybase.net/releases/FB2019_06/precomputed_files/genes/fbgn_fbtr_fbpp_fb_2019_06.tsv.gz"" rel=""nofollow noreferrer"">this</a> is the latest (<code>FB2019_06</code>) translation table available. </p>\n","57","","","","2020-02-10T16:59:40.240","","","","1","",""
"11368","2","","11362","2020-02-10T17:10:42.920","5","","<p>You can submit a package to a repo after the publication, but I would say that you really should do it before.</p>\n\n<p><a href=""https://www.bioconductor.org/developers/package-submission/"" rel=""noreferrer"">Bioconductor</a> accept only packages that are not published on CRAN, however, academic publications are fine.</p>\n\n<p>Conversely, Bioinformatics does not enforce any particular platform for sharing the code, but you have to make it available somehow (e.g. GitHub repo, university webpage, bioconductor). From the <a href=""https://academic.oup.com/bioinformatics/pages/instructions_for_authors"" rel=""noreferrer"">instructions for authors</a></p>\n\n<blockquote>\n  <p>If the manuscript describes new software tools or the implementation\n  of novel algorithms the software must be freely available to\n  non-commercial users at the time of submission, and appropriate test\n  data should be made available.</p>\n</blockquote>\n\n<p>However, you as a developer should make all the effort to make your software available and easily accessible. Also, sooner you make it available, sooner people start to cite you, which is a great advantage for you. Finally, you really need users to make sure that your package really works (i.e. debugging all the corner cases).</p>\n\n<p>I could not emphasise more, the correct order is 1. make it available 2. write a publication</p>\n","57","","57","2020-02-10T18:14:54.630","2020-02-10T18:14:54.630","","","","2","",""
"11369","2","","11353","2020-02-10T19:05:23.027","1","","<p>It's not common to sequence directly from RNA because most sequencing platforms don't have that as an option. Nanopore sequencers do allow this, but I'm not aware yet of any 2019-nCov preprints involving nanopore RNA sequencing. I expect that will change in the next month or so.</p>\n\n<p>Commercial kits exist; there are no insurmountable technical issues with it. Direct RNA sequencing can be done locally, on-site near the point of discovery without sample transfer or culture on a USB-powered device that fits in a pocket (RNA preparation takes <a href=""https://store.nanoporetech.com/catalog/product/view/id/297/s/direct-rna-sequencing-kit/category/28/"" rel=""nofollow noreferrer"">about 2 hours</a>). Flow cells that have potentially-infectious RNA inside them can be disposed as biohazard waste. However, the ease at which RNA can be quickly converted to more stable cDNA then amplified to create a much higher concentration DNA sample (which is quicker / more efficient to get results from) means that cDNA is generally preferred for sequencing unless the native RNA is needed (e.g. for looking at RNA base modifications that are destroyed when converting to cDNA).</p>\n\n<p>There is a paper on coronavirus direct RNA sequencing with nanopore <a href=""https://doi.org/10.1101/483693"" rel=""nofollow noreferrer"">here</a>; I would expect that 2019-nCoV would have a similar difficulty. The zika virus has an extremely low viral load in human blood, but has also been sequenced via direct RNA sequencing of [carefully] cultured cells (see <a href=""https://doi.org/10.4167/jbv.2019.49.3.115"" rel=""nofollow noreferrer"">here</a>).</p>\n\n<p>Regardless of whether or not RNA sequencing has actually been carried out, most genetic data analysis programs will only work with A/C/G/T sequences, so it's conventional to replace any U parts of an RNA sequence with T for data storage. There's no loss of information by doing this, as T replaces all Us in the RNA sequence.</p>\n","73","","73","2020-02-10T23:06:46.277","2020-02-10T23:06:46.277","","","","1","",""
"11370","1","11372","","2020-02-10T21:52:19.800","0","35","<p>I have this data frame</p>\n\n<blockquote>\n  <p>head(d)</p>\n</blockquote>\n\n<pre><code># A tibble: 6 x 3\n  gene  variable       value\n1 CCT5  Res_immune     10.6 \n2 CCT5  Non_res_immune 10.5 \n3 CDK1  Res_immune      9.25\n4 CXCL5 Non_res_immune  9.82\n5 GBP1  Res_immune      9.04\n6 GBP1  Non_res_immune  9.43\n&gt; \n</code></pre>\n\n<p>And this code</p>\n\n<pre><code>library(ggplot2)\nlibrary(ggrepel)\nggplot(d, aes(x = variable, y = value, fill = variable, label = gene))+\n  geom_boxplot(alpha = 0.2)+\n  geom_point(position = position_jitter(width = 0.2, seed = 2), show.legend = FALSE)+\n  geom_text_repel(position = position_jitter(0.2, seed = 2), show.legend = FALSE)\n</code></pre>\n\n<p>The genes in first column are involved in cytokine signaling pathway in two groups of patients</p>\n\n<p>This is my plot\n<a href=""https://i.stack.imgur.com/2ciYI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ciYI.png"" alt=""enter image description here""></a></p>\n\n<p>I have another data frame for genes in IL17 signaling in the sample patients like</p>\n\n<p>head(d)</p>\n\n<pre><code># A tibble: 6 x 3\n  gene  variable       value\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 CDK6  Res_immune     17.6 \n2 IL6  Non_res_immune 14.5 \n3 CXCL8  Res_immune      1.25\n</code></pre>\n\n<p>How I can put plots from both data in a grade like below where the name of pathways shown instead of DEL, INS, SNP and total like below picture?</p>\n\n<p><img src=""https://i.stack.imgur.com/aRH0K.png"" alt=""""></p>\n","4595","","6437","2020-02-11T06:34:52.317","2020-02-11T06:34:52.317","Griding ggplot object in r","<r><ggplot2>","1","1","","1"
"11371","1","","","2020-02-10T21:57:50.637","0","20","<p>I am having a tough time calling LiftoverVcf in GATK. I have concatenated all of the Vcfs of interest and removed rows with problematic values but now when I run LiftoverVCF I get the following error:</p>\n\n<pre><code>Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.0.12.0\nINFO 2020-02-10 14:36:03 LiftoverVcf Loading up the target reference genome.\nINFO 2020-02-10 14:36:16 LiftoverVcf Lifting variants over and sorting (not yet writing the output file.)\n\n[Mon Feb 10 14:59:31 EST 2020] picard.vcf.LiftoverVcf done. Elapsed time: 23.50 minutes.\nRuntime.totalMemory()=7414480896\nTo get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp\njava.lang.IllegalStateException: Key . found in VariantContext field INFO at 1:121387974 but this key isn't defined in the VCFHeader. We require all VCFs to have complete VCF headers by default.\nat htsjdk.variant.vcf.VCFEncoder.fieldIsMissingFromHeaderError(VCFEncoder.java:202)\nat htsjdk.variant.vcf.VCFEncoder.write(VCFEncoder.java:141)\nat htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:248)\nat picard.vcf.LiftoverVcf.rejectVariant(LiftoverVcf.java:456)\nat picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:357)\nat picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295)\nat org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25)\nat org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)\nat org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)\nat org.broadinstitute.hellbender.Main.main(Main.java:289)\n\n</code></pre>\n\n<p>Not sure why I am getting this error other than there may be a <code>.</code> in one of the info fields. Here is what the first several rows of my VCF looks like minus the header fields:</p>\n\n<pre><code>#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT AltaiNea\n1 10001 . T . 51.05 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=6.32;MQ0=242 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:21.05:0,21,265:1,2:0,1:1,0:116,129:2\n1 10002 . A . 36.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=3.31;MQ0=248 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:6.01:0,6,69:171,184:0,9:0,0:0,2:0\n1 10003 . A . 36.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=3.31;MQ0=248 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:6.02:0,6,72:222,201:1,0:0,0:0,8:0\n1 10004 . C . 39.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=3.61;MQ0=247 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:9.02:0,9,97:0,1:268,211:0,0:0,1:0\n1 10005 . C . 41.99 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=4.31;MQ0=246 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:12:0,12,119:0,0:341,287:0,0:0,0:0\n1 10006 . C . 36.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=3.31;MQ0=248 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:6.02:0,6,72:0,1:408,347:0,0:2,0:0\n1 10007 . T . 39.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=4.21;MQ0=246 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:9.02:0,9,97:0,0:0,0:0,0:506,451:0\n1 10008 . A . 33.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=4.05;MQ0=247 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:3.01:0,3,40:547,507:0,0:0,0:0,0:0\n1 10009 . A . 36.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=3.31;MQ0=248 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:6.02:0,6,70:600,534:0,1:0,0:0,1:0\n1 10010 . C . 42.01 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=4.72;MQ0=244 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:12.02:0,12,125:0,0:649,547:0,1:1,0:0\n1 10011 . C . 51.02 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=5.12;MQ0=242 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:21.03:0,21,209:0,2:732,612:0,1:0,0:0\n1 10012 . C . 44.99 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=3.94;MQ0=243 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:14.99:0,15,141:0,3:804,657:0,0:6,1:0\n1 10013 . T . 57.04 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=7.20;MQ0=238 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:27.04:0,27,285:0,0:1,0:0,0:904,760:0\n1 10015 . A . 60.04 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=7.19;MQ0=238 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:30.04:0,30,301:985,850:0,0:1,0:0,0:0\n1 10016 . C . 72.03 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=7.99;MQ0=235 GT:DP:GQ:PL:A:C:G:T:IR 0/0:249:42.04:0,42,427:2,1:1024,852:0,0:1,0:0\n1 10017 . C . 84.05 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=8.37;MQ0=232 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:54.06:0,54,534:0,2:1100,912:0,0:0,0:0\n1 10018 . C . 108.02 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=8.87;MQ0=224 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:78.03:0,78,757:0,0:1167,978:0,1:2,0:0\n1 10019 . T . 96.05 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=10.24;MQ0=219 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:66.05:0,66,646:1,0:1,0:1,0:1258,1090:0\n1 10020 . A . 65.97 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=9.74;MQ0=221 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:35.97:0,36,331:1309,1150:0,0:1,1:0,0:0\n1 10021 . A . 65.34 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=8.14;MQ0=228 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:35.35:0,35,489:1339,1179:3,0:0,0:1,0:0\n1 10022 . C . 92.98 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=7.79;MQ0=229 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:62.99:0,63,590:0,1:1389,1188:0,1:0,0:0\n1 10023 . C . 98.93 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=7.69;MQ0=227 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:68.94:0,69,636:2,2:1453,1242:0,0:1,2:0\n1 10024 . C . 98.95 . AC=0;AF=0.00;AN=2;DP=250;DS;MQ=7.46;MQ0=227 GT:DP:GQ:PL:A:C:G:T:IR 0/0:250:68.96:0,69,620:1,3:1497,1302:1,0:3,2:0\n</code></pre>\n\n<p>Is there a way for me to add <code>.</code> into the VCF header as a tag for all of the appropriate fields so that I can bypass this problem? I know from reading online that the <code>.</code> means that value is unknown. I would like to figure out how to instruct GATK to simply accept it regardless of the fact that it cannot be found in the VCF header.</p>\n\n<p>Also, I am open to other ways I can liftover this genome to hg38. Below is the command I am using to do so using GATK.</p>\n\n<p><code>java -jar ~/work/progs/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar LiftoverVcf -I AltaiNea.hg19_1000g.concat.LowQualRemoved.vcf -O AltaiNea.hg38_1000g.concat.LowQualRemoved.vcf -C hg19ToHg38.over.chain -R hg38.fa --REJECT rejectedRecords.vcf</code></p>\n","3442","","","","2020-02-11T20:59:47.507","VCF's unknown ('.') values causing problems for GATK's Liftover Vcf","<vcf><vcftools>","1","1","",""
"11372","2","","11370","2020-02-10T22:20:47.523","4","","<p>What you are looking for is <code>facetting</code>. Using this key word on any search engines you will find dozen of answers describing its use: <a href=""https://ggplot2.tidyverse.org/reference/facet_grid.html"" rel=""nofollow noreferrer"">https://ggplot2.tidyverse.org/reference/facet_grid.html</a> and <a href=""https://ggplot2.tidyverse.org/reference/facet_wrap.html"" rel=""nofollow noreferrer"">https://ggplot2.tidyverse.org/reference/facet_wrap.html</a></p>\n\n<p>To prepare the faceting in <code>ggplot2</code>, a possible way is to bind both dataframes together by first specifying which group they belong such as:</p>\n\n<pre class=""lang-r prettyprint-override""><code>cytokine<span class=""math-container"">$Group = ""Cytokine""\nil17$</span>Group = ""IL17""\nDF &lt;- rbind(cytokine,il17)\nDF\n\n    gene       variable value    Group\n1:  CCT5     Res_immune 10.60 Cytokine\n2:  CCT5 Non_res_immune 10.50 Cytokine\n3:  CDK1     Res_immune  9.25 Cytokine\n4: CXCL5 Non_res_immune  9.82 Cytokine\n5:  GBP1     Res_immune  9.04 Cytokine\n6:  GBP1 Non_res_immune  9.43 Cytokine\n7:  CDK6     Res_immune 17.60     IL17\n8:   IL6 Non_res_immune 14.50     IL17\n9: CXCL8     Res_immune  1.25     IL17\n</code></pre>\n\n<p>Then, use <code>facet_grid</code> function of <code>ggplot2</code>:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(ggplot2)\nlibrary(ggrepel)\nggplot(DF, aes(x = variable, y = value, fill = variable, label = gene))+\n  geom_boxplot(alpha = 0.2)+\n  geom_point(position = position_jitter(width = 0.2, seed = 2), show.legend = FALSE)+\n  geom_text_repel(position = position_jitter(0.2, seed = 2), show.legend = FALSE)+\n  facet_grid(Group~., scales = ""free"")\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/P9c3f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P9c3f.png"" alt=""enter image description here""></a></p>\n\n<p>Alternatively, you can use <code>facet_grid</code> (or <code>facet_wrap</code>) to have your plot in a single row:</p>\n\n<pre class=""lang-r prettyprint-override""><code>library(ggplot2)\nlibrary(ggrepel)\nggplot(DF, aes(x = variable, y = value, fill = variable, label = gene))+\n  geom_boxplot(alpha = 0.2)+\n  geom_point(position = position_jitter(width = 0.2, seed = 2), show.legend = FALSE)+\n  geom_text_repel(position = position_jitter(0.2, seed = 2), show.legend = FALSE)+\n  facet_wrap(.~Group, scales = ""free"")\n</code></pre>\n\n<p><a href=""https://i.stack.imgur.com/AohiU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AohiU.png"" alt=""enter image description here""></a></p>\n\n<p><strong>Reproducible data example</strong></p>\n\n<pre class=""lang-r prettyprint-override""><code>structure(list(gene = c(""CCT5"", ""CCT5"", ""CDK1"", ""CXCL5"", ""GBP1"", \n""GBP1""), variable = c(""Res_immune"", ""Non_res_immune"", ""Res_immune"", \n""Non_res_immune"", ""Res_immune"", ""Non_res_immune""), value = c(10.6, \n10.5, 9.25, 9.82, 9.04, 9.43), Group = c(""Cytokine"", ""Cytokine"", \n""Cytokine"", ""Cytokine"", ""Cytokine"", ""Cytokine"")), row.names = c(NA, \n-6L), class = c(""data.table"", ""data.frame""), .internal.selfref = &lt;pointer: 0x55bfc0db9350&gt;)\n</code></pre>\n\n<p>and</p>\n\n<pre class=""lang-r prettyprint-override""><code>structure(list(gene = c(""CDK6"", ""IL6"", ""CXCL8""), variable = c(""Res_immune"", \n""Non_res_immune"", ""Res_immune""), value = c(17.6, 14.5, 1.25), \n    Group = c(""IL17"", ""IL17"", ""IL17"")), row.names = c(NA, -3L\n), class = c(""data.table"", ""data.frame""), .internal.selfref = &lt;pointer: 0x55bfc0db9350&gt;)\n</code></pre>\n","6437","","6437","2020-02-10T22:28:07.117","2020-02-10T22:28:07.117","","","","1","",""
"11373","1","11385","","2020-02-11T09:57:11.437","1","38","<p><em>Note: this question can also be found on <a href=""https://www.biostars.org/p/421485/"" rel=""nofollow noreferrer"">Biostars</a></em></p>\n\n<p>I need to perform a stress test in a GWAS tool and the duty demands a dataset (plink format) having 100 thousand samples, having 40 million SNPs in a proportion of case:control=30:70. I'm performing the command:</p>\n\n<blockquote>\n  <p>plink1.9 --simulate ds1.sim --make-bed --out ds1 --simulate-ncases 30000 --simulate-ncontrols 70000</p>\n</blockquote>\n\n<p>But after about 3 hours execution, I'm facing <code>Error: File write failure.</code>, even though my environment has 256T of disk. (It's a container with a S3 (Object store) mounted through s3fs).</p>\n\n<p>I was wondering if there's a clever way to do that? Perhaps breaking it into small chunks and merging up later somehow? It would prevent me to be waiting hours till an error happens and I loose either time and the sample generated so far.</p>\n\n<p>Thanks in advance!</p>\n","6464","","73","2020-02-12T10:13:00.927","2020-02-12T10:57:33.907","Question: How to simulate 100k samples having 40 million SNPs in a proportion of case:control=30:70?","<gwas><plink>","1","0","",""
"11374","1","","","2020-02-11T11:50:28.810","-3","58","<p>I have CPM values for response to a drug in two RNA-seq set up. I have grouped the patients to responders and non responders to this drug and I have done pathway analysis.</p>\n\n<p>This is my data</p>\n\n<pre><code>&gt; dput(head(data))\nstructure(list(gene = c(""CCL22"", ""CCL22"", ""CCL22"", ""CCL22"", ""CCL27"", \n""CCL27""), variable = c(""Res_immune"", ""Non_res_immune"", ""Res_OBP"", \n""Non_res_OBP"", ""Res_immune"", ""Non_res_immune""), value = c(6.64466666666667, \n6.62, 6.2688, 6.34074074074074, 2.076, 0.467727272727273), Pathway = c(""Cytokine-cytokine receptor interaction"", \n""Cytokine-cytokine receptor interaction"", ""Cytokine-cytokine receptor interaction"", \n""Cytokine-cytokine receptor interaction"", ""Cytokine-cytokine receptor interaction"", \n""Cytokine-cytokine receptor interaction"")), row.names = c(NA, \n-6L), class = c(""tbl_df"", ""tbl"", ""data.frame""))\n&gt; \n\n&gt; length(unique(data<span class=""math-container"">$gene))\n[1] 44\n&gt; unique(dd$</span>Pathway)\n[1] ""Cytokine-cytokine receptor interaction"" ""IL-17 signaling""                        ""NF-kappa B signaling""                  \n[4] ""TNF signaling ""                         ""PI3K-Akt signaling ""  \n\n&gt; unique(dd$variable)\n[1] ""Res_immune""     ""Non_res_immune"" ""Res_OBP""        ""Non_res_OBP""   \n&gt;    \n\n\n\n&gt; dim(dd)\n[1] 392   4\n&gt; \n\n\n&gt; head(data)\n# A tibble: 6 x 4\n  gene  variable       value Pathway                               \n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                                 \n1 CCL22 Res_immune     6.64  Cytokine-cytokine receptor interaction\n2 CCL22 Non_res_immune 6.62  Cytokine-cytokine receptor interaction\n3 CCL22 Res_OBP        6.27  Cytokine-cytokine receptor interaction\n4 CCL22 Non_res_OBP    6.34  Cytokine-cytokine receptor interaction\n5 CCL27 Res_immune     2.08  Cytokine-cytokine receptor interaction\n6 CCL27 Non_res_immune 0.468 Cytokine-cytokine receptor interaction\n&gt; tail(data)\n# A tibble: 6 x 4\n  gene  variable       value Pathway              \n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                \n1 CD19  Res_OBP         8.89 ""PI3K-Akt signaling ""\n2 CD19  Non_res_OBP     8.90 ""PI3K-Akt signaling ""\n3 CDK6  Res_immune      8.86 ""PI3K-Akt signaling ""\n4 CDK6  Non_res_immune  8.17 ""PI3K-Akt signaling ""\n5 CDK6  Res_OBP         9.31 ""PI3K-Akt signaling ""\n6 CDK6  Non_res_OBP     9.38 ""PI3K-Akt signaling ""\n&gt; \n</code></pre>\n\n<p>I have gotten this </p>\n\n<p><a href=""https://i.stack.imgur.com/EQhKb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EQhKb.png"" alt=""enter image description here""></a></p>\n\n<p>But this is not informative</p>\n\n<p>How I can have a heatmap in which pathway are in rows and groups in column like</p>\n\n<p><a href=""https://i.stack.imgur.com/3IAZ5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3IAZ5.png"" alt=""enter image description here""></a></p>\n","4595","","","","2020-02-11T11:50:28.810","Converting this data frame to a heatmap","<r>","0","5","",""
"11375","2","","11351","2020-02-11T13:12:11.250","2","","<p>If you are using a conda version of python you can use Pymol module for this, which is actually more robust in some aspects than <code>bio.PDB</code>, although errors may cause segmentation faults. The following:</p>\n\n<pre><code>import pymol2\n## paralellisable version.\nwith pymol2.PyMOL() as pymol:\n    pymol.cmd.fetch('1a9l')\n    print(pymol.cmd.get_fastastr('polymer.nucleic'))\n</code></pre>\n\n<p>Gives me:</p>\n\n<pre><code>&gt;1a9l_A\nGGGUGACUCCAGAGGUCGAGAGACCGGAGAUAUCACCC\n</code></pre>\n\n<p>Alternatively, you can use PDBe's API.</p>\n\n<pre><code>import requests\n\nresponse = requests.get('https://www.ebi.ac.uk/pdbe/api/pdb/entry/molecules/1a9l').json()['1a9l']\n\nfor entity in response:\n    if entity['molecule_type'] == 'polyribonucleotide':\n        print(entity['sequence'])\n</code></pre>\n\n<p>Do note that missing 'residues' (residues, small molecules and nucleotides) will cause issues. PDB files do not have missing 'residues', while mmCIF files have a dictionary that does. The API will give you the latter.</p>\n","6322","","","","2020-02-11T13:12:11.250","","","","9","",""
"11376","1","","","2020-02-11T14:15:13.523","1","30","<p>What is the highest data compression ratio achieved on a population of genomes measured to include the compressing algorithm itself?  The size of the compressed database approximates its <a href=""http://www.scholarpedia.org/article/Algorithmic_information_theory"" rel=""nofollow noreferrer"">algorithmic information</a> content*.</p>\n\n<p>I've seen <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1021.1196&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">a claim of on the order of 1000:1 compression</a> but it appears dependent on an external ""reference"" database.  To be a principled measure, any reference data must be included in the size of the compressing algorithm.</p>\n\n<p>*The Universal Turing Machine notion of Minimum Description Length is also called the Kolmogorov Complexity or Algorithmic Information measure of the data.  I'm using the term ""Algorithmic Information"" as the most concise and accessible descriptive phrase for this measure.  </p>\n\n<p>The relevance of this measure is that according to the closely related theory of <a href=""http://www.scholarpedia.org/article/Algorithmic_probability"" rel=""nofollow noreferrer"">algorithmic probability</a>, the algorithm that maximally compresses a dataset also provides the most optimal data-driven predictions.</p>\n","7194","","","","2020-02-11T14:15:13.523","Algorithmic Information Measurement of Human Genome Databases?","<phylogenetics><clustering><population-genetics><data-management><correlation>","0","0","",""
"11377","2","","4977","2020-02-11T15:45:13.437","0","","<p>While I agree with not submitting snakemake to the cluster, it's not a great solution to make your pilot program run interactively - automation via batch script should not be dependent on an open shell. </p>\n\n<p>How about allocating a whole node with many cores and working within the GNU parallel abilities of snakemake.</p>\n","7196","","","","2020-02-11T15:45:13.437","","","","0","",""
"11378","1","11379","","2020-02-11T17:53:25.040","2","33","<p>I tried to find a homolog that has a PDB structure available so I can use this PDB file for comparative modeling. I ran a BLAST search but none of the search results seemed to have a known structure. Is there any way to find the results I want? </p>\n","7197","","4622","2020-02-14T08:00:21.950","2020-02-14T08:00:21.950","How to find a homolog that has a PDB structure available","<blast><pdb>","3","0","",""
"11379","2","","11378","2020-02-11T19:20:45.000","1","","<p>If you are using NCBI Blast and blastp against the pdb database gives you nothing, you can do a psiblast against the normal refseq database, do a few rounds, save the PSSM matrix and submit a new job with the PSSM matrix but against the PDB database.</p>\n\n<p>You might need to limit the species of you get lots of too close homologues —say blasting E. coli protein for which you'd want to exclude Enterobacteriales.</p>\n","6322","","","","2020-02-11T19:20:45.000","","","","0","",""
"11380","2","","11371","2020-02-11T20:59:47.507","0","","<p>My problem was solved using <a href=""https://gatk.broadinstitute.org/hc/en-us/articles/360037226512-LiftoverVcf-Picard-#--ALLOW_MISSING_FIELDS_IN_HEADER"" rel=""nofollow noreferrer"">--ALLOW-MISSING-FIELDS-IN-HEADER</a>.</p>\n","3442","","","","2020-02-11T20:59:47.507","","","","0","",""
"11381","2","","11354","2020-02-11T21:53:08.760","0","","<p>If you reran the command with the correct settings, just leave it at that.  (It is not at all clear to me that strandedness rf is correct) </p>\n\n<p>If you want people to tell you if you ran the commands right, you need to put down what commands you used.</p>\n","2388","","","","2020-02-11T21:53:08.760","","","","0","",""
"11382","2","","11378","2020-02-11T22:43:59.427","1","","<p>To expand the first answer by @MatteoFurla: re-run the P-blast and under database option, which is set by default to non-redundant database (nr) select Protein databank (PDB), the third option from the bottom of the menu. All the Blast results will be from PDB and thus all results will have associated structures. Also note that the Genbank codes will be PDB codes. </p>\n\n<p>If you have zero hits then consider PSI-Blast, how I suspect you are running P-Blast via the nr database.</p>\n","3848","","3848","2020-02-11T22:51:23.033","2020-02-11T22:51:23.033","","","","0","",""
"11383","2","","11328","2020-02-12T00:35:39.963","0","","<h2>Oligopeptide</h2>\n\n<p>The lipophilicity of an oligopeptide or a small molecule can be calculated with RDKit, the classic compChem python toolkit. The measure is <a href=""https://en.wikipedia.org/wiki/Partition_coefficient"" rel=""nofollow noreferrer"">logP or partition coefficient</a>, namely the magnitude of the ratio of the concentration in octanol over than in water, so the higher it is, the more it partitions in the octanol phase. However, with a compound of several dalton it may be good to see what the contribution of each atom to this score are. This measure is the Crippen contribution to logP, which can be handily plotted on the molecule. Note that these are may be at first glance look similar to Gastaiger partial changes, but are not.</p>\n\n<p>Okay, first the imports</p>\n\n<pre><code>from rdkit import Chem\nfrom rdkit.Chem import AllChem, Descriptors\nfrom rdkit.Chem.Draw import SimilarityMaps\n</code></pre>\n\n<p>Now let's make the molecules. Let's start with something we can simply use a SMILES string of the web: <a href=""https://en.wikipedia.org/wiki/Bortezomib"" rel=""nofollow noreferrer"">Bortezomib</a>.</p>\n\n<pre><code>name = 'Bortezomib'\nmol = Chem.MolFromSmiles('O=C(N[C@H](C(=O)N[C@H](B(O)O)CC(C)C)Cc1ccccc1)c2nccnc2')\n</code></pre>\n\n<p>Now we can do all the relevant calculations on the <code>mol</code> object:</p>\n\n<pre><code>print('logP', Descriptors.MolLogP(mol))\nAllChem.Compute2DCoords(mol)\ncontribs = Chem.rdMolDescriptors._CalcCrippenContribs(mol)\nfig = SimilarityMaps.GetSimilarityMapFromWeights(mol, [x for x,y in contribs], colorMap='BuPu', contourLines=10)\nfig.savefig(name+'_crippen.png', bbox_inches='tight')\n</code></pre>\n\n<p>LogP=0.3\n<a href=""https://i.stack.imgur.com/M6qEH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M6qEH.png"" alt=""Bortezomib""></a></p>\n\n<p><code>colorMap='BuPu'</code> is the matplotlib colorscheme. I have no idea which work best, I have spent way too much time trying and I do not know. But with BuPu, white is hydrophilic, purple is hydrophobic.</p>\n\n<p>Say the oligopeptide does not have a SMILES or we don't want to do it that way. Let's take <a href=""https://en.wikipedia.org/wiki/Casomorphin"" rel=""nofollow noreferrer"">β-Casomorphins 1–3</a>:</p>\n\n<p>name = 'β-Casomorphins 1–3'\nmol = Chem.MolFromFASTA('YPF')\nmol = AllChem.ReplaceSubstructs(mol, Chem.MolFromSmiles('N'), Chem.MolFromSmiles('[NH][OH]'), replacementConnectionPoint=0)[0]</p>\n\n<p>The result with the last block from before gives:</p>\n\n<p>logP=1\n<a href=""https://i.stack.imgur.com/1JHCC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1JHCC.png"" alt=""casomorphin""></a></p>\n\n<h2>Proteins</h2>\n\n<p>With protein, it is a lot harder. Membrane affinity of protein is dependent on their structure. In PyMOL you can use the Adaptive Poisson-Boltzmann Solver (APBS) to show the surface by charge. For transmembrane protein there is clear ring of hydrophobicity —CHARMM-GUI membrane builder and a variety of tools for example can guess where it is. For a membrane associated protein only one side will be hydrophobic with a tell-tell charged ring around it (e.g. DHR1 domain) and there are no hard and tested methods for that.</p>\n","6322","","","","2020-02-12T00:35:39.963","","","","0","",""
"11384","1","11387","","2020-02-12T10:25:28.073","0","58","<p>Good morning, \nI'm aligning with blast some species, against all nt database to analyse the vegetables. However, in some cases we have species of animals mixed with vegetables and want only to detect the vegetable species. Normally, we would only expect to detect the vegetables only. But this is normal since we are using the all nt. So now, could anyone point me out, the way to detect only vegetables? Thanks for the help.</p>\n","2686","","2686","2020-02-12T15:53:12.663","2020-02-12T15:53:12.663","Vegetable databases usage","<database><nt>","1","5","",""
"11385","2","","11373","2020-02-12T10:57:33.907","1","","<p>First comment: you should be using real data, rather than simulated data, as we are not very good at modelling genetic variation and recombination at a genome-wide scale. A way that this can be done with GCTA is mentioned in the <a href=""https://www.cog-genomics.org/plink/1.9/input#random"" rel=""nofollow noreferrer"">PLINK manual</a>.</p>\n\n<p>I'll do some calculations to estimate the size of data you need to store:</p>\n\n<p>Assume that this plink-formatted file is in tped format (one row per SNP), and the entire genome is diploid. That means that a single SNP/individual pair will take up three bytes. You want 30,000 cases and 70,000 controls, so each line from each sample will take up 300kB (plus a bit of change for the header). With 1 thousand SNPs, that's 300MB per sample; with 1 million SNPs, that's 300GB per sample; with 10 million SNPs, that's 3TB per sample; with 40 million SNPs, that's 12 TB for one sample.</p>\n\n<p>Reading the plink manual, I see that the BED format is a two-bit compression for each SNP/individual pair, so one byte per 4 individuals, or about 12x compression (i.e. 1TB for one sample).</p>\n\n<p>Okay, that's a lot. It's well within a 256TB storage limit, but might hit file size limits depending on the file system.</p>\n\n<p>If there is a data generation issue with PLINK, it might be worth trying (as you suggest) splitting the input for simulation then merging afterwards. My guess is that doing the simulation on fewer SNPs (rather than fewer individuals) would work better. Plink has a <code>--bmerge</code> <a href=""https://www.cog-genomics.org/plink/1.9/data#merge"" rel=""nofollow noreferrer"">option</a> for merging the compressed binary file formats.</p>\n\n<p>I usually approach problems like these by working my way up from something that I know works. Try first merging two datasets of 1000 SNPs. If that works, try larger datasets. Rinse and repeat until the computer crashes, you get bored, or you reach your goal.</p>\n","73","","","","2020-02-12T10:57:33.907","","","","2","",""
"11386","1","","","2020-02-12T11:03:18.040","1","57","<p>I have  very large sizes tab-delimited .vcf files and want to match these two / or 3 files based on their position and print to a new .csv file  </p>\n\n<p><strong>File structures:</strong>  </p>\n\n<p>File_1: tab-delimited file (.vcf) and its as column names as follows\n<code>\n(line number 3439) #CHROM     POS     ID       REF      ALT      QUAL    FILTER \n INFO</code></p>\n\n<p><strong>File 2: same as file_1 column names</strong>  </p>\n\n<p><code>(line number 3407) #CHROM     POS     ID      REF     ALT     QUAL    FILTER \n INFO  FORMAT  SAMPLE_1  SAMPLE_2 .....</code>  </p>\n\n<p>In file_2, column 7 (INFO) contain many substrings like\n<code>AC=46,2;AF=0.958,0.042;AN=48;DP=269;ExcessHet=3.0103;FS=0.000;InbreedingCoeff=0.5411;MLEAC=92,4;MLEAF=1.00,0.083;QD=25.36;SOR=2.488  from these strings , have to print  only information of AC=, AF=, AN=, DP=</code></p>\n\n<p><strong>Desired output files to generate:</strong></p>\n\n<ol>\n<li>position matched in two of files: common_position_matched.csv  </li>\n</ol>\n\n<p>if I have more than 3 files also, the output should be one file and important thing, if a position column (1-POS) only matched only in 2 files and 1st file line should be NA NA NA   </p>\n\n<pre><code>file_1  CHROM  POS    REF  ALT     file_2 #CHROM  POS      REF  ALT  INFO\n\n1       22    10511521  C     T         1    22  10511521   C    T   AC=46,2 AF=0.958,0.042 AN=48 DP=269  \n2       22   10510544   G     A         2    22  10510544   G    A   AC=49,2 AF=0.958,0.042 AN=89 DP=536  \n3       22   10515068  AGAT,T AGAT,AT   3    22  10515068   AAA  AAAGG,A,GAA AC=100  AF=0.958,0.042 AN=62 DP=123  \n4       22   10515118  A G,   TAA       4    22  10515118   AG,  TAA AC=32   AF=0.958,0.042 AN=45 DP=500  \n5       22   10515118  AAAG   A         5    22  10515118   AATG A   AC=50   AF=0.958,0.042 AN=49 DP=129\n</code></pre>\n\n<p><strong>note:</strong> while doing matching, not removing the duplicates, because in the same position there may be an addition or sometimes it may be deletion.      </p>\n\n<ol start=""2"">\n<li>unique position of each file, in tab-delimited  </li>\n</ol>\n\n<p>output: File1_unique.csv and File2_unique.csv etc.  </p>\n\n<p>so far was able to read the file and match them according to position and print the output, but was not able to write efficient code   </p>\n\n<pre><code>import pandas as pd\ndf1 = pd.read_csv(""File1_3.vcf"",sep='\t',usecols = ['POS']) ## Reading file1\ndf2 = pd.read_csv(""file2_3.vcf"", sep=""\t"", usecols = ['POS']) ## Reading file2\ndf3 = pd.concat([df1,df2], sort=True) ## Combining both the dataframes\ndf4 = df3.drop_duplicates(keep=False) ## Dropping the duplicates (intersect)\ndf4.to_csv(""c3-UniquePosition_of_bothData.csv"", sep=""\t"", index=False, header=True) ## Writing the unique to both\ndf1_Uni_file1_c3Posi = pd.merge(df4, df1, on='POS', how='inner') ## Identifying the unique position of File1 \ndf2_Uni_File2_c3Posi = pd.merge(df4, df2, on='POS', how='inner') ## Identifying the unique position of File2\ndf_File1_File2_common_c3Posi = pd.merge(df1, df2, on='POS', how='inner') # Identifying the common chr-position of File1 and File2```  \n\nProgram 2: (giving original file without editting)\nimport pandas as pd\ndf1= pd.read_csv(""File1_22.vcf.gz"", sep=""\t"", skiprows=3438, usecols = [0,1,2,3,4])\ndf2 = pd.read_csv(""File2_22.vcf.gz"", sep=""\t"", skiprows=3406, usecols = [0,1,3,4,7])\n#writing the output files\n#df1.to_csv(""File1_c22.csv"")\n#df2.to_csv(""File2_c22.csv"")\n#mergeing\ndf3 = pd.merge(df1, df2, on='POS', how='inner', sort=True)\ndf3.to_csv(""common_position.csv"", sep="","", index=False, header=True)\n#df3 = pd.concat([df1,df2], axis=1).to_csv('check1.csv') # this command join multiple output to single output\n</code></pre>\n\n<p>Could any one give efficient python pandas script, to do this</p>\n\n<p>Thanks All</p>\n","1162","","1162","2020-02-14T06:01:08.760","2020-02-14T06:01:08.760","Finding common and unique data set by comparing two files based on their column and to split the columns multiple strings to print in output","<python><perl>","0","3","",""
"11387","2","","11384","2020-02-12T11:22:34.040","1","","<p>Using the blast command line tool, you can limit your search to a specific taxon using the <code>–taxids</code> option.</p>\n\n<p>So, in your case <code>-taxids 3193</code> should limit your search to plants.</p>\n\n<hr>\n\n<p>If you are using the ncbi blast webinterface, you can restrict your search by entering ""plants"" into the Organism field in the ""Choose Search Set"" section</p>\n\n<p><a href=""https://i.stack.imgur.com/MwD75.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MwD75.png"" alt=""enter image description here""></a></p>\n","668","","668","2020-02-12T12:44:31.533","2020-02-12T12:44:31.533","","","","3","",""
"11388","1","11424","","2020-02-12T15:09:10.333","1","76","<p>kindly your comment highly appreciated </p>\n\n<p>have more than 4k header sequences look like:</p>\n\n<pre><code>&gt;LTR_retrotransposon100_Gypsy? \ncgtgcaccccaatgttcatagcagcactatttacaatagccaagacatggaaacaaccta aatgtcctttgacaggtgactggataaagaagctgtggtatatttatacaatggaatgct actcagccataaaaaagaataagataatgccatttacagcaacatggatagacctggaga atgttattctaagtgaagtaagccagaaagaaaaagaaaaataccatatgctatcactta\n\n&gt;LTR_retrotransposon10011_ERVK \ntaatttgaaaagatacatgcaacccaatgttcatagcagcattatttacaattgccaaga tatggaagcaatctaagtgtccatcaacagatgaatggagagagaagatgtggtatatat acaataaaatactattctgtcataaaaatgaataaaattctgccatttgcaacaacatgg atagacctggagggtattatgctatgtgaaataagtcagacagagaaagacatatactat\n\n&gt;LTR_retrotransposon10011_ERVL-MaLR \n    taatttgaaaagatacatgcaacccaatgttcatagcagcattatttacaattgccaaga tatggaagcaatctaagtgtccatcaacagatgaatggagagagaagatgtggtatatat acaataaaatactattctgtcataaaaatgaataaaattctgccatttgcaacaacatgg atagacctggagggtattatgctatgtgaaataagtcagacagagaaagacatatactat\n\n&gt;LTR_retrotransposon100_Copia \n    cgtgcaccccaatgttcatagcagcactatttacaatagccaagacatggaaacaaccta aatgtcctttgacaggtgactggataaagaagctgtggtatatttatacaatggaatgct actcagccataaaaaagaataagataatgccatttacagcaacatggatagacctggaga atgttattctaagtgaagtaagccagaaagaaaaagaaaaataccatatgctatcactta\n</code></pre>\n\n<p>I would like to use awk or sed to remove everything between the underscore, including the underscore and replace it with / \nplus add sequential RNA followed by #.  </p>\n\n<p>and the target output could be:</p>\n\n<pre><code>&gt;RNA1#LTR/Gypsy?\nctcagcagcactatttacaatagccaagacatggaaacaacctaaatgtcttatcaatag atgactggataaaggagctgtggtatatctatacaatggaataccatcagccataaaaaa gaataaaatattgccatttgcagcaacatggatggacctggagattatcattctaaggga agtaagccagaaagagaaagaaaaataccatatgatatcacttatatgtggaggtaaaaa aaaaaaaaaagacacaaatcaatttatttgcaaaacatacaTGGActttcagacatagaa\n\n&gt;RNA2#LTR/ERVL-MaLR\ntaatttgaaaagatacatgcaacccaatgttcatagcagcattatttacaattgccaaga tatggaagcaatctaagtgtccatcaacagatgaatggagagagaagatgtggtatatat acaataaaatactattctgtcataaaaatgaataaaattctgccatttgcaacaacatgg atagacctggagggtattatgctatgtgaaataagtcagacagagaaagacatatactat\n\n\n\n&gt;RNA3#LTR/ERVK\n    taatttgaaaagatacatgcaacccaatgttcatagcagcattatttacaattgccaaga tatggaagcaatctaagtgtccatcaacagatgaatggagagagaagatgtggtatatat acaataaaatactattctgtcataaaaatgaataaaattctgccatttgcaacaacatgg atagacctggagggtattatgctatgtgaaataagtcagacagagaaagacatatactat\n\n&gt;RNA4#LTR/Copia \n    cgtgcaccccaatgttcatagcagcactatttacaatagccaagacatggaaacaaccta aatgtcctttgacaggtgactggataaagaagctgtggtatatttatacaatggaatgct actcagccataaaaaagaataagataatgccatttacagcaacatggatagacctggaga atgttattctaagtgaagtaagccagaaagaaaaagaaaaataccatatgctatcactta\n</code></pre>\n","3332","","3848","2020-02-20T15:33:23.277","2020-02-20T15:33:23.277","Edit all the fasta headers using awk","<fasta><phylogenetics><filtering><data-preprocessing><awk>","3","2","",""
"11389","2","","11388","2020-02-12T16:29:01.507","1","","<pre><code>sed -e 's:^\(.*\)_.*_\(.*\)$:\1/\2:' &lt; input &gt; output\n</code></pre>\n","3332","","","","2020-02-12T16:29:01.507","","","","0","",""
"11390","1","","","2020-02-12T17:10:48.107","1","17","<p>I need to use tophat2 to align some sequences, and I wish to use the docker container. These are large sequences so it will take a long time, plus I'm working on a university server so the chances of being kicked off the server are very high. How can I use nohup and &amp; to run tophat2 through docker, if I need some user interaction?</p>\n","7209","","","","2020-02-13T07:10:03.690","Running tophat dockerfile in background","<alignment><bash><docker>","1","1","","1"
"11391","2","","11388","2020-02-12T17:27:27.617","1","","<p><strong>EDIT after OP's update:</strong></p>\n\n<p>Try this Perl one-liner:</p>\n\n<pre><code>perl -pe 'BEGIN { <span class=""math-container"">$i = 1 } $</span>i += s{&gt;([^_]+)_.*_},{&gt;RNA<span class=""math-container"">${i}#$</span>{1}/}' input_file &gt; output_file\n</code></pre>\n\n<p>Here, the command line flags are:<br>\n<code>-e</code>: tells the perl interpreter to look for the code inline, rather than in a file with the script,<br>\n<code>-p</code>: loop over the input one line at a time, execute the code in the one-liner for each iteration of the loop, and <code>print</code> the result into <code>STDOUT</code>.<br>\nThe code has regex replacement (see <a href=""https://perldoc.perl.org/perlre.html"" rel=""nofollow noreferrer""><code>perlre</code></a> for more details):<br>\n<code>s{&gt;([^_]+)_.*_},{&gt;RNA${i}#${1}/}</code>:<br>\nreplace an underscore, followed by zero or more occurrences (<code>*</code> quantifier) of any character (<code>.</code>), followed by an underscore, with slash (<code>/</code>).<br>\nKeep <code>&gt;</code> and the original header part before the underscore, captured by parens into <code>$1</code> variable.<br>\nAdd string <code>RNA${i}</code>, where <code>$i</code> is the counter of the fasta headers. The counter is incremented by 1 when the substitution occurs, that is, when the next header occurs.<br>\nThe variables are used like so in the substitution: <code>${i}</code>, <code>${1}</code> to prevent wrong interpolation.<br>\nI am also using <code>s{},{}</code> as delimiters for readability (as in most cases) instead of the default <code>s///</code>.</p>\n\n<p><strong>EXAMPLE:</strong></p>\n\n<pre><code>echo ""&gt;LTR_retrotransposon100_Gypsy?\nACGT\n&gt;LTR_retrotransposon10011_ERVK\nTCGA"" | perl -pe 'BEGIN { <span class=""math-container"">$i = 1 } $</span>i += s{&gt;([^_]+)_.*_},{&gt;RNA<span class=""math-container"">${i}#$</span>{1}/}'\n</code></pre>\n\n<p>Prints:</p>\n\n<pre><code>&gt;RNA1#LTR/Gypsy?\nACGT\n&gt;RNA2#LTR/ERVK\nTCGA\n</code></pre>\n","6251","","6251","2020-02-18T22:08:28.993","2020-02-18T22:08:28.993","","","","1","",""
"11392","2","","11334","2020-02-12T18:25:34.603","1","","<p>To add to Michael G.'s answer, there is a further wrinkle in the case of 2019-nCov that is due to the exponential growth in confirmed infections daily.</p>\n\n<p>That growth affects your second formula <code>CONFIRMED_DEATHS/(CONFIRMED_DEATHS + CONFIRMED_RECOVERIES</code> since deaths probably occur earlier on in the disease trajectory than recoveries do. Since the number of confirmed cases is growing exponentially on a daily basis, the number of deaths will presumably be inflated relative to the number of recoveries at this point. </p>\n\n<p>This feature may have the opposite effect on your first formula <code>CONFIRMED DEATHS/CONFIRMED INFECTIONS</code>, depending on when the infections are confirmed. If the infections are generally confirmed several days before death occurs, then a large portion of confirmed infections are still too early to have ended in death and <code>CONFIRMED DEATHS/CONFIRMED INFECTIONS</code> actually underestimates the actual number of fatalities. </p>\n","7210","","","","2020-02-12T18:25:34.603","","","","0","",""
"11393","1","","","2020-02-12T18:35:44.547","0","33","<p>I have few 100 raw fastq files from whole-genome sequencing data and I would like to map these files to a set of genes only (and not whole genome) so as to find SNP's associated with them. Can anyone tell me what snp analysis pipeline is best for targeted variation and not whole genome?\nI have this in mind:\nMapping the reads with bwa and then using varscan\nHope to hear for more options</p>\n","2040","","57","2020-02-12T20:24:48.210","2020-02-13T20:11:27.487","Variant calling for a subset of genes using whole gneome sequencing data","<snp><variant-calling><bwa><wgs>","2","0","",""
"11394","1","11395","","2020-02-12T20:42:15.877","1","33","<p>I have called peaks using MACS2. Then I got a narrowPeak file like this.</p>\n\n<pre><code>1   4491713 4491913 testONE_peak_1  37  .   2.86003 3.72580 1.66717 80\n1   4492782 4493153 testONE_peak_2  27  .   2.22933 2.71107 0.82442 242\n1   4571554 4572026 testONE_peak_3  72  .   4.16005 7.21000 4.85200 282\n1   4739486 4739879 testONE_peak_4  64  .   3.63128 6.42948 4.14394 275\n1   4784848 4785061 testONE_peak_5  31  .   2.24218 3.19160 1.21569 133\n</code></pre>\n\n<p>Then I tried to visualize it in UCSC Genome browser and I got following error.</p>\n\n<pre><code>Error File 'testONE_peaks.narrowPeak' - Unrecognized format line 1 of file: 1 4491713 4491913 testONE_peak_1  37  .   2.86003 3.72580 1.66717 80 (note: chrom names are case sensitive, e.g.: correct: 'chr1', incorrect: 'Chr1', incorrect: '1')\n</code></pre>\n\n<p>So I added chr to the chromosome column instead of 1 (Chromosome number). </p>\n\n<pre><code>chr1    4491713 4491913 testONE_peak_1  37  .   2.86003 3.7258  1.66717 80\nchr1    4492782 4493153 testONE_peak_2  27  .   2.22933 2.71107 0.82442 242\nchr1    4571554 4572026 testONE_peak_3  72  .   4.16005 7.21    4.852   282\nchr1    4739486 4739879 testONE_peak_4  64  .   3.63128 6.42948 4.14394 275\nchr1    4784848 4785061 testONE_peak_5  31  .   2.24218 3.1916  1.21569 133\n</code></pre>\n\n<p>When I tried to load the file again I got following error.</p>\n\n<pre><code>Error File 'testONE_peaks_NEW2.narrowPeak' - Error line 1 of custom track: thickStart out of range (chromStart to chromEnd, or 0 if no CDS)\n</code></pre>\n\n<p>Then I added following line to the top and I could load the file to UCSC Genome browser.</p>\n\n<pre><code>track type=narrowPeak name=""Somite narrowPeak""\nchr1    4491713 4491913 testONE_peak_1  37  .   2.86003 3.7258  1.66717 80\nchr1    4492782 4493153 testONE_peak_2  27  .   2.22933 2.71107 0.82442 242\nchr1    4571554 4572026 testONE_peak_3  72  .   4.16005 7.21    4.852   282\nchr1    4739486 4739879 testONE_peak_4  64  .   3.63128 6.42948 4.14394 275\nchr1    4784848 4785061 testONE_peak_5  31  .   2.24218 3.1916  1.21569 133\n</code></pre>\n\n<p>But the problem is I can not see anything in the Genome browser. Only the name is there. But nothing else. Do you know what I should do?</p>\n","856","","","","2020-02-12T21:51:33.843","How to visualize called narrowPeak files in UCSC Genome browser or IGV?","<rna-seq><chip-seq><ucsc><igv><macs2>","1","1","",""
"11395","2","","11394","2020-02-12T21:34:53.253","2","","<p>I think you might have changed the separator (or at least have some kind of inconsistency from the <a href=""https://genome.ucsc.edu/FAQ/FAQformat.html"" rel=""nofollow noreferrer"">required format</a>) for your file. Note that peak output files from MACS2 are variants of BED files.</p>\n\n<p>It seems you <strong>need to have a <em>tab</em> separator for this type of file.</strong></p>\n\n<p>I copied your example file and then ran <code>awk -v OFS=""\t"" '$1=$1' your_peaks.narrowPeaks &gt; fixed_your_peaks.narrowPeaks</code> on it. This produced a result that I was able to see in both the UCSC genome browser and IGV.</p>\n","383","","383","2020-02-12T21:51:33.843","2020-02-12T21:51:33.843","","","","3","",""
"11396","2","","11393","2020-02-12T22:43:51.447","3","","<p>Your approach is not recommended. Always align against the entire genome. If you align against a subset you might accumulate false mappings. The aligner will always try to find the best match for every read. If the true origin of the read is not in the reference then the aligner will still try to find an acceptable mapping position. Therefore, align against the full reference and then extract the regions you are interested in, e.g. with the region option of <code>samtools view</code>:</p>\n\n<p><code>samtools view [options] &lt;in.bam&gt;|&lt;in.sam&gt;|&lt;in.cram&gt; [region ...]</code></p>\n\n<p>Check its manual for details.</p>\n\n<p>While there is basically nothign wrong with <code>VarScan</code> it is quite old and not maintained anymore. I suggest you give <code>bcftools</code> a try (check its manual). Other maintained alternatives can be (among others) <code>freebayes</code>, strelka2, VarDict or the GATK from the Broad institute.</p>\n","3051","","","","2020-02-12T22:43:51.447","","","","0","",""
"11397","2","","11390","2020-02-13T07:10:03.690","2","","<p>Assuming your user is in the group permitted to use docker:</p>\n\n<pre><code>docker pull quay.io/biocontainers/tophat:2.1.1--py27_3\nnohup docker run -v /some/path:/data quay.io/biocontainers/tophat:2.1.1--py27_3 tophat2 ... &amp;\n</code></pre>\n\n<p>The only thing to remember is to mount everything you'll need into the docker container (<code>/data</code> in the example above). If you need to debug things then you can add the <code>-it</code> flag to <code>docker run</code> and use <code>bash</code> as the command.</p>\n\n<p>As an aside, you might find using <code>screen</code> or <code>tmux</code> preferable to <code>nohup ... &amp;</code>, since you can reconnect to sessions with them.</p>\n","77","","","","2020-02-13T07:10:03.690","","","","0","",""
"11398","2","","11281","2020-02-13T09:47:33.470","0","","<p>Actually I found an easy and efficient way.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport scipy\nimport math\nimport openpyxl\nfrom openpyxl import Workbook\nfrom scipy.stats import pearsonr\nfrom scipy.stats import spearmanr\nfrom scipy.stats import kendalltau\n\npro=pd.read_csv('m_test.csv', index_col=0)\n\nxInds = []\nyInds = []\nfor i in range(len(pro.index)):\n    for j in range(i+1, len(pro.index)):\n        xInds.append(i)\n        yInds.append(j)\n\nz=0\nRlist_sp = []\nPlist_sp = []\nRlist_pe = []\nPlist_pe = []\nwhile z &lt; len(xInds):\n    b=xInds[z]\n    c=yInds[z]\n\n    spR, spP = spearmanr(pro.iloc[b].values, pro.iloc[c].values)\n    peR, peP = pearsonr(pro.iloc[b].values, pro.iloc[c].values) \n\n    Rlist_sp.append(spR)\n    Plist_sp.append(spP)\n    Rlist_pe.append(peR)\n    Plist_pe.append(peP)    \n    z=z+1 \n\nG1=pd.DataFrame(xInds)\nG2=pd.DataFrame(yInds)\nR_sp=pd.DataFrame(Rlist_sp)\nP_sp=pd.DataFrame(Plist_sp)\nR_pe=pd.DataFrame(Rlist_pe)\nP_pe=pd.DataFrame(Plist_pe)        \n\nFinal=pd.concat([G1, G2, R_sp, P_sp, R_pe, P_pe], axis=1)\nnp.savetxt('Final_gg_A549.txt', Final.values, fmt='%s', delimiter='\t')  \n</code></pre>\n","6670","","","","2020-02-13T09:47:33.470","","","","0","",""
"11400","2","","11378","2020-02-13T10:59:09.257","1","","<p>You could use HMM-based comparison, for example with <a href=""https://toolkit.tuebingen.mpg.de/#/"" rel=""nofollow noreferrer"">HHpred</a>. This is generally the best way to find structural templates because structure is more conserved than sequence, and comparing HMMs will turn up hits of the same structure but minimal sequence overlap.</p>\n\n<p>If HHpred turns up nothing you are in the region of de novo modelling, which is a different problem. However, it has advanced in recent years to the point where it may be useful to you. For example, you could try our method <a href=""https://www.nature.com/articles/s41467-019-11994-0"" rel=""nofollow noreferrer"">DMPfold</a>.</p>\n","4622","","","","2020-02-13T10:59:09.257","","","","0","",""
"11401","2","","11276","2020-02-13T12:50:28.870","2","","<p><a href=""https://github.com/USDA-VS/vSNP"" rel=""nofollow noreferrer"">vSNP</a> outputs a VCF file that includes positions with no coverage, represented as ""N"".  Then there is a tool to merge a VCF file into the FASTA reference  <code>vsnp_merge_vcf_into_fasta.py</code>.  It will merge those ""N"" regions into the FASTA reference. There are options for considering frequency and quality.\n It's limited to just SNPs and those regions along the reference with no coverage.  It does not merge small indels or unmapped reads.  <a href=""https://github.com/broadinstitute/pilon/wiki/Methods-of-Operation"" rel=""nofollow noreferrer"">Pilon</a> is a better tool for applying indels, but I'm not aware of it letting regions with no coverage be represented as ""N"".</p>\n\n<pre class=""lang-py prettyprint-override""><code>vSNP_step1.py -r1 *_R1*fastq.gz -r2 *_R2*fastq.gz -r ref.fasta\n</code></pre>\n\n<p>Use <code>*zc.vcf</code> output to merge into FASTA</p>\n\n<pre class=""lang-py prettyprint-override""><code>vsnp_merge_vcf_into_fasta.py -f ref.fasta -v *_zc.vcf\n</code></pre>\n","6343","","","","2020-02-13T12:50:28.870","","","","0","",""
"11403","1","","","2020-02-13T16:14:29.463","1","49","<p>(How) can you index a BAM file using <code>pysam</code>?</p>\n\n<p>When I tried the intuitive <code>pysam.index</code> I got:</p>\n\n<pre><code>import pysam\nmy_bam = pysam.AlignmentFile(""regular_bwamem_mapping.bam"", ""rb"")\npysam.index(my_bam)\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-5-a0b3a04ecb2c&gt; in &lt;module&gt;\n----&gt; 1 pysam.index(L_bam)\n\n~/miniconda3/lib/python3.7/site-packages/pysam/utils.py in __call__(self, *args, **kwargs)\n     59             args,\n     60             catch_stdout=kwargs.get(""catch_stdout"", True),\n---&gt; 61             save_stdout=kwargs.get(""save_stdout"", None))\n     62\n     63         if kwargs.get(""split_lines"", False):\n\npysam/libcutils.pyx in pysam.libcutils._pysam_dispatch()\n\nTypeError: object of type 'pysam.libcalignmentfile.AlignmentFile' has no len()\n</code></pre>\n","57","","1775","2020-02-14T08:00:27.927","2020-02-14T08:00:27.927","Index a BAM file using pysam","<bam><indexing>","1","1","",""
"11404","2","","11403","2020-02-13T16:14:29.463","4","","<p>Oh you silly sausage, <code>pysam.index</code> takes a bam file name, not a python object.</p>\n\n<pre><code>import pysam\npysam.index(""regular_bwamem_mapping.bam"")\n</code></pre>\n\n<p>will index your .bam file.</p>\n","57","","","","2020-02-13T16:14:29.463","","","","2","",""
"11406","2","","11393","2020-02-13T20:11:27.487","0","","<p><a href=""https://github.com/USDA-VS/vSNP"" rel=""nofollow noreferrer"">vSNP</a> can be used to target specific regions of a genome. Pipeline is a 2 step process:</p>\n\n<ol>\n<li>create VCF files (via Freebayes)</li>\n<li>then in your case run all 100 VCF files to create SNP table</li>\n</ol>\n\n<p>A filter file can be supplied to step 2.  By filtering all position ranges other than the genes of interest, step 2 will output a SNP table (.xlsx) with only your genes of interest.  Also, table will include annotation if GBK is provide. </p>\n","6343","","","","2020-02-13T20:11:27.487","","","","0","",""
"11407","1","11408","","2020-02-13T22:07:36.073","2","30","<p>I recently downloaded a genome from NCBI for chimpanzee and was surprised to see no Y chromosome.  I added in one from a separate accession and carried on, but then today went to do the same for gorilla and saw the same thing, no Y.</p>\n\n<ul>\n<li>For Gorilla gorilla (western gorilla): <a href=""https://www.ncbi.nlm.nih.gov/genome/2156"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/genome/2156</a></li>\n<li>For Pan troglodytes (chimpanzee): <a href=""https://www.ncbi.nlm.nih.gov/genome/202"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/genome/202</a></li>\n</ul>\n\n<p>Why would this be?  I can see a separate entry for a Y chromosome for that same exact chimp used for the representative genome (Clint) here, but I don't see why it's not in the representative genome listing itself:</p>\n\n<p><a href=""https://www.ncbi.nlm.nih.gov/nuccore/DP000054.3"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/nuccore/DP000054.3</a></p>\n","4980","","","","2020-02-14T05:06:33.640","Why do these NCBI representative genomes for ape species have no Y chromosome?","<reference-genome><ncbi>","1","0","",""
"11408","2","","11407","2020-02-14T05:06:33.640","6","","<p>tl;dr: technical difficulties, or sex</p>\n\n<p>For the gorilla, that genome is from a female gorilla (<a href=""https://www.nature.com/news/gorilla-joins-the-genome-club-1.10185"" rel=""noreferrer"">Kamilah the gorilla</a>) so she doesn't have a Y chromosome.</p>\n\n<p>For the chimp link, there is a visualization of the chromosomes that you can click on that takes you to the genome browser for the Y chromosome (see image). Notably, there is a <a href=""https://www.ncbi.nlm.nih.gov/assembly/GCA_000001515.5"" rel=""noreferrer"">more recent genome build</a> that does have a Y chromosome sequence assembly that is directly linked, they just haven't updated the main reference sequence yet.</p>\n\n<p><a href=""https://i.stack.imgur.com/kw5so.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kw5so.png"" alt=""chimp entry""></a></p>\n\n<p>To directly address the question, the Y chromosome is <a href=""https://www.cell.com/trends/genetics/pdf/S0168-9525(17)30019-7.pdf"" rel=""noreferrer"">very hard to assemble</a>. The easy part of it to assemble is already on the X chromosome anyways (the pseudoautosomal region), and the hard part of it is often a mess of repeats, especially in mammals. Many groups will decide to just assemble the X as well as they can and call it good, due to the difficulties associated with assembling the Y. If they are working with a male individual the Y sequence is still there, it just may not be anchored to a chromosome sequence, so it's only in the unscaffolded/unanchored assembly (""master WGS"" I believe, usually). It is also probably highly fragmented (it isn't a whole chromosome of sequence stitched together, but rather a lot of little pieces of sequence).</p>\n\n<p>In some cases they will focus all sequencing efforts on female individuals specifically to make their assembly job easier, because in females the X is at the same copy number as the autosomes and doesn't have interference from homologous regions on the Y.</p>\n","2108","","","","2020-02-14T05:06:33.640","","","","0","",""
"11409","1","","","2020-02-14T10:35:26.330","0","21","<p>I installed kraken2, which includes the 16S silva database. Indeed I see 16S_silva_installation.sh. How ca I install it? </p>\n","7203","","","","2020-02-14T10:35:26.330","How can I open the 16S_silva.installation.sh in kraken2?","<database>","0","1","",""
"11410","2","","6852","2020-02-14T14:12:53.523","3","","<p>I tried to adapt this approach within rdKit to calculate exact mass (for mass spectrometry), but was not successful.  </p>\n\n<p>This library (<a href=""https://pypi.org/project/molmass/"" rel=""nofollow noreferrer"">https://pypi.org/project/molmass/</a>) appears to be able to do this:</p>\n\n<pre><code>from molmass import Formula\nf = Formula('C7H11N3O2')\nf.formula\nf.isotope.mass\n'169.08512660696'\n</code></pre>\n","7223","","","","2020-02-14T14:12:53.523","","","","0","",""
"11411","1","","","2020-02-14T16:09:59.783","0","16","<p>I ran homer differential accessibility analysis using this command getDifferentialPeaksReplicates.pl with 7 replicates of Granulocyte monocyte progenitor(GMP) vs Monocyte(Mono)</p>\n\n<p>Im bit confused when I did GMP vs Mono rna seq comparison I do see around 3500 genes deferentially expressed which goes up and down but at the same time I don't see any differential accessibility when it comes to down-regulated change. Am I doing something wrong ? </p>\n\n<pre><code>Using DESeq2 to calculate differential expression/enrichment...\n    Output Stats input vs. target:\n        Total Genes: 9454\n        Total Up-regulated in target vs. input: 8798 (93.061%) [log2fold&gt;1, FDR&lt;0.05]\n        Total Dn-regulated in target vs. input: 0 (0.000%) [log2fold&lt;-1, FDR&lt;0.05]\n</code></pre>\n","285","","","","2020-02-14T16:09:59.783","homer differential accessibility analysis","<homer>","0","0","",""
"11412","1","","","2020-02-14T16:11:43.883","1","68","<p>Is there a methodology for detecting whether a genome has been tampered with by a biologist?  For instance, say there has been an ebola outbreak such as in ""The Andromeda Strain"".  Is there a way a researcher can tell by just looking at the ebola genetic data and determine it is a genetically engineered strain?</p>\n\n<p>Some ideas I have:</p>\n\n<ol>\n<li><p>organism is abnormally close to unrelated organisms, i.e. some of the genetic code from the other organisms has been inserted into the first</p></li>\n<li><p>subsequences are very different from the rest of the genome.  If there is a consistent rate of mutation applied to the entire genome, then a subsequence that is abnormally noise free could be inserted.</p></li>\n<li><p>combination of 1 and 2: finding clear instances of unrelated organism genomes inserted into the genome being examined.</p></li>\n</ol>\n\n<p>Additionally, are there any publicly accessible reference genomes that are known to be genetically engineered, that I could use as examples to test such ideas?</p>\n\n<p>UPDATE: the wikipedia article on detecting genetically modified organism is not quite what I am looking for, since it claims current methods cannot detect unknown genetic modification.</p>\n\n<p><a href=""https://en.wikipedia.org/wiki/Detection_of_genetically_modified_organisms"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Detection_of_genetically_modified_organisms</a></p>\n\n<blockquote>\n  <p>Currently, it is highly unlikely that the presence of unexpected or\n  even unknown GMOs will be detected, since either the DNA sequence of\n  the transgene or its product, the protein, must be known for\n  detection.</p>\n</blockquote>\n","4806","","3848","2020-02-16T20:10:54.093","2020-02-16T20:42:21.050","Detecting whether genome is genetically engineered?","<phylogenetics><phylogeny><genetics>","1","5","",""
"11413","1","","","2020-02-14T19:26:31.683","0","22","<p>""Aliases"" or ""Synonyms"" should represent the same gene but with different names. But when I try to find the sequence of one alias and try to match with another alias they are significantly different.</p>\n\n<p>During bioinformatics analysis, I need to convert NAT5 alias ID to its gene symbol ID (NAA20, NAA50). When I tried to match the sequences of NAA20 and NAA50, they are very different but still NAT5 is an alias of both NAA20 and NAA50.</p>\n\n<p>I have to convert one id to another id (ENSEMBL). These gene symbol and alias is so confusing.</p>\n","6471","","","","2020-02-14T19:26:31.683","Why the gene symbol aliases have significantly different sequences?","<ensembl><identifiers><entrez>","0","9","",""
"11415","1","","","2020-02-14T23:58:11.373","1","51","<p>I would like to find a list of all the human genes and their proteins’ functional classification. </p>\n\n<p>The <code>Gene type</code> attribute on <a href=""http://useast.ensembl.org/biomart/martview/440eaadc1b7770036788e1a8d7787471"" rel=""nofollow noreferrer"">ensembl.org</a> is not specific enough, because it assign the label <code>coding protein</code> to all the proteins for which I am trying to classify. Also the <code>KEGG Pathway and Enzyme ID</code> option doesn't seem to help. </p>\n\n<p>I have translated all the gene names to their GO representation, but now I am not sure how to query for their cellular function, biological process, and cellular component.</p>\n\n<p>Is there such a platform that can retrieve this info from PANTHER?</p>\n\n<p>Sadly, the <a href=""http://www.ebi.ac.uk/reference_proteomes/"" rel=""nofollow noreferrer"">reference proteome</a> of humans doesn't contain a functional classification either:</p>\n\n<pre><code> zcat UP000005640_9606.fasta.gz | grep -i mYOSIN-2\n&gt;sp|Q9UKX2|MYH2_HUMAN Myosin-2 OS=Homo sapiens OX=9606 GN=MYH2 PE=1 SV=1\n\n# &gt;db|UniqueIdentifier|EntryName ProteinName OS=OrganismName OX=OrganismIdentifier [GN=GeneName ]PE=ProteinExistence SV=SequenceVersion\n</code></pre>\n\n<p>Is there any way to use <a href=""http://%20https://bgee.org/?page=gene&amp;gene_id=ENSG00000125414"" rel=""nofollow noreferrer"">bgee</a>, <a href=""https://www.ebi.ac.uk/QuickGO/annotations?geneProductId=Q9UKX2"" rel=""nofollow noreferrer"">QuickGO</a>, <a href=""https://tools.dice-database.org/GOnet/"" rel=""nofollow noreferrer"">GONet</a>, or <a href=""https://www.uniprot.org/uniprot/Q9UKX2"" rel=""nofollow noreferrer"">Uniport</a> to cluster genes using the <a href=""https://www.uniprot.org/uniprot/?query=proteome:UP000005640#customize-columns"" rel=""nofollow noreferrer"">customize columns</a>?</p>\n\n<p>EDIT:</p>\n\n<p>I think that getting the most specific (<code>GO term name</code>) and least specific (<code>GO domain</code>) gene's GO values can be retrieved from ensmebl:</p>\n\n<p><a href=""https://i.stack.imgur.com/QrELf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QrELf.png"" alt=""enter image description here""></a></p>\n","734","","734","2020-02-19T16:43:31.847","2020-02-19T16:43:31.847","How to get the GO information for all the human genes?","<python><gene><proteins><go-enrichment><functional-annotation>","0","4","",""
"11416","1","","","2020-02-15T15:29:39.667","1","15","<p>I want to find genes of fundamental metabolic processes in the <em>Phytophthora cinnamomi</em> genome. I'm using the genome sequence deposited in the NCBI and I'm searching ORF by ORF, using the ORF finder (<a href=""https://www.ncbi.nlm.nih.gov/orffinder/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/orffinder/</a>). Next, I use SMARTBLAST (<a href=""https://blast.ncbi.nlm.nih.gov/smartblast/?LINK_LOC=BlastHomeLink"" rel=""nofollow noreferrer"">https://blast.ncbi.nlm.nih.gov/smartblast/?LINK_LOC=BlastHomeLink</a>) to find similar organisms. \nHow do I know that the gene that I'm searching expresses proteins of fundamental metabolic processes, and if the organism is similar to mine?\nCan the best hits in SMARTBLAST tell me something about that?\nThanks</p>\n","7228","","6509","2020-02-17T19:25:31.357","2020-02-17T19:25:31.357","How can I find genes of fundamental metabolic processes for an organism?","<sequence-alignment><gene><blast><dna><sequence-homology>","0","2","",""
"11417","1","11426","","2020-02-15T20:28:43.300","0","17","<p>Plink documentation about <a href=""https://www.cog-genomics.org/plink/1.9/formats#bim"" rel=""nofollow noreferrer"">.bim</a> says the columns 5 and 6 are the Allele 1 and Allele 2 respectively. \nThe documentation about the <a href=""https://www.cog-genomics.org/plink/1.9/formats#ped"" rel=""nofollow noreferrer"">.ped</a> in turn, says ""...The first six fields are the same as those in a .fam file. The seventh and eighth fields are allele calls for the first variant in the .map file ('0' = no call); the 9th and 10th are allele calls for the second variant; and so on..""</p>\n\n<p>I've simulated a dataset as per:</p>\n\n<pre><code><span class=""math-container"">$ plink1.9 --simulate ds1.sim acgt --out ds1\n...\n$</span> head ds1.bim \n1       null_0  0       1       C       T\n1       null_1  0       2       A       C\n1       null_2  0       3       C       A\n1       null_3  0       4       T       A\n1       null_4  0       5       G       C\n1       null_5  0       6       T       C\n1       null_6  0       7       T       C\n1       null_7  0       8       G       C\n1       null_8  0       9       C       G\n1       null_9  0       10      T       A\n</code></pre>\n\n<p>Than I converted it to plain PED and MAP: </p>\n\n<pre><code><span class=""math-container"">$ plink1.9 --bfile ds1 --recode --out ds1\n...\n$</span> head ds1.ped &gt; ds1.head.ped\n$ vim ds1.head.ped\nper0 per0 0 0 2 2 T T A C A A T A C C T C T C C C....\n</code></pre>\n\n<p>Given the data above, why my .bim file shows ""C and T"" for the first variant while my .ped file shows ""T and T""?\nApparently they don't match as per described in the documentation (or more likely, I'm misinterpreting it)...</p>\n\n<p>Also, how do those 2 files related to each other? E.g.: How do I know that my Sample 1 has the SNP's null_3, null_4 or null_5, etc?</p>\n\n<p>I need to clarify those doubt because I want to fit a LightGBM model with the Genomic Data (can be plink or vcf). Not sure yet how to do. I was trying to convert the plink files to a CSV-style somehow, to be able to use in my model. Not sure if this is the way to go though... </p>\n\n<p>Thanks!</p>\n","6464","","","","2020-02-16T19:14:15.807","Why my bim file doesn't match to my ped file as the Plink documentation suggests?","<machine-learning><plink>","1","0","",""
"11418","1","","","2020-02-15T21:08:02.107","0","39","<p>Phasing a genome is the process of determining which variants lie on which copy of each chromosome. For example, position 5,430,500 might have an A on the paternal chromosome, but a T base on the maternal chromosome.</p>\n\n<p>A haplotype block is a section of the genome for which we know which haplotype has what variants. A completely phased genome has phase blocks that are the full length of each chromosome. Every variant on every chromosome can be assigned specifically to the maternal to or the paternal haplotype.</p>\n\n<p><strong>My question is</strong>, what can we learn from having completely phased genomes that we cannot from an unphased genome, or from a phased genome with short haplotype blocks? </p>\n","2085","","","","2020-02-19T04:20:52.253","Q: What analyses can I perform with a completely phased genome assembly?","<vcf><snp><phasing><variants>","2","0","",""
"11420","2","","9229","2020-02-16T02:07:04.343","0","","<p>One more thing. If empty ALT allele is not ""-"", you can change it such as ""*"", as follows.</p>\n\n<pre><code>    if ""*"" in record.alts:\n        ref = self.fasta.fetch(reference=record.chrom, start=record.pos - 1, end=record.pos)\n        record.pos = record.pos - 1\n        record.ref = f""{ref},{record.ref}""\n        record.alts = tuple(map(lambda alt: ref if alt == ""*"" else f""{ref},{alt}"", record.alts))\n</code></pre>\n","7230","","","","2020-02-16T02:07:04.343","","","","1","",""
"11421","1","","","2020-02-16T05:08:35.507","0","31","<p>I'm pretty new to bioinformatics and I have been looking for a way to simulate rna sequencing with editing, specifically A to G, but didn't find  anything i could use.\nI tried using flux simulator but didn't find a way to do this there.</p>\n\n<p>Do you have any recommendations?</p>\n\n<p>Thank you!</p>\n","7231","","","","2020-02-16T05:08:35.507","Simulating RNA Editing","<rna-seq><rna><simulated-data>","0","1","","1"
"11422","1","11423","","2020-02-16T05:48:22.880","1","33","<p>For easier management of a database of dna profiles, I'm creating a tool to perform one-to-one matching on two profiles with snp genotype information.</p>\n\n<p>Basically, something that works similar to GEDMatch's one-to-one matching feature.</p>\n\n<p>Determining the length of runs of half or fully matched snp's, including some allowance for a genotyping error rate, is the easy part.</p>\n\n<p>My question is about converting an snp run length to <a href=""https://en.wikipedia.org/wiki/Centimorgan"" rel=""nofollow noreferrer"">centimorgans</a>.</p>\n\n<p>I figure that the length needs to be adjusted by the likelihood of a change in the particular region of the chromosome.</p>\n\n<p>Since my dna profiles and the positions of snps are based on Genome Reference Consortium Human Build 37, I figure that his information could be in the reference build, directly or derivable in some way.</p>\n\n<p>Looking at the <a href=""https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.13/"" rel=""nofollow noreferrer"">GRCh37 download files</a> and the definitions of their formats, I'm unable to find such information.</p>\n\n<p>Is this information present in GRCh37, or can it be downloaded from another reference source, and from which field in the dataset can it be retrieved or derived?</p>\n","7226","","","","2020-02-16T11:41:47.583","How to calculate Centimorgan lengths from run lengths in one-to-one dna profile comparison","<snp><dna>","1","0","",""
"11423","2","","11422","2020-02-16T11:34:40.897","2","","<p>Try having a look at this link:</p>\n\n<p><a href=""http://bochet.gcc.biostat.washington.edu/beagle/genetic_maps/plink.GRCh37.map.zip"" rel=""nofollow noreferrer"">http://bochet.gcc.biostat.washington.edu/beagle/genetic_maps/plink.GRCh37.map.zip</a></p>\n\n<p>If you unzip the file, there is a file for each chromosome (1:22) and then 3 for the x chromosome. The first column of each file details which chromosome the file corresponds to, the 3rd column gives you the cumulative genetic distance in cM and the 4th column gives you the genomic coordinate (position) in bp. </p>\n\n<p>You can then calculate the distance in cM between two positions from this data by subtracting the cM positions between the two positions you are interested in. </p>\n","3694","","3694","2020-02-16T11:41:47.583","2020-02-16T11:41:47.583","","","","0","",""
"11424","2","","11388","2020-02-16T14:44:26.863","1","","<p>You have accepted your own answer but it doesn't seem to do what you ask as it doesn't add the sequential identifier to each sequence. Maybe try this?</p>\n\n<pre><code>awk -v rec=0 '{\n    if(<span class=""math-container"">$0 ~ ""^&gt;""){\n        rec++\n        sub(""&gt;"", ""&gt;RNA"" rec ""#"", $0)\n        sub(""_.*_"", ""/"", $0);\n    } \n    print $</span>0\n}' yourfile.fa\n</code></pre>\n","339","","","","2020-02-16T14:44:26.863","","","","0","",""
"11425","2","","11412","2020-02-16T17:02:25.473","1","","<p>The definition of a GMO for example UK GMO regulatory bodies (HSE) is strictly the artificial introduction of foreign genetic material using targeted genetic engineering, such as splicing,  genome integration etc... Attenuation for example by serial passage the basis of the yellow fever vaccine, does not count as a GMO. </p>\n\n<p>The simple answer is look at phylogenetic trees between the GMO strain against the total distribution of isolates. If large regions have been spliced the 'silent mutation rate' looks very different,.because the GMO parent is from a very different environment to the recipient.  The switch in replication rate affects the numbers of 'silent mutation', which is fast in RNA viruses.</p>\n\n<p>If the GMO is a point mutation, ie limited amino acid change then I've actually done this with dengue virus and it is fairly easy because the novel mutation make the GMO 'stick out'.of the phylogeny against all natural mutation patterns. At this point the outlier is tracked back to Genbank where the relavant publication defines the engineered virus.</p>\n\n<p>If you are inferring a conspiracy theory that is a different process and a different discussion, particularly if it is Ebolavirus which is a Category 4 pathogen. The investigation is again within the phylogeny, but in this case the answer is always the isolate is a natural zoonose from a sylvatic (wild) transmission cycle which has spilled over into humans.</p>\n","3848","","3848","2020-02-16T20:42:21.050","2020-02-16T20:42:21.050","","","","5","",""
"11426","2","","11417","2020-02-16T19:14:15.807","2","","<p>Responded in the plink2-users group (<a href=""https://groups.google.com/forum/#!topic/plink2-users/zgJxdXxvdLo"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/plink2-users/zgJxdXxvdLo</a> ).</p>\n","472","","","","2020-02-16T19:14:15.807","","","","0","",""
"11427","1","11467","","2020-02-17T09:38:11.897","2","158","<p>I want to compare RNA-Seq datasets obtained from the TCGA to investigate how my gene of interest is implicated in different types of cancer.</p>\n\n<p>I'm trying to download the data from the GDC Data Portal (<a href=""https://portal.gdc.cancer.gov"" rel=""nofollow noreferrer"">https://portal.gdc.cancer.gov</a>), but it is not user-friendly.</p>\n\n<p>Can somebody give me some initial directions? I found a bioconductor package that facilitates access to the TCGA (<a href=""https://bioconductor.org/packages/release/bioc/html/TCGAbiolinks.html"" rel=""nofollow noreferrer"">https://bioconductor.org/packages/release/bioc/html/TCGAbiolinks.html</a>), but first of all, I would like to understand how the files are organized, the nomenclature and how to inspect them.</p>\n\n<p>thank you very much, Fabiano</p>\n","6509","","6509","2020-02-17T10:17:41.787","2020-02-29T01:21:18.073","How to initiate RNA-Seq analysis of TCGA files?","<rna-seq>","2","2","",""
"11428","1","","","2020-02-17T10:10:35.277","2","39","<p>I'm trying to use cellranger to <code>mkfastq</code> and then <code>count</code> and <code>aggregate</code> single-cell data. This data was not generated using 10x Genomics RNASeq. </p>\n\n<p>When I run <code>count</code>, cellranger fails to auto id the chemistry and crashes out. Is it possible to wrangle the data into the cellranger method or should I look elsewhere. I'd ideally like to use the same method for all my single-cell data but not all of it is 10x.</p>\n","7232","","","","2020-02-17T20:35:40.230","Using cellranger for non-10x data","<single-cell><cellranger>","1","4","",""
"11429","2","","11418","2020-02-17T11:52:51.137","1","","<p>Some population genetic inference methods require the data to be phased because they look for long stretches of matching alleles between individuals, rather than just matching genotypes. Here, 'genotype' data is un-phased, and 'allele' data is phased. </p>\n\n<p>This is because there is a degree of ambiguity when it comes to matching heterozygous positions - if the data is un-phased, you don't know if 2 individuals share mutations on the same chromosome (here, I mean different copies of the same chromosome - like either copy of chromosome 1, for example). This is important, because the fact that the mutation is shared on the same chromosome means that they are much more likely to share recent common ancestry with one another. </p>\n\n<p>Let's say we have genotype data for two individuals (where the number corresponds to the number of non-reference alleles present):</p>\n\n<pre><code>Ind 1\n\n0 ----- 1 ----- 2 ----- 0\n\nInd 2\n\n0 ----- 1 ----- 2 ----- 0\n</code></pre>\n\n<p>If were looking at unphased genotype data only, we know the alleles match exactly in the 1st, 3rd and 4th positions, because they are homozgyous (either 0 or 2 copies of the non-referencee allele), but because the second position is heterozygous, we can't be sure which chromosome the non-reference allele is on. This is because the haplotype configurations for individual 1 could be either </p>\n\n<pre><code>0 ----- 0 ----- 1 ----- 0 Chromosome1a\n0 ----- 1 ----- 1 ----- 0 Chromosome1b\n</code></pre>\n\n<p>or</p>\n\n<pre><code>0 ----- 1 ----- 1 ----- 0 Chromosome1a\n0 ----- 0 ----- 1 ----- 0 Chromosome1b\n</code></pre>\n\n<p>i.e., we don't know which chromosome the non-reference allele at position 2 is on. </p>\n\n<p>If we then phased the data, and found that the non-reference allele was on chromosome 1a for Individual 1, and chromosome 1b for Individual 2 there is less evidence of recent shared ancestry than otherwise. I think this is more or less the approach that 23andme takes when they are inferring your relationship to other individuals in the dataset. After writing this, I realised there is a much better description of what I just said on the <a href=""https://www.23andme.com/en-gb/ancestry-composition-guide/"" rel=""nofollow noreferrer"">23andme website</a>! </p>\n\n<p>For example, the above applies to the <a href=""https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002453"" rel=""nofollow noreferrer"">ChromoPainter/fineSTRUCTURE</a> algorithms. This means that they are able to detect more subtle genetic differences between individuals than if you were to use non-phased data.</p>\n\n<p>This is a slightly niche example, and there occasions where you need phased data, but this is the one I am most familiar with :). Off the top of my head, methods such as <a href=""https://www.nature.com/articles/ng.3015"" rel=""nofollow noreferrer"">Multiply Sequential Markovian Coalescent</a> (MSMC), <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=6&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwivhv-gwtjnAhVXTxUIHUutCcUQFjAFegQIBRAB&amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41588-019-0484-x&amp;usg=AOvVaw1y_rNgh_j-Wi_My5iXUrp4"" rel=""nofollow noreferrer"">RELATE</a>, and <a href=""https://www.nature.com/articles/nature01140"" rel=""nofollow noreferrer"">Extended Haplotype Homozygosity</a> all require the data to be phased. </p>\n","3694","","","","2020-02-17T11:52:51.137","","","","0","",""
"11430","1","","","2020-02-17T13:48:28.077","1","30","<p>I want to remove variant including in copy number changed area by maftools but I am permanently get this error</p>\n\n<pre><code>&gt; tcga.ab.3009.het = inferHeterogeneity(maf = laml_res, tsb = tsb, segFile = seg, vafCol = 'T_VAF')\nCN data for following samples not found. Ignoring them ..\n [1] ""LP6008337-DNA_A07"" ""LP6008460-DNA_F02"" ""LP6008460-DNA_G03"" ""LP6008460-DNA_D01"" ""LP6008202-DNA_B03"" ""LP6008460-DNA_A04""\n [7] ""LP6005935-DNA_G02"" ""LP6008334-DNA_B02"" ""LP6008269-DNA_F06"" ""LP6008336-DNA_F02"" ""LP6005334-DNA_H01"" ""LP6005500-DNA_D03""\n[13] ""LP6008334-DNA_A04"" ""LP6008336-DNA_G01"" ""LP6008336-DNA_H02"" ""LP6007600""        \nProcessing LP6008334-DNA_D02..\nError in data.table::foverlaps(x = tsb.dat, y = seg, by.x = c(""Chromosome"",  : \n  The last two columns in by.x should correspond to the 'start' and 'end' intervals in data.table 'x' and must be integer/numeric type.\n\n\n&gt; head(dat.tsb[1:3,1:6])\n   Tumor_Sample_Barcode Chromosome Start_Position End_Position Reference_Allele Tumor_Seq_Allele2\n1:    LP6008334-DNA_A03       chr5       70858261     70858261                -                 A\n2:    LP6008334-DNA_A03      chr10      115987869    115987869                A                 -\n3:    LP6008334-DNA_A03      chr16       29859285     29859287              ACC                 -\n&gt; \n</code></pre>\n\n<p>Can one please help me?</p>\n\n<p>Thanks</p>\n","3962","","","","2020-02-17T13:48:28.077","Permanent error with maftools","<r>","0","2","",""
"11431","1","","","2020-02-17T15:14:13.103","2","44","<p>I collected my own denovo library using many different tools Pipeline (structure-based ) and I updated all the headers corresponding to RepeatMasker format, to obtain Repeat coverage in the genome and .tbl annotation.</p>\n\n<p>when I run repeatmasker the annotation results assume to be Homosapien, where Homo annotation table ( .tbl) is limited as shown below Table (1)</p>\n\n<p>Actually my denovo repeat library consists of more than what is in the table above, example, rolling circles, Copia, Gypsy etc.. </p>\n\n<pre><code>RepeatMasker -Lib myOwnLibrary.fa Mygenome.fa\n</code></pre>\n\n<p>Table (1)</p>\n\n<p><a href=""https://i.stack.imgur.com/2S9Dq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2S9Dq.png"" alt=""enter image description here""></a></p>\n\n<p>My question is how can I run RepeatMasker with my own library using -lib Flag and result like the table (2) below, </p>\n\n<p>the table below assume to be vertebrates genome, this is my aim. </p>\n\n<pre><code>RepeatMasker -Species Vertebrates Mygenome.fa\n</code></pre>\n\n<p>Table (2)</p>\n\n<p><a href=""https://i.stack.imgur.com/b97Gw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b97Gw.png"" alt=""enter image description here""></a></p>\n\n<p>Regards</p>\n","3332","","3332","2020-02-19T15:51:37.950","2020-02-26T16:54:24.577","RepeatMasker annotation","<annotation><repeat-elements><repeatmasker><repeat>","1","2","","0"
"11432","1","","","2020-02-17T19:22:17.310","1","27","<p>I have a file with Ensembl gene and Transcript IDs :</p>\n\n<blockquote>\n  <p><a href=""https://i.stack.imgur.com/Dg9J0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dg9J0.png"" alt=""enter image description here""></a></p>\n</blockquote>\n\n<p>I want to retain only the first entires from both the column so that the output looks like this:</p>\n\n<pre><code>ENSG00000003137 ENST00000001146\nENSG00000003402 ENST00000309955\n</code></pre>\n\n<p>and ignore the rest. \nHave tried awk <code>awk '!a[$1$2]++'</code> and versions of it. Nothing works.\nKindly suggest some way to get the desired output.</p>\n\n<p>regards.</p>\n","7236","","3541","2020-02-17T19:24:42.540","2020-02-18T18:02:08.557","select unique names while removing the duplicates from a column","<ensembl>","2","0","",""
"11433","2","","11428","2020-02-17T20:35:40.230","1","","<p>Possible?  Sure.  Your biggest problem is that 10x is expecting barcodes from its whitelist.  I used to alter the python scripts to accept a 'new' kind of chemistry where the barcodes and umi lengths matched what I had, but I found that my modifications stopped working when the version updated,  and I couldn't figure it out, so I started modifying fastqs instead.</p>\n","2388","","","","2020-02-17T20:35:40.230","","","","2","",""
"11434","1","","","2020-02-18T09:09:26.197","1","44","<p>I am running kraken 2.0.8 beta to do taxonomic classification of micorbiota data coming from a MinION run based on the 16S gene. To do so I am using the pre-bult kraken SILVA  database. I managed to get the standard kraken output format but now I would like to replace the taxonomic ID column with  scientific names. According to the manual (<a href=""https://ccb.jhu.edu/software/kraken2/index.shtml?t=manual"" rel=""nofollow noreferrer"">https://ccb.jhu.edu/software/kraken2/index.shtml?t=manual</a>) I should run the command: </p>\n\n<p>kraken2 --use-names</p>\n\n<p>but it gives the error: </p>\n\n<p>command kraken2 not found</p>\n\n<p>I also tried variants such as: </p>\n\n<p>./kraken 2 --use-names</p>\n\n<p>but still it gives an error.  </p>\n\n<p>I also tried without kraken2 and thus, </p>\n\n<p>--use-names </p>\n\n<p>but it gives this error: </p>\n\n<p>--use-names command not found </p>\n\n<p>Do you have any idea how I can proceed? </p>\n\n<p>In the picture you can see that everything works to obtain the standard kraken output for barcode 76. But if I try to implement the --use-names command then it does not work  </p>\n\n<p><a href=""https://i.stack.imgur.com/hWvgY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hWvgY.png"" alt=""ERROR THAT I GET IF I TRY TO USE THE --USE-NAMES COMMAND""></a>  </p>\n\n<p>I solved the problem: it was a matter of the position of the --use-names command. By changing position (see picture) it worked.\n<a href=""https://i.stack.imgur.com/vnCGH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vnCGH.png"" alt=""problem solved picture""></a></p>\n","7203","","7203","2020-02-18T16:48:39.640","2020-02-18T16:48:39.640","Scientific names of taxids with kraken 2.0.8 beta, command kraken2 --use-names error","<database>","0","4","",""
"11435","1","11438","","2020-02-18T09:52:42.540","2","54","<p>I will use GATK for SNP calling (<a href=""https://gatk.broadinstitute.org/hc/en-us/articles/360035531412-HaplotypeCaller-in-a-nutshell"" rel=""nofollow noreferrer"">HaplotypeCaller</a>). I need to feed the interval file in the command, otherwise I get errors (even though I want to use the whole genome, not a subset  - btw it's not the point of the question, so will not elaborate). </p>\n\n<p>The <a href=""https://gatkforums.broadinstitute.org/gatk/discussion/1319/collected-faqs-about-interval-lists"" rel=""nofollow noreferrer"">interval file</a> for GATK can be for example a <code>bed</code> file, with 3 columns: <code>chr_name  chr_start  chr_end</code>. I don't have this file, but have the genome and the reads. </p>\n\n<p>To obtain the intervals per chr/scaf, I proceeded this way: </p>\n\n<ul>\n<li>converted the alignment to interval file, using <code>bam2bed</code>    (<a href=""https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/bam2bed.html"" rel=""nofollow noreferrer"">BEDOPS</a>)</li>\n<li>extract the columns $1,2,3 (using <code>cut</code>)</li>\n<li>get the min &amp; max coordinates per chr/scaff with a small R script:</li>\n</ul>\n\n<pre><code>bd &lt;- read.table('all.bed', h=FALSE)\nbd_min &lt;- aggregate( bd$V3 ~ bd$V1 , bd, function(x) min(x))\nbd_max &lt;- aggregate( bd$V2 ~ bd$V1 , bd, function(x) max(x))\nbd_coord &lt;- merge(bd_min, bd_max, by='bd$V1')\nwrite.table(bd_coord, file='coords.bed', sep='\t', quote=FALSE, col.names=FALSE, row.names=FALSE)\n</code></pre>\n\n<p>These should thus represent the mapped intervals, conceptually same as not feeding the intervals at all (= considering all the sequences).</p>\n\n<p>The ideas for this:</p>\n\n<ul>\n<li>in the bed file: <code>$1</code> is be the feature, <code>$2</code> the start position, <code>$3</code> the end position</li>\n<li>so, I need to get the min of <code>$2</code> for each unique item in <code>$1</code>(for the start coord) and the max of <code>$3</code> for each item in <code>$1</code> (for the end coord)</li>\n</ul>\n\n<p>The output looks plausible:</p>\n\n<pre><code>$ cat coords.bed | head -3\nBla_chrm1   678 43860826\nBla_chrm10  181 20381540\nBla_chrm11  343 20367560\n</code></pre>\n\n<p>My question here:</p>\n\n<ol>\n<li>is this a correct way to proceed?</li>\n<li>is there a standardized way to perform this?</li>\n</ol>\n\n<p>My main concern is that I will have to re-build this ""coordinate"" files for every GATK run, because I think the coordinates per chr/scaf will shift, even slightly, at each GATK run with different datasets.</p>\n\n<p>My other minor concern, is that this approach is a bit slow (given the 160M lines in the current bed file), so an unix-tools solution will also be accepted. I tried to compute myself the max/min using awk (e.g. <code>awk '$3&gt;max[$1]{max[$1]=$3; row[$1]=$0} END{for (i in row) print row[i]}'</code>), but I get different results than with the R approach... </p>\n","294","","294","2020-02-19T05:46:29.210","2020-02-19T05:46:29.210","Get start and end coordinates per chromosome","<file-formats><bed><gatk>","2","0","",""
"11436","2","","11432","2020-02-18T09:59:51.153","0","","<p>That sound like a job for a perl one-liner:</p>\n\n<pre><code>perl -ane 'unless(exists <span class=""math-container"">$seen{$F[0]}){print $_; $seen{$</span>F[0]}++}'\n</code></pre>\n\n<p>Explanation:<br/> \nThe input is read line-wise and printed back to the command line if the ID in the first column has not been seen before. </p>\n\n<ul>\n<li><code>perl -ane '</code> reads input line-wise and splits fields to array @F</li>\n<li><code>unless(exists $seen{$F[0]}){</code> checks if ID exists in hash %seen and continues if that is not the case</li>\n<li><code>print $_;</code> prints the original input to STDOUT </li>\n<li><code>$seen{$F[0]}++}'</code> adds the ID to the hash %seen </li>\n</ul>\n","668","","","","2020-02-18T09:59:51.153","","","","0","",""
"11437","1","","","2020-02-18T10:05:34.167","-1","41","<p>I have generated rooted species tree with 1000 bootstraps by RAxML. Further, I need to generate ultrametric tree for the same. Therefore, Please suggest me how to generate ultrametric tree from the rooted species tree.</p>\n\n<p>Thank you.</p>\n","4712","","3848","2020-02-20T13:13:05.850","2020-02-20T13:13:05.850","How to generate ultrametric phylogenetic trees?","<python><phylogenetics><phylogeny><linux><perl>","1","4","",""
"11438","2","","11435","2020-02-18T13:25:38.093","0","","<p>You can try this, since you have the bam file, all of them are aligned to the same reference genome (I hope)(and apologies for the awful awk.. been a while since I used it):</p>\n\n<pre><code>samtools view -H some.bam | awk '{if(<span class=""math-container"">$1~/@SQ/){print $2""\t1\t""$</span>3}}' | sed 's/[SL]N://g' &gt; coords.bed\n</code></pre>\n\n<p>For the human genome I got this:</p>\n\n<pre><code>chr1    1   195471971\nchr2    1   182113224\nchr3    1   160039680\nchr4    1   156508116\nchr5    1   151834684\nchr6    1   149736546\nchr7    1   145441459\nchr8    1   129401213\nchr9    1   124595110\n</code></pre>\n\n<p>What I did is to get the chromosome length, and make every line start at 1.</p>\n\n<p>If you use R, you can do:</p>\n\n<pre><code>library(Rsamtools)\nCL = scanBamHeader(&lt;bam file&gt;)[[1]]$targets\nwrite.table(data.frame(names(CL),start=1,end=CL),\n""coords.bed"",row.names=FALSE,quote=F,col.names=FALSE,sep=""\t"")\n</code></pre>\n","5077","","","","2020-02-18T13:25:38.093","","","","5","",""
"11439","2","","11435","2020-02-18T13:58:32.173","1","","<p>Not sure if I completely understood your question, but if you want simply an interval of your entire genome  you could do instead:</p>\n\n<pre><code>samtools faidx MyGenome.fasta\n</code></pre>\n\n<p>Obviously that means that you need access to that, not sure if you have.\nThe resulting fai file contains a format similar to that:</p>\n\n<pre><code>000000F 33203223        94      60      61\n000001F 28828106        33756799        60      61\n000002F 27810542        63065468        60      61\n</code></pre>\n\n<p>Where the first column is your chromosome and the second one your length.\nI guess you could then just do a <code>awk '{OFS=FS=""\t""},{print $1,""0"",$2-1}'</code> to get your file.</p>\n","1709","","","","2020-02-18T13:58:32.173","","","","3","",""
"11440","2","","11437","2020-02-18T16:16:24.727","0","","<p>An ultrametric tree is simply a global molecular clock and can be easily generate in PAML using a single evolutionary rate. You take you RAxML tree in phylip format (PAML might do nexus format know), and click the ""clock function"". PAML will do the rest.</p>\n\n<p>You can alternatively generate it de novo via UPGMA neighbor-joining tree which is found in the MEGAX package (which is probably easier and considered a technical definition of an ultrametric tree). </p>\n\n<p>A global molecular clock (as opposed to a local molecular clock) is quite a specific request, but if you performed a likelihood ratio test, it would identify ""clock-like"" behaviour.</p>\n","3848","","3848","2020-02-18T16:58:39.830","2020-02-18T16:58:39.830","","","","0","",""
"11442","1","","","2020-02-18T16:42:00.117","0","24","<p>I have three types of transcripts for <em>Rosa chinensis</em> ""Old Blush"" homozygous genome v2.0 from GDR genome browser.</p>\n\n<pre><code>RcHm_v2.0_CPg0501991\nRcHm_v2.0_Chr0c12g0499591  or RcHm_v2.0_Chr1g0313011\nRcHm_v2.0_MTg0498341\n</code></pre>\n\n<p>Does anyone know what this means. At least ""CPg"" and ""Mtg"". I know ""Chr"" means chromosome. And there is ""1"" at each and every transcript. But it is not separated using anything, like people do to mention the transcript number. I will be grateful if someone can help.\nThank you.</p>\n","856","","","","2020-02-18T16:42:00.117","What are the meanings of these transcript ids?","<proteins><transcriptome><rna><dna><nomenclature>","0","2","",""
"11444","2","","11432","2020-02-18T18:02:08.557","0","","<p>Keep it simple:</p>\n\n<pre><code>awk '!a[$1]++' file.txt \n</code></pre>\n\n<p>If you have a different command line separator you need to specify with the <code>-F</code> option.</p>\n\n<p>Space</p>\n\n<pre><code>awk -F' ' '!a[$1]++' text.txt \n</code></pre>\n\n<p>Tab</p>\n\n<pre><code>awk -F'\t' '!a[$1]++' text.txt \n</code></pre>\n\n<p>If you don't have the first column in order maybe should be a good idea sort first. </p>\n\n<pre><code>sort text.txt | awk -F' ' '!a[$1]++'\n</code></pre>\n\n<p>The name of the array in this case is <code>a</code>. There is a better explanation in <a href=""https://superuser.com/a/1107659"">this post</a>.</p>\n","4282","","","","2020-02-18T18:02:08.557","","","","0","",""
"11445","1","","","2020-02-18T18:00:16.417","1","42","<p>I'm new here. I need to construct a phylogenetic tree from 16s Sequence data (merged 151 bp) and color the branches/nodes based on my experimental conditions. From <a href=""https://towardsdatascience.com/molecular-phylogenetics-using-bio-phylo-57ce27492ee9"" rel=""nofollow noreferrer"">this blog</a>, it looks like I need a .phy file to make the tree. I want to use Biopython for this since I'm reasonably comfortable with python. </p>\n\n<p>I have a csv file with the 16s sequences, the phylogenetic information, and the ""name"" of the sequence, and the experimental conditions. How can I make a .phy file from this? </p>\n\n<p>I would appreciate any help. \nThank you. </p>\n","7244","Kaumz","3848","2020-02-20T13:17:09.977","2020-02-20T13:17:09.977","How to create a .phy file for constructing a phylogenetic tree?","<phylogenetics><biopython><phylogeny>","2","0","",""
"11446","2","","11445","2020-02-18T19:39:14.560","1","","<p>You simply use Biopython's SeqIO and AlignIO.\nThe documentation is <a href=""https://biopython.org/wiki/AlignIO"" rel=""nofollow noreferrer"">https://biopython.org/wiki/AlignIO</a> \netc ...\nThe format you seek is phylip</p>\n","3848","","","","2020-02-18T19:39:14.560","","","","4","",""
"11448","2","","11445","2020-02-19T00:29:43.177","1","","<p>Python works great for tying everything together and file manipulations.  I find stepping out of Python, using system calls, best for alignment and tree building.  For example:</p>\n\n<pre><code>os.system(""mafft --auto {} &gt; {}"" .format(""unaligned_16s_seq_in_file.fasta"", ""alignment_out_file.fasta""))\n\nos.system(""raxml -f a -s {} -p 12345 -x 12345 -# 100 -m GTRCAT -n {}"" .format(""alignment_out_file.fasta"", ""tree_out_file.tre""))\n</code></pre>\n","6343","","","","2020-02-19T00:29:43.177","","","","0","",""
"11449","1","","","2020-02-19T01:24:37.053","1","38","<p>I'm new to bioinformatic .. i'm reading a paper and what i got that they made a classification for the people if they have a sequence of a disease or not ... </p>\n\n<p><code>The research focused on two different class of DNA which are Hepatitis B Virus\n(HBV) Genotype B nucleotide sequence and Non-HBV nucleotide sequences</code></p>\n\n<p>How can i get the data set or how can search to find it ? \nDoes data sets will be sequences of illness and not illness people ? </p>\n","7248","","3848","2020-02-21T17:30:49.497","2020-02-21T18:29:48.603","How can i get data set of illness and not illness people?","<sequencing><phylogenetics><phylogeny><ncbi><nucleotide-models>","1","2","",""
"11450","1","","","2020-02-19T01:37:53.527","1","26","<p>I found <a href=""https://bioinformatics.stackexchange.com/q/3510/57"">a very good link</a> to generate bwt suffix array in BW based tools.</p>\n\n<p>@user172818 provides a nice code. could anyone tell me how to store SA elements' original position in the original string?  </p>\n","7246","","57","2020-02-19T10:18:12.010","2020-02-19T10:18:12.010","bwt suffix array construction","<bwa>","0","0","",""
"11451","2","","10689","2020-02-19T03:51:06.657","1","","<p>I recommend using <code>densitree</code>: <a href=""https://www.cs.auckland.ac.nz/~remco/DensiTree/"" rel=""nofollow noreferrer"">https://www.cs.auckland.ac.nz/~remco/DensiTree/</a></p>\n\n<p>And <code>rwty</code>: <a href=""https://github.com/danlwarren/RWTY"" rel=""nofollow noreferrer"">https://github.com/danlwarren/RWTY</a></p>\n\n<p>Densitree plots the superimposed set of trees in a heatmap-like format: <a href=""https://i.stack.imgur.com/6LcOe.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6LcOe.jpg"" alt=""enter image description here""></a></p>\n\n<p>And <code>rwty</code>, a very powerful <code>R</code> package that allows visualisation of treespace in an NMDS format (among other methods): <a href=""https://i.stack.imgur.com/S2fsR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2fsR.png"" alt=""enter image description here""></a></p>\n","3381","","3381","2020-02-25T23:36:30.260","2020-02-25T23:36:30.260","","","","1","",""
"11452","2","","11418","2020-02-19T04:20:52.253","0","","<p>A situation where having phasing data is useful is with allele-specific protein binding.\nSome individuals will have a heterozygous mutation within the binding site of a particular protein.\nIf you perform a standard chromatin immunoprecipitation (ChIP) sequencing assay, you'll find sequencing reads around the protein's binding site, but you won't know which allele they'll come from since sequencing is an ensemble measurement.</p>\n\n<p>However, if you have phasing information, you can measure whether this specific mutation has an effect on that protein's binding, and get some insight into the function of that mutation in the context of that individual's cells.</p>\n","240","","","","2020-02-19T04:20:52.253","","","","0","",""
"11453","1","","","2020-02-19T05:16:37.840","1","15","<p>How is it possible to get the SwissProt Proteins into my Jbrowser in the same way like shown below?</p>\n\n<p><a href=""https://i.stack.imgur.com/jbxR0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jbxR0.png"" alt=""enter image description here""></a></p>\n\n<p>Thank you in advance,</p>\n","2477","","","","2020-02-19T05:16:37.840","SwissProt Proteins into my Jbrowser","<annotation><sequence-annotation><functional-annotation><genome-annotation>","0","0","",""
"11454","1","","","2020-02-19T11:03:25.273","2","29","<p>I am an avid user of Snakemake. Recently we have been refreshing our pipelines and I saw that a cluster.json file is no longer the recommended way to store the cluster configuration.</p>\n\n<p>I used to start my pipelines like this:</p>\n\n<pre><code>snakemake --cluster-config cluster.json --cluster ""qsub -W umask=002 -d cluster_outputs  -l nodes={cluster.nodes}:ppn={cluster.ppn} -N {rule}"" --jobs 245 --use-conda --rerun-incomplete --keep-going --latency-wait 60\n</code></pre>\n\n<p>With the cluster.json looking like this:</p>\n\n<pre><code>{\n    ""__default__"" :\n    {\n        ""cluster""   : ""qsub"",\n        ""jobs""      : 70 ,\n        ""nodes""     : 1,\n        ""ppn""       : 1\n    },\n    ""rsem_quantify"" :\n    {\n        ""ppn""       : 8\n    },\n    ""star_map_human"" :\n    {\n        ""ppn""       : 4\n    },\n}\n</code></pre>\n\n<p>Now, I understand that some of this goes in to the new profiles.yaml like this:</p>\n\n<pre><code>cluster: qsub\njobs: 70\n</code></pre>\n\n<p>But how do I set the ppn values per rule? And how do I set the other configuration options into profile.yaml (like: snakemake ""qsub -W umask=002 -d cluster_outputs  -l nodes={cluster.nodes}:ppn={cluster.ppn} -N {rule}"", etc.)</p>\n\n<p>Any examples are appreciated, <a href=""https://github.com/snakemake-profiles"" rel=""nofollow noreferrer"">https://github.com/snakemake-profiles</a> is not really helping</p>\n","3964","","","","2020-02-19T19:38:12.100","Snakemake: Migrating from deprecated cluster.json to new profiles.yaml","<snakemake>","1","0","",""
"11455","2","","11431","2020-02-19T15:05:52.753","0","","<p>After long years of just conceptual knowledge of what the software does, I was dealing with <code>RepearMasker</code> this week. So, I am not entirely sure I am correct, but this is how I understand it.</p>\n\n<p>The two flags you explored serve for different scenarios:</p>\n\n<ul>\n<li><code>-species</code> - if the taxa/species is present in the repeat database (<code>repBase</code> or <code>Dfam</code>)</li>\n<li><code>-lib</code> - if you want to give repeatModeler something different.</li>\n</ul>\n\n<p>In your case it seems like your library is completely missing all LINEs, SINEs and a few others. What you can do is to get from the database all repetitions of the corresponding taxa and catinate it with the repeats you are interested in (the one you supplied in the <code>-lib</code> argument in the first command).</p>\n\n<pre><code>util/queryRepeatDatabase.pl -species Vertebrates &gt; flavoured_vertebrate_repetitions.lib\ncat myOwnLibrary.fa &gt;&gt; flavoured_vertebrate_repetitions.lib\nRepeatMasker -lib flavoured_vertebrate_repetitions.lib Mygenome.fa\n</code></pre>\n\n<p>This should produce annotation similar to the bottom case, but also using the repetitions you manually added.</p>\n\n<p>P.S. <code>util/queryRepeatDatabase.pl</code> is a script to query the database of repeats and it is shipped together with <code>repeatModeler</code>.</p>\n\n<p>-- edit --</p>\n\n<p>The TE classification in the summary file of repeatMasker is based on headers of the repetitive elements in the fasta file you specify in to <code>-lib</code> argument. So if you want to get the second-table-like, you need to somehow reclassify the repeat library to contan repBase-like classification of TEs. The explanation of lib argument from the <a href=""https://www.animalgenome.org/bioinfo/resources/manuals/RepeatMasker.html"" rel=""nofollow noreferrer"">manual</a>:</p>\n\n<blockquote>\n  <p>The recommended format for IDs in a custom library is:</p>\n  \n  <p>>repeatname#class/subclass or simply</p>\n  \n  <p>>repeatname#class</p>\n  \n  <p>In this format, the data will be processed (overlapping repeats are\n  merged etc), alternative output (.ace or .gff) can be created and an\n  overview .tbl file will be created. Classes that will be displayed in\n  the .tbl file are 'SINE', 'LINE', 'LTR', 'DNA', 'Satellite', anything\n  with 'RNA' in it, 'Simple_repeat', and 'Other' or 'Unknown' (the\n  latter defaults when class is missing). Subclasses are plentiful. They\n  are not all tabulated in the .tbl file or necessarily spelled\n  identically as in the repeat files, so check the RepeatMasker.embl\n  file for names that can be parsed into the .tbl file.</p>\n</blockquote>\n","57","","57","2020-02-26T16:54:24.577","2020-02-26T16:54:24.577","","","","5","",""
"11456","2","","11454","2020-02-19T19:38:12.100","1","","<p>It looks like you can just bundle your cluster config in its own YAML file, and reference that from within a profile YAML.  Here's a clumsy little example using a fake job submission script: <a href=""https://github.com/ressy/example-snakemake-profiles"" rel=""nofollow noreferrer"">https://github.com/ressy/example-snakemake-profiles</a></p>\n\n<hr>\n\n<p>The <a href=""https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#cluster-configuration-deprecated"" rel=""nofollow noreferrer"">current documentation</a> makes it sound like cluster configuration support is going away and being replaced by profiles:</p>\n\n<blockquote>\n  <p>While still being possible, cluster configuration has been deprecated by the introduction of Profiles.</p>\n</blockquote>\n\n<p>...but from those example profiles, that might be misleading; here's an example template for a profile YAML that references a separate cluster-config YAML:</p>\n\n<p><a href=""https://github.com/Snakemake-Profiles/generic/blob/master/%7B%7Bcookiecutter.profile_name%7D%7D/config.yaml"" rel=""nofollow noreferrer"">https://github.com/Snakemake-Profiles/generic/blob/master/%7B%7Bcookiecutter.profile_name%7D%7D/config.yaml</a></p>\n\n<pre><code>...\ncluster-config: ""$((INSTALDIR))/overwrite_cluster_param.yaml"" #abs path\n...\n</code></pre>\n\n<p>That cluster-config YAML has the same default and per-rule entries just your JSON version does:</p>\n\n<p><a href=""https://github.com/Snakemake-Profiles/generic/blob/master/%7B%7Bcookiecutter.profile_name%7D%7D/overwrite_cluster_param.yaml"" rel=""nofollow noreferrer"">https://github.com/Snakemake-Profiles/generic/blob/master/%7B%7Bcookiecutter.profile_name%7D%7D/overwrite_cluster_param.yaml</a></p>\n\n<pre><code>...\n# specific parameters for certain rules, which need more time/memory\n\n#run_assembler:\n#  queue: bigmem\n#  time: 100\n#   threads and memory definen in config file\n</code></pre>\n\n<p>So I think you can just set up a pair of YAML files accordingly including options for your rsem_quantify and star_map_human rules.  From my own test it looks like it handles the cluster argument templating just as it did before when using a profile.  (If this is correct we should nag them to make the new documentation more explicit though.)</p>\n","4980","","","","2020-02-19T19:38:12.100","","","","0","",""
"11457","1","","","2020-02-19T22:32:03.520","2","35","<p>This isn't quite my speciality, but I've been tasked with finding any evidence of Mobile Genetic Elements (like transposable elements, retrotransposons, bacteriophages, etc.) within some WGS samples. </p>\n\n<p>Is there a ""definitive"" database used with sequences? How ""species-specific"" would this be?</p>\n\n<p>Furthermore, are there standard algorithms used to find these MGEs? I suspect we don't have a comprehensive database of all possible MGEs, but it may be possible to query against the sequences we do indeed have (or detect any repetitive elements within the genome). </p>\n","1770","","","","2020-02-21T22:59:02.440","Is there a ""definitive"" database for Mobile Genetic Elements?","<blast><repeat-elements><transposable-elements><bacteria>","2","0","",""
"11458","2","","11449","2020-02-20T05:00:26.107","0","","<p>The data set will be available on <a href=""https://www.ncbi.nlm.nih.gov/"" rel=""nofollow noreferrer"">Genbank</a>, you simply search for the ""authors surname and HBV"" and their entire data set should be revealed. </p>\n\n<p>The HBV genome is 5Kb, the smallest viral genome so its not alot of data. </p>\n\n<p>I am confused by ""Genotype B nucleotide sequence and non-HBV nucleotide sequences"". What I suspect is the investigators looked at HBV genotype B in comparison to HBV genotypes A, C to I. Essentially they wanted to assess the relationship between genetic variation and pathogenesis, which is an attempt to assess virus-induced cancer. Obviously I don't know the paper but I'm aware of the basic research front in this area. HBV has a very wierd evolution and I personally wouldn't want to investigate it.</p>\n","3848","","","","2020-02-20T05:00:26.107","","","","0","",""
"11459","2","","10881","2020-02-20T09:37:27.830","0","","<p>There's no default for the <code>export_to</code> option, so you just need to specify <code>export_to=""Seurat""</code>. Note that monocle2 doesn't seem to be compatible with the newest Seurat (3.1) releases.</p>\n","77","","","","2020-02-20T09:37:27.830","","","","0","",""
"11460","1","","","2020-02-20T12:15:34.050","0","55","<p>There is a function does need an input as selected genomic site across genomic regions. But my genomic region bed file already is a subset of interesting site (like promoters). I want this function to work not complaining about genomic site\nHowever when I leave genomic site says </p>\n\n<pre><code>&gt; prepare_elements_regions_tf &lt;- prepare_elements(""regions.bed"",\n+                                                 window_size = 50000)\nSuccessfully read elements\n Error in utils::read.delim(sites_file, stringsAsFactors = FALSE, header = FALSE) : \n  argument ""sites_file"" is missing, with no default \n</code></pre>\n\n<p>The whole function is here </p>\n\n<p><a href=""https://github.com/wenmm/ActiveDriverWGS/blob/master/R/ActiveDriverWGS.R"" rel=""nofollow noreferrer"">https://github.com/wenmm/ActiveDriverWGS/blob/master/R/ActiveDriverWGS.R</a></p>\n\n<p>But part of function that nags is this </p>\n\n<pre><code>prepare_elements = function(elements_file, sites_file, window_size = 50000, mc.cores = 1, recovery_dir = NA) {\n\n    try_error = ""try-error""\n\n\n\n    element_coords_filename = paste0(recovery_dir, ""/ADWGS_elements_coords_recovery_file.rsav"")\n\n    element_coords_1 = NULL\n\n    load_result = suppressWarnings(try(load(element_coords_filename), silent = TRUE))\n\n    if (class(load_result) == try_error) {\n\n        element_coords_1 = prepare_element_coords_from_BED(elements_file)\n\n        if (!is.na(recovery_dir)) {\n\n            save(element_coords_1, file = element_coords_filename)\n\n        }\n\n        cat(""Successfully read elements\n"")\n\n    } else {\n\n        cat(""Recovered elements\n"")\n\n    }\n\n\n\n    element_coords = coords_sanity(element_coords_1)\n\n\n\n    element_3n_filename = paste0(recovery_dir, ""/ADWGS_element_3n_recovery_file.rsav"")\n\n    element_3n = NULL\n\n    load_result = suppressWarnings(try(load(element_3n_filename), silent = TRUE))\n\n    if (class(load_result) == try_error) {\n\n        element_3n = add_trinucs_by_elements(get_3n(element_coords))\n\n        if (!is.na(recovery_dir)) {\n\n            save(element_3n, file = element_3n_filename)\n\n        }\n\n    }\n\n\n\n    window_coords_filename = paste0(recovery_dir, ""/ADWGS_window_coords_recovery_file.rsav"")\n\n    window_coords = NULL\n\n    load_result = suppressWarnings(try(load(window_coords_filename), silent = TRUE))\n\n    if (class(load_result) == try_error) {\n\n        window_coords = coords_sanity(get_window_coords(element_coords, window_size))\n\n        if (!is.na(recovery_dir)) {\n\n            save(window_coords, file = window_coords_filename)\n\n        }\n\n    }\n\n\n\n    window_3n_filename = paste0(recovery_dir, ""/ADWGS_window_3n_recovery_file.rsav"")\n\n    window_3n = NULL\n\n    load_result = suppressWarnings(try(load(window_3n_filename), silent = TRUE))\n\n    if (class(load_result) == try_error) {\n\n        window_3n = add_trinucs_by_elements(get_3n(window_coords))\n\n        if (!is.na(recovery_dir)) {\n\n            save(window_3n, file = window_3n_filename)\n\n        }\n\n    }\n\n\n\n    raw_site_coords_filename = paste0(recovery_dir, ""/ADWGS_raw_site_coords_recovery_file.rsav"")\n\n    site_coords_filename = paste0(recovery_dir, ""/ADWGS_site_coords_recovery_file.rsav"")\n\n    raw_site_coords = NULL\n\n    site_coords = NULL\n\n    sites_load_result = suppressWarnings(try(load(site_coords_filename), silent = TRUE))\n\n    if (class(sites_load_result) == try_error) {\n\n        load_result = suppressWarnings(try(load(raw_site_coords_filename), silent = TRUE))\n\n        if (class(load_result) == try_error) {\n\n            raw_site_coords = utils::read.delim(sites_file, stringsAsFactors = FALSE, header = FALSE)\n\n            colnames(raw_site_coords) = c(""chr"", ""starts"", ""ends"", ""site_info"")\n\n            raw_site_coords<span class=""math-container"">$chr = gsub(""chr"", """", raw_site_coords$</span>chr, ignore.case = TRUE)\n\n            raw_site_coords<span class=""math-container"">$chr = paste0(""chr"", raw_site_coords$</span>chr)\n\n            if (!is.na(recovery_dir)) {\n\n                save(raw_site_coords, file = raw_site_coords_filename)\n\n            }\n\n            cat(""Successfully read active sites\n"")\n\n        } else {\n\n            cat(""Recovered active sites\n"")\n\n        }\n\n\n\n        if (!is.null(raw_site_coords[[1]])) {\n\n            # first only keep sites that overlap with element coordinates\n\n            site_gr = GenomicRanges::GRanges(raw_site_coords$chr,\n\n                IRanges::IRanges(start=raw_site_coords<span class=""math-container"">$starts, end=raw_site_coords$</span>ends))\n\n            element_gr = GenomicRanges::GRanges(element_coords$chr, \n\n                IRanges::IRanges(start=element_coords<span class=""math-container"">$starts, end=element_coords$</span>ends))\n\n\n\n            rm(raw_site_coords)\n\n\n\n            overlaps_found = GenomicRanges::findOverlaps(element_gr, site_gr)\n\n            site_here_gr = site_gr[S4Vectors::subjectHits(overlaps_found)]\n\n\n\n            rm(site_gr, element_gr, overlaps_found)\n\n\n\n            site_coords = do.call(rbind, parallel::mclapply(1:nrow(element_coords), \n\n                get_sites_per_element, element_coords, site_here_gr, mc.cores=mc.cores))\n\n            rm(site_here_gr)\n\n            site_coords = coords_sanity(site_coords)\n\n        }\n\n        if (!is.na(recovery_dir)) {\n\n            save(site_coords, file = site_coords_filename)\n\n        }\n\n    } else {\n\n        cat(""Recovered active sites\n"")\n\n    }\n\n    rm(sites_load_result, element_coords)\n\n\n\n    site_3n_filename = paste0(recovery_dir, ""/ADWGS_site_3n_recovery_file.rsav"")\n\n    site_3n = NULL\n\n    load_result = suppressWarnings(try(load(site_3n_filename), silent = TRUE))\n\n    if (class(load_result) == try_error) {\n\n        site_3n = get_3n(site_coords)\n\n        if (!is.na(recovery_dir)) {\n\n            save(site_3n, file = site_3n_filename)\n\n        }\n\n    }\n\n\n\n    list(element_coords=element_coords_1, element_3n=element_3n, window_coords=window_coords, window_3n=window_3n, site_coords=site_coords, site_3n=site_3n)\n\n\n\n}\n</code></pre>\n\n<p>How I get rid of nagging part?</p>\n","4595","","","","2020-02-20T17:34:03.507","How I change a function not to need an input","<r>","1","2","",""
"11461","2","","11457","2020-02-20T14:52:27.747","1","","<p>Back in the Day™ <a href=""http://www.repeatmasker.org/"" rel=""nofollow noreferrer"">RepeatMasker</a><sup>[1]</sup> was the go-to tool everyone used for TE annotation in eukaryotes. At the time, the <a href=""https://www.girinst.org/repbase/"" rel=""nofollow noreferrer"">RepBase database from GIRI</a><sup>[2]</sup> (license required) was the <em>de facto</em> database recommended for RepeatMasker users. I know <em>de facto</em> isn't necessarily <em>definitive</em>, but it may have been the only public database supported for some time.</p>\n\n<p>It looks like <a href=""https://github.com/rmhubley/RepeatMasker#libraries-overview"" rel=""nofollow noreferrer"">more recent efforts</a> have focused on developing the <a href=""https://dfam.org/home"" rel=""nofollow noreferrer"">Dfam database</a>, mirrored after the design of databases like Pfam and Rfam.</p>\n\n<hr>\n\n<p><sup>[1]</sup>Smit AFA, Hubley R, Green P (2013-2015) RepeatMasker Open-4.0, <a href=""http://www.repeatmasker.org"" rel=""nofollow noreferrer"">http://www.repeatmasker.org</a>. </p>\n\n<p><sup>[2]</sup>Jurka J, Kapitonov VV, Pavlicek A, Klonowski P, Kohany O, Walichiewicz J (2005) Repbase Update, a database of eukaryotic repetitive elements. <em>Cytogentic and Genome Research</em> 110:462-467, <a href=""https://doi.org/10.1159/000084979"" rel=""nofollow noreferrer"">doi:10.1159/000084979</a>.</p>\n","96","","","","2020-02-20T14:52:27.747","","","","1","",""
"11462","2","","11460","2020-02-20T17:34:03.507","0","","<p>Think about it this way - your <code>elements_file</code> already has only the sites of interest and hence does not need to be subject to a subset operation. In other words, if you were to have a <code>sites_file</code>, it would have the same content as the <code>elements_file</code>.</p>\n\n<p>Can you see where I am going with this?</p>\n","650","","","","2020-02-20T17:34:03.507","","","","6","",""
"11463","1","11468","","2020-02-20T17:48:40.397","0","16","<p>My current settings are <code>megahit -r input.fq --num-cpu-threads 32 --min-contig-len 300 --presets meta-large -o output</code>. I picked the 'presets meta-large' because I am dealing with a quite large and diverse metagenome out of which I only need some genes.</p>\n\n<p>I've got a contig of interest ~19kb, with a depth of 16. Could I make this contig larger with more relaxed settings? Help would be appreciated!</p>\n","4401","","","","2020-02-20T22:03:54.463","Can I get longer contigs by changing MEGAHIT settings?","<assembly><metagenome>","1","0","",""
"11464","2","","11457","2020-02-20T18:38:58.330","1","","<p>Would advise doing Denovo identification method structure-based. Then use these public databases (  RepBase, Dfam ) to look for high similarity or homology exploration, this possibility if your species, the repeats not yet discovered, or deposited in the databases.</p>\n\n<p>If the species well-investigated earlier and the mobile element recognised, I assume it will be a straightforward task using species-specific search via Repeatmasker tool with the -species flag to obtain the MobileElement annotation in the genome.</p>\n\n<p>Let me know if you need any data from Repbase related to your species, i have an entree. </p>\n","3332","","3332","2020-02-21T22:59:02.440","2020-02-21T22:59:02.440","","","","2","",""
"11465","1","","","2020-02-20T19:12:39.510","1","21","<p>I have a large BAM file and I want to filter it by just what's defined in a bed file, and write it to a new file.</p>\n\n<p>Is it possible to do this with samtools, and if so, how? Is there a better tool I could use to do this?</p>\n","7259","","","","2020-02-20T21:18:35.383","How to filter a SAM file by a bed file?","<bam><samtools><sam><pysam>","1","3","",""
"11466","2","","11465","2020-02-20T21:06:20.237","1","","<p>Thanks to the helpful comments I figured it out:</p>\n\n<pre><code>samtools view -b -h -L bedfile.bed originalbam.bam &gt; newbam.bam\n</code></pre>\n\n<p>-b = Output as bed file<br>\n-h = include header<br>\n-L = only output alignments overlapping the input bed file     </p>\n","7259","","7259","2020-02-20T21:18:35.383","2020-02-20T21:18:35.383","","","","0","",""
"11467","2","","11427","2020-02-20T21:26:17.393","3","","<p>The GDC API user guide (<a href=""https://docs.gdc.cancer.gov/API/PDF/API_UG.pdf"" rel=""nofollow noreferrer"">https://docs.gdc.cancer.gov/API/PDF/API_UG.pdf</a>) provides some guidance on the structure of GDC data. So for getting RNA-Seq data from GDC, you would provide this filter to get the 551 text files with workflow_type set as ""HTSeq - FPKM-UQ"" and ""TCGA-LUSC"" for the project. There are other workflow types as well:</p>\n\n<pre><code>cases.project.program.name in [""TCGA""]\nand cases.project.project_id in [""TCGA-LUSC""]\nand files.access in [""open""]\nand files.analysis.workflow_type in [""HTSeq - FPKM-UQ""]\nand files.data_format in [""txt""]\nand files.data_type in [""Gene Expression Quantification""]\nand files.experimental_strategy in [""RNA-Seq""]\n</code></pre>\n\n<p>(You don’t need all those filters to get the 551 files; I included them to show what is available.) The txt files are returned as gzipped tar files that hold text files with rows of the form:</p>\n\n<p>ENSG00000158486.12  5341.21601153</p>\n\n<p>The above filters provide all the FPKM-UQ files for TCGA-LUSC. If you wanted to just get the data for normal samples as controls, that would require filtering on sample type as well, adding in the additional filter:</p>\n\n<pre><code>and cases.samples.sample_type in [“solid tissue normal”]\n</code></pre>\n\n<p>That will return 49 files. If you instead wished to get only the tumor files, the additional filter would instead be:</p>\n\n<pre><code>and cases.samples.sample_type in [“primary tumor”] \n</code></pre>\n\n<p>and that returns 502 files. Note that there are several different sample types for tumor and normal tissues; those just happen to be the ones used here.\n(Note that if you are using the GDC Repository portal, adding that additional filter requires you to select “Add a Case/Biospecimen Filter” on the “Cases” filter tab.)</p>\n\n<p>Since GDC is part of the NCI Cancer Research Data Commons (CRDC), instead of downloading the data, you can actually work with it in the cloud using the various CRDC Cloud Resources such as the ISB-CGC: <a href=""https://www.isb-cgc.org/"" rel=""nofollow noreferrer"">https://www.isb-cgc.org/</a>. In fact, all the RNA-Seq data for TCGA is in an ISB-CGC Google BigQuery table: (<a href=""https://console.cloud.google.com/bigquery?p=isb-cgc&amp;d=TCGA_hg38_data_v0&amp;t=RNAseq_Gene_Expression&amp;page=table"" rel=""nofollow noreferrer"">https://console.cloud.google.com/bigquery?p=isb-cgc&amp;d=TCGA_hg38_data_v0&amp;t=RNAseq_Gene_Expression&amp;page=table</a>)\nAnd the above count data can be pulled using the query:</p>\n\n<pre><code>SELECT HTSeq__FPKM_UQ\nFROM `isb-cgc.TCGA_hg38_data_v0.RNAseq_Gene_Expression`\nWHERE Ensembl_gene_id_v = ""ENSG00000158486.12"" AND project_short_name = ""TCGA-LUSC""\n</code></pre>\n\n<p>If you want to discriminate between normal and tumor expression levels, the ISB-CGC BQ tables make it pretty easy to tag the expression values with the type of tissue involved. The sample type can be assigned by joining the previous query with the biospecimen table:</p>\n\n<pre><code>WITH bio as (\nSELECT sample_barcode, sample_type_name\nFROM `isb-cgc.TCGA_bioclin_v0.Biospecimen`\n),\nexpr as (SELECT sample_barcode, HTSeq__FPKM_UQ\nFROM `isb-cgc.TCGA_hg38_data_v0.RNAseq_Gene_Expression`\nWHERE Ensembl_gene_id_v = ""ENSG00000158486.12"" AND project_short_name = ""TCGA-LUSC"")\nSELECT bio.sample_type_name, expr.HTSeq__FPKM_UQ\nFROM expr\nJOIN bio\nON bio.sample_barcode = expr.sample_barcode\n</code></pre>\n\n<p>The GDC has listed some of the available sample types on this page: <a href=""https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/sample-type-codes"" rel=""nofollow noreferrer"">https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/sample-type-codes</a>\nAlso, BigQuery has a user interface that makes exploring data sets and writing queries easy, but you need to log in with a Google ID, so the ISB-CGC has a BigQuery Table Search (<a href=""https://isb-cgc.appspot.com/bq_meta_search/"" rel=""nofollow noreferrer"">https://isb-cgc.appspot.com/bq_meta_search/</a>) that allows you to explore the available tables without logging in. If you want to run queries, Google offers a free tier that you can use to try things out and run small jobs (<a href=""https://cloud.google.com/free/"" rel=""nofollow noreferrer"">https://cloud.google.com/free/</a>) and ISB-CGC offers free credits too. ISB-CGC also has examples of working in R with BigQuery within the Community Notebook Repository (<a href=""https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/HowTos.html"" rel=""nofollow noreferrer"">https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/HowTos.html</a>). A few useful notebooks would be 'How do I create cohorts of patients?', 'How do I plot a heatmap using data in BigQuery?', and 'How do I get started fast?'.</p>\n","6566","","6566","2020-02-29T01:21:18.073","2020-02-29T01:21:18.073","","","","6","",""
"11468","2","","11463","2020-02-20T22:03:54.463","1","","<p>You can use the option <code>--k-step 10</code> and <code>--kmin-1pass</code> for low-coverage datasets.</p>\n","4282","","","","2020-02-20T22:03:54.463","","","","0","",""
"11469","1","","","2020-02-20T22:30:54.330","1","16","<p>I'm currently using Procheck to check a protein backbone. The process is:</p>\n\n<ol>\n<li>Determine backbone phi, psi, and omega angles</li>\n<li>Compare to known angle distributions to identify outliers, and angles in disallowed regions</li>\n</ol>\n\n<p>Is Whatcheck equivalent for this task?</p>\n","6966","","","","2020-02-20T22:30:54.330","What's the difference between Procheck and Whatcheck?","<protein-structure>","0","0","","1"
"11470","1","11472","","2020-02-20T23:37:57.450","2","64","<p>I recently discovered <a href=""https://github.com/snakemake-workflows/rna-seq-star-deseq2"" rel=""nofollow noreferrer"">this Snakemake pipeline</a> for RNASeq that uses STAR's quantMode to quantify gene expression for DESeq2 differential expression analysis.</p>\n\n<p>In the past I've always seen the workflow as:</p>\n\n<ol>\n<li>STAR alignment</li>\n<li>RSEM/HTSeqCount quantification</li>\n<li>DESeq2 analysis</li>\n</ol>\n\n<p>More recently I've also seen:</p>\n\n<ol>\n<li>Kallisto/Salmon pseudo-alignment quantification</li>\n<li>DESeq2/Sleuth analysis</li>\n</ol>\n\n<p>I tried searching online for differences between STAR quantMode vs other quantification algorithms but couldn't find many details. What are the benefits and drawbacks of using STAR quantMode vs RSEM/Kallisto/Salmon?</p>\n","6674","","","","2020-02-21T19:53:43.243","STAR quantMode vs RSEM vs Kallisto","<rna-seq><differential-expression><deseq2><rsem><kallisto>","2","0","",""
"11471","1","","","2020-02-21T02:31:57.870","1","22","<p>Hello I'm helping to develop a cloud docking tool for screening compounds, similar to Swissdock but with mass throughput and GPU optimizations.</p>\n\n<p>Specifically helping screen existing drugs against coronavirus proteins (couple structures solved, rest TASSER).</p>\n\n<p>I'm planning to use GPU optimized AutoDock-GPU , which takes in .maps.fld\nI know you can use autogrid to select the bounding box and generate the .maps.fld, but I've been unable to figure out the workflow.\nAlso for preliminary screening I want to search the whole protein without specifying a bounding box.\nIs there a script for converting protein.pdb to .maps.fld?</p>\n\n<p>Collaborators welcome.</p>\n\n<p>Thanks!</p>\n","7263","","","","2020-02-22T17:51:02.627","Protein ligand docking: how to convert <protein>.pdb to <protein>.maps.fld?","<protein-structure><docking><structural-biology>","1","0","",""
"11472","2","","11470","2020-02-21T09:01:16.057","2","","<p>STAR quantMode (GeneCounts) essentially provides the same output as HTSeq-Count would, ie. number of reads that cover a given gene. This is the most simple measure of expression you could get from RNA-seq data. Kallisto and Salmon utilize pseudo-alignment to determine expression measures of transcripts (as opposed to genes). RSEM uses some algorithm to determine isoform fractions, but still based on alignments. (Note that I don't have any personal experience using RSEM.)</p>\n\n<p>It ultimately depends on the research question which method will be most appropriate for you.</p>\n\n<h1>read counts</h1>\n\n<p>pros:</p>\n\n<ul>\n<li>relatively easy to understand concept-wise</li>\n<li>can be used to generate many different expression measures (FPKMs, TPM, CPM, etc.) and is, thus, more versatile in down-stream analyses</li>\n</ul>\n\n<p>cons:</p>\n\n<ul>\n<li>you can't distinguish isoforms (from the counts themselves)</li>\n<li>requires alignment and, thus, more time to obtain</li>\n</ul>\n\n<h1>salmon/kallisto:</h1>\n\n<p>pros:</p>\n\n<ul>\n<li>faster (due to lack of alignment step)</li>\n<li>quantifies (known) isoforms</li>\n</ul>\n\n<p>cons:</p>\n\n<ul>\n<li>expression values are more abstract, less easy to explain</li>\n<li>gene-level analyses will require some extra work</li>\n</ul>\n\n<h1>RSEM</h1>\n\n<p>Likely lies somewhere in between, still requiring alignments will make it slower. But it does give you expression values for isoforms. </p>\n","6955","","","","2020-02-21T09:01:16.057","","","","0","",""
"11473","1","","","2020-02-21T11:57:01.360","0","31","<p>EDIT: I posted this as a bug, perhaps wait for the result of that? <a href=""https://github.com/snakemake/snakemake/issues/240"" rel=""nofollow noreferrer"">github</a></p>\n\n<p>All,</p>\n\n<p>I am using Snakemake to build a pipeline. To define input and output names, I constructed them based on a filename base (from the samples table), a read indicator (like R1, R2) and the fastq file extension, like so:</p>\n\n<pre><code>samples = pd.read_csv('samples.tsv', sep='\t').set_index('samples', drop=False)\nreads = ['R1', 'R2']\nfastq_file_extension = _001.fastq.gz\n</code></pre>\n\n<pre><code>rule fastqc:\n    input:\n        os.path.join(config['Directories']['fastq_raw_dir'], '{sample},{read}' + fastq_file_extension)\n    output:\n        html=os.path.join(config['Directories']['fastqc_dir'], '{sample},{read}_fastqc.html'),\n        zip=os.path.join(config['Directories']['fastqc_dir'], '{sample},{read}_fastqc.zip')\n    wrapper:\n        ""0.49.0/bio/fastqc""\n</code></pre>\n\n<p>However, during pipeline execution already I can see strange things happening, for this rule it reports:</p>\n\n<pre><code>[Fri Feb 21 12:39:57 2020]\nrule fastqc:\ninput: fastq_raw/0053_P2017BB3S20R_S2_R1_001.fastq.gz\noutput: qc/fastqc/0053_P2017BB3S20R_S2_R1_fastqc.html, qc/fastqc/0053_P2017BB3S20R_S2_R1_fastqc.zip\njobid: 10\nwildcards: sample=0053_P2017BB3S20R_S2_R, read=1\n</code></pre>\n\n<p>As you can see, Snakemake pulled the R part of the read to the sample and set the read to 1 (should be R1). But this still works because it produces output files as Snakemake expects.</p>\n\n<p>But the next rule, trim_galore_pe:</p>\n\n<pre><code>rule trim_galore_pe:\n    input:\n        [os.path.join(config['Directories']['fastq_raw_dir'], '{sample}' + read + fastq_file_extension) for read in reads]\n    output:\n        fastq_read1 = os.path.join(config['Directories']['fastq_trimmed_dir'], '{sample}.1_val_1.fq.gz'),\n        fastq_read2 = os.path.join(config['Directories']['fastq_trimmed_dir'], '{sample}.2_val_2.fq.gz'),\n        report_read1 = os.path.join(config['Directories']['trim_galore_qc_dir'], '{sample}.1.fastq.gz_trimming_report.txt'),\n        report_read2 = os.path.join(config['Directories']['trim_galore_qc_dir'], '{sample}.2.fastq.gz_trimming_report.txt')\n    params:\n        extra=""--retain_unpaired""\n    wrapper:\n        ""0.49.0/bio/trim_galore/pe""\n</code></pre>\n\n<p>This goes wrong, during pipeline execution Snakemake reports:</p>\n\n<pre><code>[Fri Feb 21 12:40:37 2020]\nError in rule trim_galore_pe:\njobid: 7\noutput: fastq_trimmed/0053_P2017BB3S20R_S2_.1_val_1.fq.gz, fastq_trimmed/0053_P2017BB3S20R_S2_.2_val_2.fq.gz, qc/trim_galore/0053_P2017BB3S20R_S2_.1.fastq.gz_trimming_report.txt, qc/trim_galore/0053_P2017BB3S20R_S2_.2.fastq.gz_trimming_report..txt\nconda-env: /home/nlv24077/projects/snaqs/snakemake_envs/1b94430b\ncluster_jobid: 11904662.osios\nError executing rule trim_galore_pe on cluster (jobid: 7, external: 11904662.osios, jobscript: /home/nlv24077/temp/RNASeqPairedEnd/.snakemake/tmp.ooypuklc/snakejob.trim_galore_pe.7.sh).\nFor error details see the cluster log and the log files of the involved rule(s).\n</code></pre>\n\n<p>This is understandeble because the file fastq_trimmed/0053_P2017BB3S20R_S2_.1_val_1.fq.gz does not exist, it should be fastq_trimmed/0053_P2017BB3S20R_S2.1_val_1.fq.gz, somehow Snakemake added an extra underscore, which you can see in the execution output, under ""wildcards"" (0053_P2017BB3S20R_S2_).</p>\n\n<pre><code>[Fri Feb 21 12:39:57 2020]\nrule trim_galore_pe:\ninput: fastq_raw/0053_P2017BB3S20R_S2_R1_001.fastq.gz, fastq_raw/0053_P2017BB3S20R_S2_R2_001.fastq.gz\noutput: fastq_trimmed/0053_P2017BB3S20R_S2_.1_val_1.fq.gz, fastq_trimmed/0053_P2017BB3S20R_S2_.2_val_2.fq.gz, qc/trim_galore/0053_P2017BB3S20R_S2_.1.fastq.gz_trimming_report.txt, qc/trim_galore/0053_P2017BB3S20R_S2_.2.fastq.gz_trimming_report..txt\njobid: 7\nwildcards: sample=0053_P2017BB3S20R_S2_\n</code></pre>\n\n<p>So how can I prevent Snakemake from changing the samples and read wildcard in seemingly arbitrary ways?</p>\n","3964","","3964","2020-02-21T12:45:05.590","2020-02-24T11:45:11.663","Prevent Snakemake from changing wildcards: sample for every rule","<snakemake>","2","0","",""
"11474","2","","11473","2020-02-21T14:33:17.483","0","","<p>Placing two wildcards directly behind each other will make it impossible for snakemake to determine where one wildcard ends and the other begins. So it is simply taking the longest possible match for the first wildcard (<code>sample</code>) and then one character for the second (<code>read</code>). This will also cause the <code>_</code> to be included in the wildcard value <code>sample</code>.</p>\n\n<p>I'm assuming that this same pattern is used for the inputs of another rule to refer to the trim_galore_pe output. Hence why the <code>_</code> is included in the wildcards there.</p>\n\n<p>You might be able to fix this, simply by adding a <code>_</code> in-between <code>{sample}</code> and <code>{readgroup}</code> in your first rule (and any other rules using this same pattern):</p>\n\n<pre><code>rule fastqc:\n    input:\n        os.path.join(config['Directories']['fastq_raw_dir'], '{sample}_{read}' + fastq_file_extension)\n    output:\n        html=os.path.join(config['Directories']['fastqc_dir'], '{sample},{read}_fastqc.html'),\n        zip=os.path.join(config['Directories']['fastqc_dir'], '{sample},{read}_fastqc.zip')\n    wrapper:\n        ""0.49.0/bio/fastqc""\n</code></pre>\n","6955","","","","2020-02-21T14:33:17.483","","","","6","",""
"11475","1","","","2020-02-21T14:41:39.810","2","64","<p><strong>I am interested in identifying gene pairs (or better: sets of genes) whose expression is mutually exclusive.</strong>\nIdeally, both genes (or gene sets) would be widely expressed but I am also interested in cases that are very cell type specific.</p>\n\n<p>Familiar examples of the former would include some of the cyclin genes, in humans e.g. cyclin E and cyfclin B, <a href=""https://en.wikipedia.org/wiki/File:Cyclin_Expression.svg"" rel=""nofollow noreferrer"">which are expressed during S-phase and M-phase, respectively.</a> Another example would be some of the clock-genes, where <a href=""https://www.researchgate.net/publication/232233128_Early_Chronotype_and_Tissue-Specific_Alterations_of_Circadian_Clock_Function_in_Spontaneously_Hypertensive_Rats"" rel=""nofollow noreferrer"">Per1 and Per2 are often highly expressed in the suprachiasmatic nucleus (SCN) before midday, and whereas Bmal1 is expressed after midday</a>.\nHowever, such genes need not be cyclically expressed, they can simply be very specific markers for certain cell types. For example, GAD65 and GAD67 are very specific for interneurons, whereas CamKIIa is very specific for excitatory glutamatergic cells.</p>\n\n<p>I have been able to find a single paper so far that systematically tries to find such genes from gene expression data, namely: <a href=""https://www.researchgate.net/publication/221239044_Mining_for_Mutually_Exclusive_Gene_Expressions"" rel=""nofollow noreferrer"">Tzanis &amp; Vlahavas (2010) Mining for Mutually Exclusive Gene Expressions</a>. According to Google Scholar, this paper is cited by a single other paper that does not seem to be relevant. In Tzanis &amp; Vlahavas (2010), they use a <a href=""http://mlkd.csd.auth.gr/mutex/datasets.html"" rel=""nofollow noreferrer"">SAGE library</a> (from 2002) of short expression sequence tags to find these strong negative associations. By today's standards, the data set is pretty small (basically, it contains 90 samples). The paper itself is hence anything but a canonical source for such negative interactions.</p>\n\n<h3>Questions:</h3>\n\n<ol>\n<li>Is there a curated database of mutually exclusive gene pairs or gene sets?</li>\n<li>If not, is there a curated database that would allow me to find such genes?</li>\n</ol>\n\n<p>Otherwise, my current plan would be to use the human or mouse cell atlas single cell data to define a signed, weighted gene expression network.\nI would then search for pairs of clusters/modules, where genes have predominantly positive correlations within a cluster but predominantly negative correlations between clusters.\nSo essentially, signed, weighted gene correlation network analysis (WGCNA) with extra steps.</p>\n\n<ol start=""3"">\n<li><p>Has somebody published such an analysis already (the WGCNA literature is pretty substantial)?</p></li>\n<li><p>Is there other (recent) related work that I am unaware off?</p></li>\n</ol>\n\n<p>I am fairly new to bioinformatics, so the lack of papers and resources that I am able to find is likely mostly a reflection of my lack of ability to find the correct search term.</p>\n\n<h3>Addendum</h3>\n\n<p>Similar biostars <a href=""https://www.biostars.org/p/397493/"" rel=""nofollow noreferrer"">thread</a> from 5 months ago, also without an answer.</p>\n","7266","","7266","2020-02-24T15:22:34.453","2020-02-28T23:15:55.373","Identifying mutually **exclusive** gene sets","<scrnaseq><networks><wgcna><correlation>","1","11","","2"
"11477","1","","","2020-02-21T19:30:48.607","1","12","<p>I have called peaks using MACS2 and now I'm trying to run ChIPQC bioconductor package. Can someone please tell me should I use sorted BAM files or unsorted original BAM files? I have created sorted BAM files and from that indexes for BAM files. When I was trying run ChIPQC earlier, I got an error. Then I figured out that is because I don't have index files. Then I created indexes after sorting BAM files.</p>\n","856","","","","2020-02-21T19:30:48.607","When running ChIPQC bioconductor package after peak calling, should I use sorted BAM files or unsorted original BAM files?","<r><bioconductor><samtools><chip-seq><macs2>","0","1","",""
"11478","2","","11470","2020-02-21T19:53:43.243","0","","<p>A shorter version...STAR's is convenient, in that it gets done right alongside the alignment, but RSEM and Kallisto are smarter about dealing with ambiguous reads.  So I'd say your results from either of those are superior.</p>\n\n<p>Personally, I use STAR for alignment, then give the bam to RSEM.  STAR's gene count is good for a reality check to make sure that I got the strandedness right, and RSEM's gene level expected counts are good for DESeq once they are rounded.</p>\n","2388","","","","2020-02-21T19:53:43.243","","","","1","",""
"11479","2","","11471","2020-02-22T09:43:38.137","0","","<p>There are a few steps in between <code>pdb</code> and <code>.maps.fld</code>.</p>\n\n<p>Here is the list of several scripts that can do tasks for you that you downloaded with autodock MGLTools: <a href=""http://autodock.scripps.edu/faqs-help/faq/where-can-i-find-the-python-scripts-for-preparing-and-analysing-autodock-dockings"" rel=""nofollow noreferrer"">http://autodock.scripps.edu/faqs-help/faq/where-can-i-find-the-python-scripts-for-preparing-and-analysing-autodock-dockings</a>. Look at the <code>prepare_</code> files. Also note that the scripts come with a pre-bundled python, but you can install these scripts with conda.\nHere are the basic order of things:</p>\n\n<pre><code>prepare_receptor4.py -r protein.pdb\nprepare_ligand4.py -l ligand.mol2\nprepare_gpf4.py -l ligand.pdbqt -r protein.pdbqt -y\nautogrid4 -p protein.gpf\nprepare_dpf4.py -l ligand.pdbqt -r protein.pdbqt\n</code></pre>\n\n<h2>partial charge</h2>\n\n<p>Your PDB needs partial charges so first you convert it to <code>PDBQT</code> file.\nThis is true for your protein and your ligand. That is no complicated parameterisation for your ligand —which is not necessarily a good thing!\nIn this step it is essential to correct protonation if absent each have their own flags.</p>\n\n<h2>grid</h2>\n\n<p>The you make a grid file (<code>gpf</code>), which contains your grid. To make a box that spans the whole protein simply use the unit cell dimensions form the PDB. For an mmCIF, if you open the header dictionary you need <code>_pdbx_struct_oper_list.matrix</code> using <code>MMCIF2Dict</code> from biopython's PDB submodule. For a PDB file, just search for the <code>CRYST1</code> line (cf. <a href=""http://www.wwpdb.org/documentation/file-format-content/format33/sect8.html"" rel=""nofollow noreferrer"">format</a>).</p>\n\n<p>Once you have have the grid you can run autogrid. This step will create the <code>map.fld</code></p>\n\n<h3>Why a big box is bad for a VS</h3>\n\n<p>However, all those who take this approach opt for a box that is huge: as there is no solvent having a huge bounding has little penalty. However, this is considered a very bad strategy for a virtual screen. This not because it uses up computer resources as these are dirty cheap. Structural knowledge is important and targeting the active site of an enzyme stops it, while binding to the surface does nothing unless it's an interface. So manually reading what the stuff are and choosing the box wisely will save you a lot of time downstream in analysis. There is after all the saying ""one week in the lab will save you an hour in the library""...</p>\n\n<h2>Protein and ligand</h2>\n\n<p>Lastly, merge the protein with the small molecule in a <code>dpf</code> file.</p>\n\n<h2>Caveat against docking with models</h2>\n\n<p>It is not a good idea dock against models. Swissmodel (unless virtually identical), I-Tasser, Phyre or EVFold models. Docking is very sensitive to small structural difference that may have resulted from the modelling, so docking to models is highly discouraged. For Coronavirus protein, dock against SARS, which is highly similar. Or at the very least pay strong attention to the I-Tasser C-scores and discard the bottom 2/3 of their models.</p>\n\n<h2>Note about coronavirus</h2>\n\n<p>Your objective of docking coronavirus may be a bit too late however.\nI did <a href=""https://michelanglo.sgc.ox.ac.uk/r/coronavirus"" rel=""nofollow noreferrer"">a wee interactive summary of the literature</a> about the solved structure which is the protease. Proteases are very easy to rationally design drugs for.\nThere are already:</p>\n\n<ul>\n<li>licensed HIV protease inhibitors lopinavir and ritonavir that are effective —<a href=""https://clinicaltrials.gov/ct2/show/NCT04252885"" rel=""nofollow noreferrer"">a clinical trial</a> is underway and FDA approval may even come in July 2020</li>\n<li><a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1197287/"" rel=""nofollow noreferrer"">a paper</a> that has an empirically validated coronavirus specific ligand</li>\n<li>three manuscripts about virtual screens in bioarxiv. </li>\n</ul>\n","6322","","6322","2020-02-22T17:51:02.627","2020-02-22T17:51:02.627","","","","3","",""
"11480","1","11500","","2020-02-22T16:38:47.847","2","44","<p>This is a link to the UCSC Genome Browser showing a portion of Ch 17 containing TP53 <a href=""http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&amp;lastVirtModeType=default&amp;lastVirtModeExtraState=&amp;virtModeType=default&amp;virtMode=0&amp;nonVirtPosition=&amp;position=chr17%3A7570000%2D7593868&amp;hgsid=802661817_ENKSAs3qbWfOxWUAXXfwDPZXbC8L"" rel=""nofollow noreferrer"">here</a>.</p>\n\n<p>There are several representations of the gene of varying length and number of exons. I presume these are equivalent to alternate splicings.</p>\n\n<p>Assuming this presumption is correct(?), how can I determine which result in RNAs that are experimentally verified?</p>\n","7271","","3848","2020-02-22T23:12:17.733","2020-02-24T20:00:09.833","Distinguishing Splicings that Produce Functional Transcripts Amongst Alternates Shown","<genome-browser>","1","1","",""
"11481","2","","11427","2020-02-22T21:07:52.063","0","","<p>I find UCSC Xena to be well-organized and easy to use: <a href=""https://xenabrowser.net/datapages/"" rel=""nofollow noreferrer"">https://xenabrowser.net/datapages/</a></p>\n\n<p>Plain text files that are clearly labeled for all TCGA data (that is not protected).</p>\n","35","","35","2020-02-22T21:14:26.100","2020-02-22T21:14:26.100","","","","0","",""
"11482","2","","9229","2020-02-22T23:45:42.910","0","","<p>This is my first post and I will add some explanations. When I tried to use this script, <i>print(*map(self.fix_record, self.vcf), sep="""", end="""")</i> will increase memory usage because it will load everything into memory and then start to apply fix_record function. So, I just used two lines <i>for line in self.vcf: print(self.fix_record(line), sep="""", end="""")</i>. Because pysam uses generator by default, if we process one by one by pysam's VariantFile function, it will consume very little memory. In my case, the VCF file was more than 200GB, so I had to fix this script for my VCF file.</p>\n\n<p>Second my VCF file used ""<em>"" for empty ALT, so I fixed two lines (1) <i> if ""</em>"" in record.alts: </i> (2) <i> record.alts = tuple(map(lambda alt: ref if alt == ""*"" else f""{ref},{alt}"", record.alts)) </i></p>\n\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport argparse\nimport sys\n\nimport pysam\n\n\nclass Application:\n    def __init__(self):\n        self.args = Application.get_args()\n        self.vcf = pysam.VariantFile(sys.stdin)\n        self.fasta = pysam.FastaFile(self.args.fasta)\n\n    @staticmethod\n    def get_args() -&gt; argparse.Namespace:\n        """"""\n        Read command line arguments and set input and output sources.\n\n        :return: parsed command line arguments\n        """"""\n\n        parser = argparse.ArgumentParser(prog=""fixvcf.py"", description=""Fixes deletion represented by -"")\n        parser.add_argument('--version', action='version', version='%(prog)s 0.1')\n\n        parser.add_argument(""input"", help=""input file, use - to read from stdin"")\n        parser.add_argument(""-f"", ""--fasta"", help=""fasta reference file"", required=True)\n        parser.add_argument(""-o"", ""--output"", help=""output file"")\n\n        if len(sys.argv) == 1:\n            parser.print_help(sys.stderr)\n            sys.exit(1)\n\n        args = parser.parse_args()\n\n        if args.output:\n            sys.stdout = open(args.output, ""w"")\n\n        if args.input != ""-"":\n            sys.stdin = open(args.input, ""r"")\n\n        return args\n\n    def fix_record(self, record: pysam.libcbcf.VariantRecord) -&gt;     pysam.libcbcf.VariantRecord:\n        """"""\n        If the ALT column of the records contains a `-`, the variant position is shift one position to the left.\n        For reaching this the reference base for the position is determined and prepended to the current REF bases.\n        The ALT value `-` is replaced with the determined ref base on position before. In case there are multiple ALT\n        values the determined ref base is prepended to them.\n\n        :param record: vcf file record\n        :return: fixed vcf record\n        """"""\n\n        if ""*"" in record.alts:\n            ref = self.fasta.fetch(reference=record.chrom, start=record.pos - 1, end=record.pos)\n\n            record.pos = record.pos - 1\n            record.ref = f""{ref},{record.ref}""\n            record.alts = tuple(map(lambda alt: ref if alt == ""*"" else f""{ref},{alt}"", record.alts))\n\n        return record\n\n    def start(self):\n        """"""\n        Print the header of the vcf file. Afterwards the fix_record method is applied to each record in the vcf file.\n\n        For more information about python's map() see: \nhttps://docs.python.org/3.6/library/functions.html#map\n\n        For more information about the unpacking mechanism using `*`see:\n        https://docs.python.org/3/tutorial/controlflow.html#tut-unpacking-arguments\n        """"""\n\n        print(self.vcf.header, end="""")\n        #print(*map(self.fix_record, self.vcf), sep="""", end="""")\n        for line in self.vcf:\n            print(self.fix_record(line), sep="""", end="""")\n\nif __name__ == ""__main__"":\n    app = Application()\n    app.start()\n</code></pre>\n","7230","","","","2020-02-22T23:45:42.910","","","","0","",""
"11483","1","","","2020-02-23T01:26:52.720","1","44","<p>I have multiple single cell samples to analyze and I'm following the instructions in Satija Lab's website. I want to merge all the count files from all the samples at once, and associate the metadata to each sample. </p>\n\n<p>For the <code>Read10x</code> command, is there a way to read multiple files at once, such as merging all the filter_featire_bc_matrix files?</p>\n\n<p>I have all the samples saved in a directory on my desktop, is it possible to call the directory within the script for it to read all the raw counts?</p>\n\n<p>This is what I have thus far:</p>\n\n<pre><code># load the required libraries\nlibrary(dplyr)\nlibrary(Seurat) \nlibrary(ggplot2)\n\n-----------------------------------------------\n\n# set up the working directory\n\nwd = (""~/Seurat"") setwd(wd)\n\n# read all metadata\n\nmetadata = read.table(""metadata_Jan2020.csv"", sep=';', header=T, stringsAsFactors=F)\nrownames(metadata) = metadata$LibraryID\n\n#metadata\n#mf = metadata\n\nmf = metadata %&gt;% filter(Type %in% c(""PDAC"", ""NORMAL""))\nmf = mf %&gt;% filter(!(ID %in% c(""PDAC-HTB2854"")))`\n</code></pre>\n","6827","","48","2020-02-26T15:27:55.577","2020-02-26T15:27:55.577","Reading multiple raw files in Seurat","<r><scrnaseq><seurat><10x-genomics>","1","0","",""
"11484","2","","11483","2020-02-23T06:18:15.340","2","","<p>Seurat has <a href=""https://satijalab.org/seurat/v3.1/merge_vignette.html"" rel=""nofollow noreferrer"">a vignette specifically for combining multiple 10x libraries</a>.</p>\n","35","","","","2020-02-23T06:18:15.340","","","","6","",""
"11485","1","11487","","2020-02-23T06:24:35.260","5","190","<p>I know of the common green glow gene but I forgot the name and I also know that some algae glow blue. There are so many types of bioluminesent organisms, so I am wondering what species have which genes and what other genes are associated with their metabolism and the systems that regulate these genes.</p>\n","7272","","7272","2020-02-23T15:32:40.143","2020-02-23T16:32:28.027","What are the different kinds of bioluminescent genes?","<gene><blast><systems-biology>","3","3","","1"
"11486","2","","11485","2020-02-23T08:32:00.970","2","","<p>The green protein is called GFP of 238 amino acid residues (26.9 kDa) and produced by:</p>\n\n<ol>\n<li>the jellyfish Aequorea victoria, It has a major excitation peak at a wavelength of 395 nm. </li>\n<li>the sea pansy, Renilla reniformis of 498 nm. </li>\n</ol>\n\n<p>However, the protein is widespread and not restricted to these species.</p>\n","3848","","","","2020-02-23T08:32:00.970","","","","3","",""
"11487","2","","11485","2020-02-23T09:37:44.257","4","","<p>There are three processes wherein light is emitted.</p>\n\n<ul>\n<li><strong>Bioluminescence</strong>: a chemical reaction releases light. The enzyme that does this is luciferase, while its substrate is luciferin, a small molecule. There are half a dozen organisms that have analogous (evolved independently) enzymes and totally different substrates, such as fireflies, Alivibrio bacteria, diatomes, deep sea fish, some mushrooms etc. The <a href=""https://en.wikipedia.org/wiki/Luciferin"" rel=""nofollow noreferrer"">wiki article</a> has a list of these and their small molecules.</li>\n<li><strong>Fluorescence</strong>: light is absorbed by the molecule and is emitted at a lower energy (wider wavelength, redshifted). Green fluorescent protein of Aequorea victoria jellyfish is the classic example of this. It is excited with UV and emits green light. Here is <a href=""https://michelanglo.sgc.ox.ac.uk/data/5bb431b4-e2b6-49ac-aef8-71370bec04e8"" rel=""nofollow noreferrer"">an interactive model</a>, I threw together. Variants exists for biotech reasons that span the whole of the visible spectrum. Other species have a homologous beta barrel, such as corals (red).</li>\n<li><strong>Phosphorescence</strong>: light is absorbed by the molecule and emitted later.  Ignoring the picosecond delay in fluorescent small molecules (cf. fluorescent lifetime microscopy), this is exclusively a properties of lanthanide elements (e.g. Europium). Lathanide binding domains have been designed, but do no exist in nature.</li>\n</ul>\n","6322","","","","2020-02-23T09:37:44.257","","","","6","",""
"11488","1","","","2020-02-23T13:26:03.780","0","38","<p>I have a copy number segment file with this by SCAT R package</p>\n\n<pre><code>Chromosome  Start   End\n1           10583   120983\n1           121009  245455\n1           404187  5719467\n1           5719471 5735807\n</code></pre>\n\n<p>I want to count the number of markers in these locations</p>\n\n<p>I asked somebody how to do this, he replied:</p>\n\n<p><strong>We created marker file by dividing the genome into 10 kb bins, so the markers file contains the start and stop positions for each of these 10kb segments for each chromosome along with ASCAT segments start and end positions. Then, for each ASCAT copy number segment, we count how many markers fall within that region for total probes.</strong></p>\n\n<p>Can somebody helps to understand what he mean by dividing the genome </p>\n","4595","","","","2020-02-23T16:27:04.087","What does this statement mean?","<r><wgs><cnv>","1","2","",""
"11489","1","","","2020-02-23T13:36:28.380","0","24","<p>I have hundreds of bacterial genomes and I would like to know which is the best informatic tool in which I can run all genomes on one go and get a 16S rRNA gene sequence of each genome as output.</p>\n\n<p>Could anyone give me some advice?</p>\n","7273","","","","2020-02-24T12:18:17.070","Looking for a tool to find 16S rRNA in hundreds of genomes","<genome><bacteria>","1","1","",""
"11490","2","","11488","2020-02-23T16:27:04.087","4","","<p>They created 10kb bins, which could be called ""dividing the genome"". These are just 10kb long contiguous stretches (so position 1-10000,then 10001-20000, etc.). This is, of course, a nonsensical way of going about things, because ""counting those up"" in the regions you posted is the same as taking the width of the regions, dividing by 10000, and rounding a bit.</p>\n\n<p>Presumably you have your markers in something like BED format, so just use bedtools to intersect them and do the counting for you.</p>\n","77","","","","2020-02-23T16:27:04.087","","","","0","",""
"11491","2","","11485","2020-02-23T16:32:28.027","0","","<p>There appears to be a dinoflagellate with Luciferase and the diversity of their genes was studied in the Baltic, found described in an article here: <a href=""https://www.tandfonline.com/doi/full/10.1080/09670262.2016.1160441"" rel=""nofollow noreferrer"">https://www.tandfonline.com/doi/full/10.1080/09670262.2016.1160441</a></p>\n\n<p>This article by Smithsonian: <a href=""https://ocean.si.edu/ocean-life/fish/bioluminescence"" rel=""nofollow noreferrer"">https://ocean.si.edu/ocean-life/fish/bioluminescence</a> describes a lot of Luciferin metabolizing species in the oceans, saying that different species independently evolved the forms they use. It describes some of the systems like endosymbiotic bacteria, special tissue, and photo-proteins. </p>\n","7272","","","","2020-02-23T16:32:28.027","","","","0","",""
"11492","1","","","2020-02-23T19:21:53.437","2","23","<p><strong>Problem</strong>: I have low-coverage sequencing data from humans in vcf format. For each sample, I have PL scores / preliminary genotype calls from the software ATLAS (it's a bit like GATK for those who don't know). Because the samples are low coverage (mean 1x, with lots of missing data), I am looking to enhance the genotype calls by using a reference panel and genetic map, before go on to phase the samples. Briefly, the reference panel and genetic map can be used as a kind of prior to inform one about genotypes when there is little data present.</p>\n\n<p><strong>What I've done so far</strong>: So far I have been using Beagle4 with a command:</p>\n\n<pre><code>beagle4 gl=lowCoverageSamples.vcf ref=highCoverageReference.vcf map=geneticMap out=lowCoverageSamples.newCalls.vcf\n</code></pre>\n\n<p>However, Beagle4 is extremely slow, very memory inefficient and quite old. There have been many advances in phasing methods, which are now much more accurate, faster and memory efficient than the beagle4 model. However, I haven't come across any similar improved methods for the genotype re-estimation step.</p>\n\n<p>Does anyone knew of any newer pieces of software which do such a thing, which may more faster and more accurate than Beagle4. It needs to have 2 things in particular:</p>\n\n<p>1) Accept data input in PL score format, as to allow the appropriate representation of uncertainty in low coverage samples.</p>\n\n<p>2) Produce a genotype call / genotype probability for each SNP in each individual as output.</p>\n","3694","","","","2020-02-23T19:21:53.437","Can someone recommend software for reference-based genotype-estimation?","<vcf><genotyping><phasing>","0","0","",""
"11493","1","11496","","2020-02-24T11:02:26.147","2","26","<p>I am seeking a virological database comprising the following information:</p>\n\n<ul>\n<li>vaccine.</li>\n<li>antibody</li>\n<li>receptor.</li>\n</ul>\n\n<p>Preferably hosting multiple strains/samples of a given virus samples of strain.</p>\n\n<p>Any assistance would be welcome.</p>\n","7219","","3848","2020-02-24T11:12:24.107","2020-02-24T14:43:36.787","Database with antigens and current vaccines","<database><phylogenetics><phylogeny>","1","3","",""
"11494","2","","11473","2020-02-24T11:45:11.663","0","","<p>I think you want to prevent snakemake from interpreting your wildcard strings as regular expressions. For this, escape each wildcard value using <code>wildcard_constraints</code>. E.g., towards the top of your Snakefile add:</p>\n\n<pre><code>wildcard_constraints:\n    read= '|'.join([re.escape(x) for x in reads]),\n    sample= '|'.join([re.escape(x) for x in samples])\n</code></pre>\n\n<p>(Personally, I use the <code>wildcard_constraints</code> directive as above quite liberally to constrain every wildcard)</p>\n","339","","","","2020-02-24T11:45:11.663","","","","2","",""
"11495","2","","11489","2020-02-24T12:18:17.070","1","","<p>You can use <a href=""https://github.com/tseemann/barrnap"" rel=""nofollow noreferrer"">Barrnap</a></p>\n\n<pre><code>barrnap -k bac --threads ""$(nproc)"" \\n        -o output_rrna.fna &lt; input_file.fna \\n        &gt; output_rrna.gff 2&gt; /dev/null\n</code></pre>\n\n<p>Put the file in one line of sequence per fasta id and get sequence with 16S annotation.</p>\n\n<pre><code>perl -pe 'if(/\&gt;/){s/\n/\t/}; s/\n//; s/\&gt;/\n\&gt;/' output_rrna.fa | \\n    perl -pe 's/\t/\n/' | tail -n+2 | cut -d\: -f1 | \\n    grep -A1 16S &gt; output_16S.fasta\n</code></pre>\n\n<p>Check the <code>output_rrna.gff</code> and make a BLAST of the sequence to be sure about the predicted 16S.</p>\n","4282","","","","2020-02-24T12:18:17.070","","","","1","",""
"11496","2","","11493","2020-02-24T13:36:36.447","1","","<p>Given ""any virus will do"", the viruses that are ""head and shoulders"" above the rest in terms of volumes of data are HIV and influenza.</p>\n\n<p>The influenza database is <a href=""https://www.fludb.org/brc/home.spg?decorator=influenza"" rel=""nofollow noreferrer"">here</a></p>\n\n<p>The influenza viral spike that attaches to the cell receptor is the HA protein – hemagglutinin. The cell receptor is sialic acid – a small sugar that is attached to many different proteins on the cell surface. You can of course simply search PDB for HA or else Genbank.</p>\n","3848","","3848","2020-02-24T14:43:36.787","2020-02-24T14:43:36.787","","","","0","",""
"11497","1","11515","","2020-02-24T16:05:39.103","0","48","<p>Is there some tool that will convert a vcf/bcf file to some gene annotation, based on the fact that I have the gene annotation of the already exsisting reference genome. Or some tool that will give me the genes based on the most probable chance.</p>\n","6573","","","","2020-02-26T16:37:19.170","vcf file to gene annotations and or genes","<vcf><gene>","1","7","",""
"11498","1","11502","","2020-02-24T17:14:46.217","2","41","<p>I'm looking for a way to download multiple proteomes at once from one clade, as fasta. Probably from the NCBI, because it looks more user-friendly than Ensembl, but Ensembl is okay too. In the best case scenario, I would like to constrain the download to a specific clade (eg. Actinobacteria, Chloroflexi etc.), maybe with a specific RefSeq Category (eg. reference/representative genome) or even depending on the genome assembly level. Command line would probably be the best option, but if there's a direct download option it would be nice to know it as well.\nDoes anything like this exist? Help would be much appreciated!</p>\n","4401","","","","2020-02-26T15:02:06.597","How to download multiple proteomes at once?","<ncbi><ensembl><data-download><proteomics>","2","1","","1"
"11499","2","","10717","2020-02-24T19:45:19.593","1","","<ol>\n<li>Go to <a href=""https://www.ncbi.nlm.nih.gov/nuccore/NC_030853.1?report=fasta&amp;log$=seqview&amp;format=text..."" rel=""nofollow noreferrer"">the main FASTA sequence page</a>, view source, and get the ""ncbi_uidlist"" value.  In this case, it's <code>1061361601</code></li>\n<li><p>Use the ID in the query string to download directly:</p>\n\n<p><a href=""https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?id="" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?id=</a><strong>1061361601</strong>&amp;db=nuccore&amp;report=fasta&amp;retmode=text&amp;withmarkup=on&amp;tool=portal&amp;log$=seqview&amp;maxdownloadsize=100000000</p></li>\n</ol>\n\n<p>You can also get this link directly, by using Chrome's developer tools (F12), viewing the Network tab, then loading the page in (1).</p>\n","7279","","","","2020-02-24T19:45:19.593","","","","0","",""
"11500","2","","11480","2020-02-24T20:00:09.833","1","","<p>There are two resources worth checking: <a href=""https://gtexportal.org/"" rel=""nofollow noreferrer"">GTEx portal</a> and <a href=""https://www.ensembl.org/"" rel=""nofollow noreferrer"">Ensembl</a>.\nUniprot and NCBI will call ""canonical"" the longest sequence comprising valid exons. These are not necessarily the biologically valid ones. </p>\n\n<p>According to Ensembl (<a href=""https://www.ensembl.org/Homo_sapiens/Gene/TranscriptComparison?db=core;g=ENSG00000141510;r=17:7661779-7687550"" rel=""nofollow noreferrer"">TP53</a>, there are a bunch that are experimentally validated and are all class 1. Note the Uniprot column being nearly homologeous: to match the transcripts one has to look at the length, in this case TP53-201 is 393aa long, which is class 1. As mentioned this is not always the case.</p>\n\n<p>So which are more expressed in tissue X?\nIn <a href=""https://gtexportal.org/home/gene/ENSG00000141510"" rel=""nofollow noreferrer"">GTEx</a>, under the collapsed div labelled ""Exon Expression"" press the ""isoform Expression"" tab. The most expressed is ENST00000620739.4 (354aa long) join with ENST00000269305.8 (393aa).</p>\n\n<p>In the Exon Expression you will find out what exons are the issue.</p>\n\n<p>Now. to get the sequence of a transcript or covert a variant Ensembl drops the ball. There is no clear web tab that I know of. Nor an API route.\n<a href=""ftp://ftp.ensembl.org/pub/release-99/fasta/homo_sapiens/pep/Homo_sapiens.GRCh38.pep.all.fa.gz"" rel=""nofollow noreferrer"">ftp://ftp.ensembl.org/pub/release-99/fasta/homo_sapiens/pep/Homo_sapiens.GRCh38.pep.all.fa.gz</a> contains all the sequences however. For a project of mine (<code>ENSTMapper</code> in <a href=""https://github.com/matteoferla/MichelaNGLo-app/blob/master/michelanglo_app/views/transcript.py"" rel=""nofollow noreferrer"">this link</a>) to map an amino acid mutation on one transcript to another I ended up write my own short converter, although there are a few similar tools out there.</p>\n","6322","","","","2020-02-24T20:00:09.833","","","","2","",""
"11501","1","11503","","2020-02-25T00:06:49.247","2","40","<p>I have stranded, paired end RNASeq reads that I have aligned using STAR. I plan do conduct a differential expression analysis with DESeq2. After running quality control checks, a good portion of my reads are aligned but I'm a little concerned about the proportion of properly paired reads</p>\n\n<p><a href=""https://i.stack.imgur.com/5YUeX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5YUeX.png"" alt=""enter image description here""></a></p>\n\n<p>Here is the inner distance plot for the samples</p>\n\n<p><a href=""https://i.stack.imgur.com/hLB1a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hLB1a.png"" alt=""enter image description here""></a></p>\n\n<p>For human RNA sequencing, does anyone have a rule of thumb that they'd consider is an appropriate proportion of properly paired reads? Are there any good papers that talk about this issue? As MultiQC marks the column as yellow, I'm concerned the proper pair percentage may not be satisfactory. </p>\n\n<p>How important are properly paired reads for differential expression analyses?</p>\n","6674","","","","2020-02-26T16:34:36.737","Importance of Proper Pairs vs Aligned Reads for RNASeq data","<rna-seq><sequencing><transcriptome><quality-control><star>","2","0","",""
"11502","2","","11498","2020-02-25T09:01:52.020","1","","<p>You could try <a href=""https://github.com/pmenzel/download-refseq-genomes"" rel=""nofollow noreferrer"">download-refseq-genomes</a>. It will fetch all genomes from the NCBI FTP server that are in a specific subtree of the phylogeny. </p>\n\n<p>For example, downloading the amino acid sequences from all proteins in each genome assembly for all species belonging to Chloroflexi (NCBI taxon ID = <a href=""https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Tree&amp;id=200795&amp;lvl=3&amp;keep=1&amp;srchmode=1&amp;unlock"" rel=""nofollow noreferrer"">200795</a>):</p>\n\n<pre><code>download-refseq-genomes.pl -t faa 200795\n</code></pre>\n\n<p>This will download all assemblies with status=Complete. Use option <code>-a</code> to download all assemblies with any status.</p>\n","1802","","","","2020-02-25T09:01:52.020","","","","0","",""
"11503","2","","11501","2020-02-25T09:16:37.897","1","","<p>Differential gene analysis sees only the read counts. So the proportion of not properly paired reads will have an effect on the counts, if the programme / software used, counts fragments / read pairs instead of single reads. For example HTSeq will count</p>\n\n<blockquote>\n  <p>From <a href=""https://htseq.readthedocs.io/en/release_0.11.1/count.html"" rel=""nofollow noreferrer"">https://htseq.readthedocs.io/en/release_0.11.1/count.html</a></p>\n  \n  <p>For paired-end data, does htseq-count count reads or read pairs? Read\n  pairs. The script is designed to count “units of evidence” for gene\n  expression. If both mates map to the same gene, this still only shows\n  that one cDNA fragment originated from that gene. Hence, it should be\n  counted only once.</p>\n</blockquote>\n\n<p>In Rsubread, featureCounts you set <code>requireBothEndsMapped=TRUE</code> to count fragments.</p>\n\n<p>Short anaswer is if the sequencing is deep enough, you should still have enough reads. In your situation, I estimated that you lose about 40% of your reads if you use only properly paired reads, which means you lose quite a bit of counts.  </p>\n\n<p>The larger question is what reads are you missing? </p>\n\n<p>I see that you have about 20-30% of reads that are non-uniquely mapped, I suppose part of this will contribute towards the not properly paired reads. I would check first whether most of the not properly paired comes from multiple aligned. </p>\n\n<p>Then if not, you can check where these not properly paired reads are falling, if they are some repetitive regions or say rRNA etc, then most likely thats a problem with the library prep..</p>\n","5077","","","","2020-02-25T09:16:37.897","","","","0","",""
"11504","2","","10689","2020-02-25T11:35:12.153","-1","","<p>For comparing a large number of trees side-by-side - Dendroscope.</p>\n\n<p>For combining the trees into a network - Splitstree.</p>\n","4401","","","","2020-02-25T11:35:12.153","","","","0","",""
"11505","1","","","2020-02-25T15:51:58.903","1","39","<p>I was wondering if anyone could help with my little issue in manipulating tsv files in order to calculate estimated coverage across many tsv files. </p>\n\n<p>I have tsv files that look something like this:</p>\n\n<pre><code>ID        Info    Info2  x\nM80428    info    info   1000 \nT83241    info    info   8400\nE81292    info    info   10200\n</code></pre>\n\n<p>I also have another tsv file with all possible IDs and a corresponding number like so:</p>\n\n<pre><code>ID        y\nM80428    27311\nE11123    46531\nP38163    14213\nA34197    12892\nT83241    37416\nQ21345    27321\nE81292    13101\n</code></pre>\n\n<p>x in the first tsv are estimated read counts and the y in the second is the genome length of each ID.</p>\n\n<p>I would like to divide the ""x"" by the ""y"" in the second tsv that has the corresponding ID in order to get the estimated coverage of each hit. e.g. the first row would be 1000/27311, second row 8400/37416. I would then want this to be written to the fifth column of the tsv. </p>\n\n<p>I am working in bash shell mostly here, and think that the way I would probably do this is with awk and by making the second tsv an associative array but I can't work out how exactly to do so. The python package dict seems like another option but my Python skills are practically zero. </p>\n\n<p>Thanks for the help in advance!</p>\n","7287","","7287","2020-02-25T18:56:05.433","2020-02-26T12:40:23.703","Trying to divide number in fourth TSV column by a certain number according to its corresponding identifier in the first column","<python><bash><awk>","2","5","",""
"11506","2","","11505","2020-02-25T17:20:03.553","1","","<p>I would do it in R:</p>\n\n<pre><code>tab1 &lt;- read.table('file1.tsv', header = T, stringsAsFactors = F)\ntab2 &lt;- read.table('file2.tsv', header = T, stringsAsFactors = F)\nmerged_tav &lt;- merge(reads_tab, genome_len_tab)\nmerged_tav<span class=""math-container"">$z &lt;- merged_tav$</span>x / merged_tav$y\nmerged_tav\n</code></pre>\n\n<p>returns</p>\n\n<pre><code>      ID Info Info2     x     y          z\n1 E81292 info  info 10200 13101 0.77856652\n2 M80428 info  info  1000 27311 0.03661528\n3 T83241 info  info  8400 37416 0.22450289\n</code></pre>\n\n<p>note that only IDs that are in both tables will be preserved.</p>\n","57","","57","2020-02-26T12:40:23.703","2020-02-26T12:40:23.703","","","","0","",""
"11507","1","","","2020-02-26T04:20:18.240","0","20","<p>Hope all of you are fine. I am wondering if exists a program easy to use that can bring me Basic Statistics of Quality Control from raw data made by MinION device (nanopore).</p>\n\n<p>I have read about FASTAQC and Porechop. My sequencing raw data was a result of a metagenomics analysis (16s and 18s). My main objective is only to filter the sequences and obtain a basic statistic about the lenght and the number of sequences ordered by the lenght. Thank you ladies and gentlemen</p>\n\n<p>PD Sorry for my poor english. Day to day i'm trying to improve it.</p>\n","7291","","","","2020-02-26T07:23:06.887","Program Quality control for data sequencing (nanopore)","<nanopore>","2","0","",""
"11508","2","","11507","2020-02-26T07:06:53.893","0","","<p>A simple start would be to use <a href=""https://github.com/roblanf/minion_qc"" rel=""nofollow noreferrer"">MinIONQC</a>, which is also available via bioconda under the name <code>r-minionqc</code>.</p>\n","1802","","","","2020-02-26T07:06:53.893","","","","0","",""
"11509","2","","11507","2020-02-26T07:23:06.887","0","","<p>I assume raw data is in Fastq format, but ideally you should also use the sequencing_summary.txt file generated by the basecaller. Other alternatives for QC are <a href=""https://github.com/a-slide/pycoQC"" rel=""nofollow noreferrer"">PycoQC</a> and <a href=""https://github.com/wdecoster/NanoPlot"" rel=""nofollow noreferrer"">NanoPlot</a>. I developed and maintain NanoPlot, so let me know if there is something I can help you with.</p>\n","681","","","","2020-02-26T07:23:06.887","","","","0","",""
"11510","2","","11505","2020-02-26T10:32:27.757","2","","<p>Python dictionaries are indeed a good tool for that.</p>\n\n<p>Here is a script that takes 2 arguments (the file containing the counts and the file containing the genome length info), and prints the input data, with an added column containing coverage:</p>\n\n<pre><code>#!/usr/bin/env python3\nimport sys\n\ncounts_fname = sys.argv[1]\ngenome_lengths_fname = sys.argv[2]\n\ngenome_length_dict = {}\nwith open(genome_lengths_fname) as genome_lengths_f:\n    # Checking the header line, for extra safety\n    header = genome_lengths_f.readline().strip().split(""\t"")\n    assert header[0] == ""ID""\n    assert header[1] == ""y""\n    # Reading the data lines\n    for line in genome_lengths_f:\n        [genome_id, genome_length] = line.strip().split(""\t"")\n        genome_length_dict[genome_id] = int(genome_length)\n\nwith open(counts_fname) as counts_f:\n    print(""ID\tInfo\tInfo2\tx\tx/y"")\n    # Checking the header line, for extra safety\n    header = counts_f.readline().strip().split(""\t"")\n    assert header[0] == ""ID""\n    assert header[1] == ""Info""\n    assert header[2] == ""Info2""\n    assert header[3] == ""x""\n    # Reading the data lines\n    for line in counts_f:\n        [genome_id, info1, info2, counts] = line.strip().split(""\t"")\n        # /!\ It would be different in python 2:\n        # different print and division behaviours\n        print(\n            genome_id, info1, info2, int(counts),\n            int(counts) / genome_length_dict[genome_id],\n            sep=""\t"")\n\nsys.exit(0)\n</code></pre>\n\n<p>Testing it:</p>\n\n<pre><code>$ ./coverage.py x.tsv y.tsv\nID  Info    Info2   x   x/y\nM80428  info    info    1000    0.0366152832192157\nT83241  info    info    8400    0.22450288646568314\nE81292  info    info    10200   0.7785665216395695\n</code></pre>\n","292","","292","2020-02-26T10:37:34.387","2020-02-26T10:37:34.387","","","","1","",""
"11511","1","11523","","2020-02-26T13:47:34.340","0","41","<p>I'm downloading translated viral genomes initially via Blast - which had shortcomings - and now by <code>efetch</code>.</p>\n\n<p>What I want to do is obtain a complete list of all translated protein ids from a nucleotide Genbank entry.</p>\n\n<pre><code>efetch -db nucleotide -format gpc -id MT039887 | xtract -insd ....\n</code></pre>\n\n<p>The protein id and the translated gene sequences are within the <code>efetch -db nucleotide -format gpc -id MT039887</code> output ... but how do I use <code>xtract</code> to parse them?</p>\n","3848","","3848","2020-02-27T18:11:28.230","2020-02-27T18:11:28.230","efetch nucleotide -> protein ids","<phylogenetics><genome><ncbi><data-download><efetch>","1","0","",""
"11512","1","","","2020-02-26T13:59:05.083","2","18","<p>I have two sets of PacBio reads from genomic DNA of an Aspergillus species that were made from separate preps of  the culture. One of them has two additional peaks at 38% and 60% in the percent GC histogram produced by FastQC. Do these additional peaks indicate that there is contamination in that sample?</p>\n\n<p><a href=""https://i.stack.imgur.com/IdvnD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IdvnD.png"" alt=""enter image description here""></a></p>\n","1454","","","","2020-02-26T13:59:05.083","Do additional peaks in percent GC of PacBio gDNA reads indicate contamination?","<genome><pacbio>","0","1","",""
"11513","2","","11498","2020-02-26T14:30:56.180","0","","<p>You can use NCBI resources to download these data directly. For the entire proteome, you can download the sequences of all proteins annotated on the genome assembly in FASTA format. In the NCBI FTP directories, these files have the names with the suffix <code>_protein.faa.gz</code>. More details about the files in the FTP directories are described in the <a href=""https://ftp.ncbi.nlm.nih.gov/genomes/all/README.txt"" rel=""nofollow noreferrer"">README</a>. </p>\n\n<h2>Command-line method using EDirect</h2>\n\n<p>This method uses <a href=""https://www.ncbi.nlm.nih.gov/books/NBK179288/"" rel=""nofollow noreferrer"">Entrez Direct</a> to first query the Assembly database for all available genomes, parses the output to extract the ftp path and downloads data using wget command. </p>\n\n<pre><code>esearch -db assembly -query 'txid200795[organism] AND lastest_refseq[filter]' \\n  | esummary \\n  | xtract -pattern DocumentSummary -element FtpPath_RefSeq \\n  | while read -r url ; do \n    path=$(echo $url | perl -pe 's/(GC[FA]_\d+.*)/\1\/\1_protein.faa.gz/g') ; \n    wget -q --show-progress ""$path"" -P protein_data ; \n  done\n</code></pre>\n\n<p>Here, I am restricting the results to latest RefSeq data by using the <code>latest_refseq[filter]</code> but you can use any other Entrez filters to further narrow down your results. Along the same lines, you can modify the code minimally as shown below to download multiple file types: </p>\n\n<pre><code>esearch -db assembly -query 'txid200795[organism] AND latest_refseq[filter]' \\n  | esummary \\n  | xtract -pattern DocumentSummary -element FtpPath_RefSeq \\n  | while read -r url ; do \n    base_path=$(echo $url | perl -pe 's/(GCF_\d+.*)/\1\/\1_/g') ; \n    for suf in genomic.fna.gz protein.faa.gz ; do\n      wget -q --show-progress ""$base_path$suf"" -P ncbi_data ; \n    done ;\n  done\n</code></pre>\n\n<h2>GUI method</h2>\n\n<p>You can use the same query as shown above in the <a href=""https://www.ncbi.nlm.nih.gov/assembly/"" rel=""nofollow noreferrer"">NCBI Assembly</a> portal and then use the 'Download Assemblies' button to pick and choose the type of files you would like to download. By default, if you do not select any assemblies in the results, this interface downloads files for all results.\n<a href=""https://i.stack.imgur.com/ZbQY9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZbQY9.png"" alt=""enter image description here""></a></p>\n","3577","","3577","2020-02-26T15:02:06.597","2020-02-26T15:02:06.597","","","","0","",""
"11514","2","","11501","2020-02-26T16:34:36.737","2","","<p>If you check the <a href=""http://rseqc.sourceforge.net/#inner-distance-py"" rel=""nofollow noreferrer"">RSeQC docs</a> you can get an explanation of how the inner distance stats work. By the look of your inner-distance MultiQC plot I guess that your sequencing is 2x100bp reads. This plot shows that your insert sizes on the pairs is pretty short, meaning that a lot of the paired reads are overlapping significantly. This will be affecting the alignments a little, resulting in the low proportion of properly paired reads.</p>\n\n<p>The good news is that for differential expression, this probably doesn't really matter too much. What is important is getting fragment counts, so as long as at least one of the pair mates is mapping properly, then you will will be getting sensible counts - even if read 2 is not mapping properly. You should be able to have confidence in your results from multiple replicates if in doubt - you have more than just <code>-rep1</code> right? ;)</p>\n\n<p>Note that the yellow background in the MultiQC column is not passing judgement with categories like pass/warn/fail in FastQC - it just happens that the colour scheme ended up there for your samples. It's a continuous gradient from red to yellow to green for those values (0-100%).</p>\n","224","","","","2020-02-26T16:34:36.737","","","","0","",""
"11515","2","","11497","2020-02-26T16:37:19.170","2","","<p>Creating a FASTA file by applying variants found in a VCF file is called ""creating a consensus FASTA file"". You can use <a href=""https://samtools.github.io/bcftools/bcftools.html#consensus"" rel=""nofollow noreferrer""><code>bcftools consensus</code></a>. Unfortunately, the tools only works genome-wide (at the FASTA level), so you might need to use a different tool to extract a gene sequence from the FASTA file using the gene's co-ordinates.</p>\n\n<p>If the VCF file is annotated with HGVS notations for the variants, there might be a way to use those notations to apply the edit to the reference gene/transcript sequence directly. I know <a href=""https://hgvs.readthedocs.io/en/stable/"" rel=""nofollow noreferrer"">the hgvs python module</a> works well with the HGVS notation, but I don't see a way to ""apply"" the variant to the reference sequence.</p>\n","650","","","","2020-02-26T16:37:19.170","","","","0","",""
"11516","1","","","2020-02-26T17:15:15.317","2","35","<p>I have a <code>data.frame</code> with results from a CRISPR screen.</p>\n\n<p>I want to calculate the % enrichment of a variable of interest <code>var</code> from baseline to treatment group.</p>\n\n<p>Here is the distribution of <code>var</code> in linear:\n<a href=""https://i.stack.imgur.com/cDOXS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cDOXS.png"" alt=""enter image description here""></a></p>\n\n<p>I can do scale <code>var</code> with log transformation\n<a href=""https://i.stack.imgur.com/tzXXW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tzXXW.png"" alt=""enter image description here""></a></p>\n\n<p>But many rows with <code>var</code> = 0 will be lost.\nMoreover, I cannot compare % enrichment by dividing <code>treatmentgroup::var</code> to <code>Control/baseline::var</code> because I will run into the problem of dividing by zero.</p>\n\n<p>I've seen people circumvent this problem by adding a small value ex)0.01 to the entire column of <code>var</code>.\nBut I'm wondering if there's a better way of circumventing this problem. For example, a simple transformation that can deal with zeros and negative values.</p>\n\n<p>Appreciate any input!</p>\n","7056","","","","2020-02-26T17:15:15.317","How to deal with zeros in the denominator when calculating percent enrichment?","<r><statistics>","0","4","",""
"11517","1","","","2020-02-26T19:13:17.757","0","45","<p>I have had a segmentation file (copy number) like below</p>\n\n<pre><code>  Sample      Chromosome  Start      End          Segment_Mean\nLP6008334-DNA_C02   1     10583      62271      -0.992452416\nLP6008334-DNA_C02   1   17201069    17230953    -0.992452416\nLP6008334-DNA_C02   1   17235034    17235196     0.592510084\nLP6008334-DNA_C02   1   142781738   142967188   -0.992452416\n</code></pre>\n\n<p>I was using bedtools to count the number of markers in above genomic range (Start, End) when I got as below</p>\n\n<pre><code>1   10583   62271   6\n1   10583   62271   6\n1   10583   750153  75\n1   14907   1062025 106\n1   121009  245455  13\n1   404187  5719467 532\n1   732809  5726842 500\n1   821926  1120377 31\n1   952428  1179983 23\n1   1181751 1469134 29\n1   3458681 143542402   14010\n1   5726844 5736229 2\n1   5735836 16832446    1111\n1   5736682 8081290 236\n1   8353020 8476455 13\n</code></pre>\n\n<p>Basically the same files but the second one has extra last column for the number of marker but sorted one I then I can not matched this with the above sample names and Segment_Mean</p>\n\n<p>How I can add the corresponding sample names and Segment_Mean to my second file?</p>\n\n<p>I have tried this where I am getting this mess</p>\n\n<pre><code>DF &lt;- left_join(df_res1,df, by = ""Start"")\n\n&gt; head(DF)\n           Sample.x Chromosome.x     Start     End.x Num_Probes          Sample.y Chromosome.y     End.y\n1 LP6008334-DNA_C02            1     10583     62271          9 LP6008334-DNA_C02            1     62271\n2 LP6008334-DNA_C02            1     10583     62271          9 LP6008337-DNA_A07            1     62271\n3 LP6008334-DNA_C02            1     10583     62271          9 LP6008460-DNA_F02            1    750153\n4 LP6008334-DNA_C02            1  17201069  17230953         20 LP6008334-DNA_C02            1  17230953\n5 LP6008334-DNA_C02            1  17235034  17235196          1 LP6008334-DNA_C02            1  17235196\n6 LP6008334-DNA_C02            1 142781738 142967188         40 LP6008334-DNA_C02            1 142967188\n&gt; \n</code></pre>\n\n<p>Also with this I have some overlaps like</p>\n\n<pre><code>300 segment overlaps detected in file '/temp/hgig/fi1d18/man/examplefiles/res1.txt'.\n</code></pre>\n\n<p>First overlap detected between segments at lines 1 and 9.</p>\n","4595","","4595","2020-02-27T09:43:12.013","2020-02-27T09:43:12.013","Joining columns to a sorted file","<r><bedtools><wgs><cnv>","1","2","",""
"11518","1","","","2020-02-26T19:49:00.570","0","12","<p>I'm using cdbfasta command to retrieve sequences using ids, all works well but I find some sequences repeated in my output and the command does not retrieve all the sequences I want. </p>\n","6328","","","","2020-02-26T19:49:00.570","Retrieve FASTA sequences using sequence ids in cdbfasta","<fasta><data-retrieval>","0","1","",""
"11519","2","","11517","2020-02-27T00:09:09.077","2","","<p>Rearrange your text file into a sorted BED file:</p>\n\n<pre><code><span class=""math-container"">$ tail -n+2 segmentation.txt | awk -vFS=""\t"" -vOFS=""\t"" '{ print $2, $3, $4, $</span>1 }' | sort-bed - &gt; segmentation.bed\n</code></pre>\n\n<p>Use BEDOPS <code>bedmap</code> with <code>--echo-map-id-uniq</code> and <code>--count</code> operators on merged regions:</p>\n\n<pre><code>$ bedmap --delim '\t' --echo --count --echo-map-id-uniq --fraction-map 1 &lt;(bedops --merge segmentation.bed) segmentation.bed &gt; answer.bed\n</code></pre>\n\n<p>The file <code>answer.bed</code> will contain merged intervals from <code>segmentation.bed</code> in the first three columns, the number of intervals from <code>segmentation.bed</code> which are contained entirely within the merged interval in the fourth column, and a unique listing of IDs of those contained intervals in the fifth column.</p>\n","776","","","","2020-02-27T00:09:09.077","","","","5","",""
"11520","1","","","2020-02-27T02:54:34.880","1","40","<p>This is my first post here. I can think of no way of giving an easily reproducible example, as per the stack ethos. SO apologies in advance and any feedback on question format appreciated.</p>\n\n<p>I have called samtools mpileup (Version: 1.9 (using htslib 1.9)) on some oxford nanopore data (mapped to GRCh38).</p>\n\n<p>I am seeing some instances like below (grouped with pythons Counter), here reference is T at this position:</p>\n\n<pre><code> Counter({',': 34, '.': 33, 'c': 9, 'g': 6, 'a': 4, 't': 2, '-:ACTC': 1, '-:ACTCGG': 1, '-:5': 1, '$': 1, '*': 1})\n</code></pre>\n\n<p>Why is 't': 2' in there?</p>\n\n<p>flags used: -A -B -Q 1 -R -x</p>\n","7153","","","","2020-02-27T16:47:40.930","Why does samtools mpileup sometimes include ref bases (other than ',' or '.')?","<python><samtools><long-reads><mpileup>","1","3","",""
"11522","1","","","2020-02-27T15:31:33.137","1","23","<p>I know for example <code>PolyX</code> from Pierre's <a href=""http://lindenb.github.io/jvarkit/"" rel=""nofollow noreferrer""><code>Jvarkit</code></a> to find the number of repeated REF bases (homopolymers) around a position in a VCF file as described <a href=""https://www.biostars.org/p/140069/"" rel=""nofollow noreferrer"">here</a> exists.</p>\n\n<p>I would like to know however if there tools that describes tandem repeats around a reported variant (TR) up to 6 repeated bases. I can imagine 3 different scenarios:</p>\n\n<p><strong>Scenario 1:</strong></p>\n\n<pre><code>chrK    posY    C     G\n\nTR of (ATC)\n____________v\nATCATCATCATCGATGATAC\n</code></pre>\n\n<p><strong>Scenario 2:</strong></p>\n\n<pre><code>chrK    posY    A     T\n\n             TR of (AACT)\n            v____________\nCAGTCGATTGGATAACTAACTAACT\n</code></pre>\n\n<p><strong>Scenario 3:</strong></p>\n\n<pre><code>chrK    posY    C     A\n\nTR of (AG)    TR of (AG)\n____________v__________\nAGAGAGAGAGAGCAGAGAGAGAG\n</code></pre>\n\n<p>If you know any tools in mind that can be used for VCFs, I'd really appreciate citing them.</p>\n\n<p>Thanks in advance.</p>\n","4413","","4413","2020-02-27T15:43:13.550","2020-02-27T20:35:01.080","Are there any tools for discovering tandem repeats around a variant in a VCF file?","<vcf><repeat-elements>","2","0","","1"
"11523","2","","11511","2020-02-27T15:51:05.097","1","","<p>If you want the cds in fasta format, try:</p>\n\n<pre><code>esearch -db nucleotide -query MT039887 | efetch -format fasta_cds_na\n</code></pre>\n\n<p>And for the proteins sequence in fasta format:</p>\n\n<pre><code>elink -db nucleotide -target protein -id MT039887 | efetch -format fasta \n</code></pre>\n","4282","","","","2020-02-27T15:51:05.097","","","","1","",""
"11524","2","","11522","2020-02-27T15:58:14.937","1","","<p>After researching a bit, I found a couple of tools that take as input BAM files. I did not test them yet:</p>\n\n<ul>\n<li><a href=""https://github.com/gymreklab/GangSTR"" rel=""nofollow noreferrer"">GangSTR</a></li>\n<li><a href=""https://github.com/mgymrek/lobstr-code"" rel=""nofollow noreferrer"">lobSTR</a></li>\n<li><a href=""https://github.com/tfwillems/HipSTR"" rel=""nofollow noreferrer"">hipSTR</a></li>\n<li><a href=""https://tandem.bu.edu/trf/trf.html"" rel=""nofollow noreferrer"">Tandem Repeat Finder</a> (thanks to Michael G.'s answer)</li>\n</ul>\n\n<p>If I find more tools, I will update this post. After testing them, I will come back to share my feedback.</p>\n","4413","","4413","2020-02-27T20:35:01.080","2020-02-27T20:35:01.080","","","","0","",""
"11525","2","","11520","2020-02-27T16:47:40.930","0","","<blockquote>\n  <p>Why is 't': 2' in there?</p>\n</blockquote>\n\n<p>How else do you think the pileup should show you that two reads (in the reverse direction) have a T in that position?</p>\n","2388","","","","2020-02-27T16:47:40.930","","","","2","",""
"11526","2","","11522","2020-02-27T20:05:25.973","1","","<p>The old program for doing this last updated in 2016 is Tandem Repeat Finder (TRF) <a href=""https://tandem.bu.edu/trf/trf.html"" rel=""nofollow noreferrer"">here</a>. This will not find a microsatellite sequence at a given variant, but instead find all tandem repeats in the genome and their position. It has been around for a long time.</p>\n\n<p>You would then simply screen this output against the position of your variant. TRF may have been modified however to perform a search within given nucleotide boundaries.</p>\n","3848","","","","2020-02-27T20:05:25.973","","","","1","",""
"11527","2","","11250","2020-02-28T14:40:04.963","1","","<p>Way, there are multiple ways to get same genetic information for 8000amino acid's then being ""evolutionary ancestor"". They are related more like people who studied same school, than having same parents when it comes to viruses.</p>\n\n<p>There isn't much space for mutation in this short string of text, that would allow them to be different for independently evolved virus..., it's like e-coli from different sources in environment with antibiotics present, they both get same mutation, because there is not many possible mutations, which would allow them for survival...</p>\n\n<p>If we evolve MKT and MMT and let only the MFT pass, with random mutations, no same parents, but same result, due to environment they have to fit in.</p>\n","7219","","","","2020-02-28T14:40:04.963","","","","0","",""
"11528","1","","","2020-02-28T17:58:40.910","1","49","<p>I have the data below. I need to use a clustering method to classify them and into categories of ""Heterozygotote, Allele 1, Allele 2 and No Call. The values in RFU1 and RFU2 are used to determine the call variable.</p>\n\n<p>Any values in RFU1 and RFU2 that are negative should automatically be classified as No Call. I want to develop my own validation check to check the PCR program calls.</p>\n\n<p><strong>Data</strong></p>\n\n<pre><code>structure(list(Well = structure(1:96, .Label = c(""A01"", ""A02"", \n""A03"", ""A04"", ""A05"", ""A06"", ""A07"", ""A08"", ""A09"", ""A10"", ""A11"", \n""A12"", ""B01"", ""B02"", ""B03"", ""B04"", ""B05"", ""B06"", ""B07"", ""B08"", \n""B09"", ""B10"", ""B11"", ""B12"", ""C01"", ""C02"", ""C03"", ""C04"", ""C05"", \n""C06"", ""C07"", ""C08"", ""C09"", ""C10"", ""C11"", ""C12"", ""D01"", ""D02"", \n""D03"", ""D04"", ""D05"", ""D06"", ""D07"", ""D08"", ""D09"", ""D10"", ""D11"", \n""D12"", ""E01"", ""E02"", ""E03"", ""E04"", ""E05"", ""E06"", ""E07"", ""E08"", \n""E09"", ""E10"", ""E11"", ""E12"", ""F01"", ""F02"", ""F03"", ""F04"", ""F05"", \n""F06"", ""F07"", ""F08"", ""F09"", ""F10"", ""F11"", ""F12"", ""G01"", ""G02"", \n""G03"", ""G04"", ""G05"", ""G06"", ""G07"", ""G08"", ""G09"", ""G10"", ""G11"", \n""G12"", ""H01"", ""H02"", ""H03"", ""H04"", ""H05"", ""H06"", ""H07"", ""H08"", \n""H09"", ""H10"", ""H11"", ""H12""), class = ""factor""), Sample = c(NA, \nNA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \nNA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \nNA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \nNA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \nNA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \nNA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), \n    Call = structure(c(4L, 4L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 1L, \n    3L, 3L, 4L, 4L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 1L, 4L, \n    4L, 1L, 1L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 1L, 1L, \n    3L, 3L, 3L, 3L, 1L, 1L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, \n    1L, 2L, 2L, 1L, 1L, 3L, 3L, 1L, 1L, 4L, 1L, 1L, 1L, 1L, 1L, \n    1L, 1L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, \n    2L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L), .Label = c(""Allele 1"", \n    ""Allele 2"", ""Heterozygote"", ""No Call""), class = ""factor""), \n    Type = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, \n    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, \n    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, \n    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, \n    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, \n    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, \n    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = ""Auto"", class = ""factor""), \n    RFU1 = c(-0.295502405, 0.964070798, 3381.332182, 3532.769062, \n    3431.836843, 3242.966511, 2104.791167, 2220.008503, 3548.252161, \n    3506.51418, 2290.273178, 2281.587684, -5.64819475, -11.73109864, \n    3784.914039, 3619.00781, 3618.211608, 3248.106466, 3394.650325, \n    3339.870196, 2449.202902, 2426.835174, 3432.153478, 2761.392304, \n    -9.267907504, -7.365704356, 3743.092314, 3787.241702, 2172.027787, \n    2096.845649, 2135.649551, 2149.145547, 2293.757257, 2348.099108, \n    2321.019045, 2022.168867, -17.93532331, -12.59832941, 3805.416768, \n    3498.998138, 2304.597239, 2509.63987, 2181.11547, 2261.011876, \n    3432.453036, 3662.758933, 2371.11049, 3068.827061, 2612.107589, \n    2687.824075, 3179.315918, 3688.525218, 3465.327523, 3405.154043, \n    2535.514915, 2452.200079, 374.435299, 423.6015308, 3742.515563, \n    3578.777925, 2634.955017, 2527.514043, 3817.579252, 3550.999412, \n    -10.72035816, 3294.486334, 3352.40368, 3463.150507, 3472.576514, \n    3741.898759, 3571.369947, 3720.645869, 3739.569593, 3855.583168, \n    418.6837047, 49.47548241, 2171.034284, 2155.314713, 3432.363384, \n    3582.508917, 3425.415274, 3487.203299, 3505.23909, 3413.342096, \n    113.5100691, 128.6414053, 2454.588175, 2323.061591, 3188.705702, \n    3376.950546, 3291.072437, 3181.001961, 3195.013863, 3776.919197, \n    2284.22659, 2277.338631), RFU2 = c(-8.346468029, 235.4058561, \n    637.9218251, 650.3759507, 617.4161748, 604.0792911, 4270.310727, \n    4199.615749, 689.863543, 712.6144338, 4274.287194, 4541.168491, \n    -1.626221758, -2.437395631, 802.0941252, 730.5998997, 686.9037384, \n    625.8245403, 644.3586836, 642.8833044, 4937.691887, 5159.479928, \n    725.4449756, 573.3910899, -4.006398006, 213.2859144, 739.7910786, \n    731.0150586, 4408.81923, 4767.533969, 4302.641493, 4325.913445, \n    4597.47663, 4666.904418, 4800.357526, 4142.535329, -17.23239968, \n    178.5311942, 778.305843, 743.1438168, 4214.507094, 4553.703511, \n    4629.339014, 4459.697405, 661.7299014, 727.1054982, 4553.170272, \n    5482.231486, 4520.517999, 4737.802036, 643.3599887, 726.4314715, \n    696.5968338, 697.6099599, 411.8118071, 409.4943424, 5687.32635, \n    5757.51512, 766.4240193, 779.2403225, 4745.055632, 4582.267792, \n    749.5679421, 675.8747055, -7.254521898, 628.3467565, 631.116767, \n    672.7064514, 687.2642132, 718.1192917, 731.785499, 668.3686048, \n    784.8055727, 791.3155894, 4471.047168, 4501.597841, 4504.670332, \n    4442.621066, 682.0632225, 706.6204595, 680.5242182, 683.9558692, \n    684.2909706, 618.6535251, 5727.684954, 6098.485474, 5099.952926, \n    4779.742057, 571.4303822, 614.9258218, 602.9830491, 651.2847695, \n    591.8833499, 742.2387568, 4443.376841, 4716.792177)), class = ""data.frame"", row.names = c(NA, \n-96L))\n</code></pre>\n\n<p><strong>What I have tried so far</strong></p>\n\n<pre><code>library(cluster)\nlibrary(factoextra)\nlibrary(formattable)\n\ndf &lt;- df[,c(1,5,6)]\ndf<span class=""math-container"">$RFU1[df$</span>RFU1 &lt; 0] &lt;- 0\ndf<span class=""math-container"">$RFU2[df$</span>RFU2 &lt; 0] &lt;- 0\ndf<span class=""math-container"">$RFU1 &lt;- formattable(df$</span>RFU1, digits = 2, format = ""f"")\ndf<span class=""math-container"">$RFU2 &lt;- formattable(df$</span>RFU2, digits = 2, format = ""f"")\ndf<span class=""math-container"">$Well &lt;- as.numeric(df$</span>Well)\n\n\nclusters &lt;- kmeans(df, centers = 4)\nKmeans_plot &lt;- fviz_cluster(clusters, data = df)\n</code></pre>\n\n<p><strong>This is the plot generated</strong>\n<a href=""https://i.stack.imgur.com/Rr86P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rr86P.png"" alt=""enter image description here""></a></p>\n\n<p>Points in the top right 57,58,75,76,85,86 should be in another cluster (classed as allele 2 but placed in cluster 1 (rest of points in this cluster are heterozygote)</p>\n\n<p>Also points 24, 55, 56 should be clustered with cluster 2 as all these points are classed as allele 1</p>\n\n<p><strong>I need an algorithm that can detect optimal cluster center, some PCR runs may only have 3 different call variables instead of 4</strong></p>\n\n<p><strong>Programs output - Software company stated their algorithm isn't sophisticated</strong></p>\n\n<p><a href=""https://i.stack.imgur.com/D69v9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D69v9.jpg"" alt=""enter image description here""></a></p>\n","7305","","7305","2020-02-29T00:16:14.107","2020-02-29T00:16:14.107","K means clustering, would PCA be a better option?","<r><clustering><k-mer><pca><pcr>","3","0","",""
"11529","2","","11528","2020-02-28T18:34:25.293","0","","<p>What happens when </p>\n\n<pre><code>clusters &lt;- kmeans(df, centers = 3)\n</code></pre>\n\n<p>Anyway K means is pretty good and hasn't done a bad job of your data, although I don't know what you mean by the ""cluster centre"". There are methods to establish 'centroids'.</p>\n\n<p>With K-means you need to vary the group size or have a very good a priori why it is 4, rather than any other number.</p>\n\n<p>The analysis to perform is tSNE ... either by itself or else following PCA. This will provide a clear a priori of the number of groups in your analysis. I don't know R, but it is present in scikit-learn in Python.</p>\n\n<hr>\n\n<p>Your observation that some of points are in the wrong cluster ... increase the number of groups in k-means, or switch to PCA, tSNE or PCA-tSNE. If the points remain in the ""wrong groups"" in all analyses at different group sizes for k-means, then basically thats the result. K-means is good, but not perfect, and misclassification will happen. </p>\n\n<p>Another route is the ML repertoire of lasso regression, ridge regression, decision trees, random forests etc ... they all do the same basic job but I'd look at K-means, PCA, tSNE first and if ok ... last. Regression style calculations will give different answers. </p>\n\n<hr>\n\n<p>t-SNE is a dimensionality reduction approach to produce a bivariate plot and the fondness of this approach is purely observational rather than empirical. It works and also works very well following PCA. Its the method used in Seurat. K-means is used in ML, and certainly has a fan base, but it is not to say it will give you a correct answer.</p>\n","3848","","3848","2020-02-28T20:23:44.057","2020-02-28T20:23:44.057","","","","2","",""
"11530","2","","11528","2020-02-28T18:54:22.057","1","","<p>I would stay away from using k-means, and instead use a method that doesn't <em>a priori</em> define a number of clusters to detect. It also looks as though your clusters aren't exactly spherical, which is an assumption of k-means. I personally am a fan of <code>dbscan</code>, which is available in the R package of the same name. The other poster recommended t-SNE (available in the <code>Rtsne</code> package) but wasn't exactly clear on how / why to use it. t-SNE is a nonlinear dimension reduction technique used specifically for visualization, so you could cluster your points in <em>n</em>-dimensional PCA space and then plot the results in two-dimensional t-SNE space. I think the other poster was recommending plotting your data in t-SNE space before clustering to get an idea of how many true clusters are available, which is another option you could pursue.  </p>\n","6232","","","","2020-02-28T18:54:22.057","","","","1","",""
"11531","1","","","2020-02-28T19:09:25.193","0","11","<p>did anyone know how to order the cells of samples according to the control cells trajectory? As we can easily order cells(A) according to the pseudo time(B), as shown in the picture. But if I would like to get the control cells trajectory first, and then plot the Estrogen cell on the control cells trajectory, is that possible? If it is possible, how can I do it? Thank you!\n<a href=""https://i.stack.imgur.com/hNJoC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hNJoC.png"" alt=""enter image description here""></a></p>\n","3171","","","","2020-02-28T19:09:25.193","how to order cells by self setting trajectory","<single-cell><monocle>","0","0","",""
"11532","2","","11528","2020-02-28T21:46:06.040","0","","<p>I don't think clustering algorithms are working here, it looks like you can do this much simpler by looking at whatever determines the X axis in your plot.  The samples split cleanly when looking in just one dimension.</p>\n","2388","","","","2020-02-28T21:46:06.040","","","","4","",""
"11533","1","","","2020-02-28T22:56:38.310","0","14","<p>I have 2 dataframe with same genes both different data sets</p>\n\n<p>like </p>\n\n<pre><code>&gt; head(heat1[1:3,1:4])\n             A10         A11        A12        A2\nATP5F1 2.1163092  0.32044094 -0.5661269 0.7978236\nDDX5   0.6277182 -1.67055853 -0.0123082 0.8895472\nEEF1G  1.6459212  0.02923401 -1.2737676 0.5842162\n&gt; \n\n&gt; head(heat2[1:3,1:4])\n              A10        A12          A3          A4\nA2M    -0.6031969  0.6378864 0.000573381 -0.07769313\nABCB11  0.5600164  0.3087120 0.171636836  1.25681512\nABCC2   0.2093133 -0.5608016 0.782278709  0.16618683\n&gt;\n</code></pre>\n\n<p>I want to plot them side by side with the same order of genes </p>\n\n<p>I tried</p>\n\n<pre><code>&gt; ht1 = Heatmap(heat1)\n&gt; ht2 = Heatmap(heat2)\n&gt; ht_list = ht1 + ht2\n&gt; draw(ht_list, ht_gap = unit(1, ""cm""))\nWarning message:\nRow names of heatmap 2 is not consistent as the main\nheatmap (1) \n</code></pre>\n\n<p>But I getting not the same order of genes</p>\n\n<p><a href=""https://i.stack.imgur.com/4Lojp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Lojp.png"" alt=""enter image description here""></a></p>\n\n<p>Can you help me with this?</p>\n","4595","","4595","2020-02-29T00:14:26.820","2020-02-29T02:04:28.177","Drawing 2 heatmaps with the same gene order","<r><complexheatmap>","1","0","",""
"11534","2","","11475","2020-02-28T23:15:55.373","0","","<p>""Mutually exclusive"" is not a precise statistical term because it could be seen as independence. </p>\n\n<p>Anyway if truly mutually exclusive AND their behaviour follows periodicity, i.e. sine waves, which it might do in a cell cycle, i.e. the cells divide at periodic intervals, you can do this via asynchronicity. </p>\n\n<p>If the gene expression can be described under periodicity analysis you simply sum the amplitude between two genes. When the amplitude is zero ... they are expression is antogonistic - when one is expressed the other is silent. </p>\n\n<p>Amplitude simply means the height of the period.</p>\n\n<p>Periodicity analysis is quite simple if you have uniform time measurements. </p>\n\n<p>If you have 'missing data', i.e. you time intervals are not uniform, for example you skipped a reading then this is called missing data in periodicity analysis. Missing data periodicity analysis is complicated, but its doable.</p>\n\n<p>So in this context its quite simple, you would identify the periodicity of all genes and construct a 2x2 matrix subtracting their respective amplitudes. It would be kinda juicy to do. </p>\n","3848","","","","2020-02-28T23:15:55.373","","","","0","",""
"11535","2","","11533","2020-02-29T02:04:28.177","1","","<p>Please read through the <a href=""https://jokergoo.github.io/ComplexHeatmap-reference/book/"" rel=""nofollow noreferrer"">ComplexHeatmap manual</a>, especially the section on row and column orders: <a href=""https://jokergoo.github.io/ComplexHeatmap-reference/book/a-single-heatmap.html#get-orders-and-dendrograms-from-heatmap"" rel=""nofollow noreferrer"">https://jokergoo.github.io/ComplexHeatmap-reference/book/a-single-heatmap.html#get-orders-and-dendrograms-from-heatmap</a></p>\n\n<p>You can use <code>row_order</code> and <code>column_order</code> following a <code>draw()</code> (which you can nest between a <code>pdf(null)</code> and <code>dev.off()</code> to disable drawing it on any device). That way, you'll be sure to get the right order from the first heatmap to use with the second.</p>\n\n<p>By nesting, I mean like so:</p>\n\n<pre><code>h1 &lt;- Heatmap(...)\npdf(NULL)\nh1d &lt;- draw(h1)\ndev.off()\nro1 &lt;- row_order(h1d)\nco1 &lt;- column_order(h1d)\n</code></pre>\n\n<p>Use whichever (row/column) order is appropriate to gene order.</p>\n","650","","","","2020-02-29T02:04:28.177","","","","0","",""
"11536","1","","","2020-02-29T21:42:36.700","1","8","<p>I have two FASTAs/assemblies of the same species, but the bases are somewhat different. I would like to explore this. </p>\n\n<p>What tools/methods exist to compare two FASTAs and see the difference in structure between them? Are there visualization tools available, something like IGV? </p>\n\n<p>I think users would need to do a multiple-sequence alignment between these FASTAs, and then check for the differences between them. However, I don't know if a ""standard approach"" exists to do this. </p>\n","1770","","","","2020-02-29T23:16:13.777","Tools for comparing/visualizing FASTAs?","<sequence-alignment><fasta><assembly><visualization>","1","0","",""
"11537","2","","11536","2020-02-29T23:16:13.777","1","","<p>You can try <a href=""http://darlinglab.org/mauve/mauve.html"" rel=""nofollow noreferrer"">Mauve</a> multiple genome aligner.</p>\n\n<p><a href=""https://i.stack.imgur.com/tWfoR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tWfoR.png"" alt=""enter image description here""></a></p>\n","35","","","","2020-02-29T23:16:13.777","","","","1","",""
"11538","1","","","2020-03-01T00:11:16.100","0","3","<p>I have two groups of patients; for each patient I have an output file (RNA-seq) contains this information</p>\n\n<pre><code>                  readcounts_union  readcounts_intersectionNotEmpt  genelength  R/FPKM_union    R/FPKM_intersectionNotEmpt\nENSG00000000003.10             8                      8               3688     1.06120392730222        1.05722142116966\n\nENSG00000000419.8             31                     31               4032     3.76132572546529               3.74721015127372\n</code></pre>\n\n<p>People say</p>\n\n<pre><code>readcounts_union\n</code></pre>\n\n<p><strong><em>reads that overlap a single feature are included in the count for that feature; reads that overlap more than one feature are not included in the counts</em></strong></p>\n\n<pre><code>readcounts_intersectionNotEmpty\n</code></pre>\n\n<p><strong><em>regions that are shared between features are excluded; reads that overlap a remaining region are included in the count for the feature that includes that region; reads that overlap regions from more than one feature are not included in the counts</em></strong></p>\n\n<pre><code>R/FPKM_union\n</code></pre>\n\n<p><strong><em>readcounts_union normalised for gene length and sequencing depth</em></strong></p>\n\n<pre><code>R/FPKM_intersectionNotEmpty\n</code></pre>\n\n<p><strong><em>readcounts_intersectionNotEmpty normalised for gene length and sequencing depth</em></strong></p>\n\n<p>I want to do differential expression analysis between groups but I don't know which column I should use for DESeq2</p>\n\n<p>Anybody familiar with such a format?</p>\n","4595","","","","2020-03-01T00:11:16.100","How these definitions are relate to differential expression","<r><rna-seq><differential-expression><deseq2><normalization>","0","0","",""
